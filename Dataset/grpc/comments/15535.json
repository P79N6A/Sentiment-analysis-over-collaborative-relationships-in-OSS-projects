[
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/393268153",
    "html_url": "https://github.com/grpc/grpc/issues/15535#issuecomment-393268153",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15535",
    "id": 393268153,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzI2ODE1Mw==",
    "user": {
      "login": "ncteisen",
      "id": 6148140,
      "node_id": "MDQ6VXNlcjYxNDgxNDA=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/6148140?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ncteisen",
      "html_url": "https://github.com/ncteisen",
      "followers_url": "https://api.github.com/users/ncteisen/followers",
      "following_url": "https://api.github.com/users/ncteisen/following{/other_user}",
      "gists_url": "https://api.github.com/users/ncteisen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ncteisen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ncteisen/subscriptions",
      "organizations_url": "https://api.github.com/users/ncteisen/orgs",
      "repos_url": "https://api.github.com/users/ncteisen/repos",
      "events_url": "https://api.github.com/users/ncteisen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ncteisen/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-05-30T18:26:56Z",
    "updated_at": "2018-05-30T18:26:56Z",
    "author_association": "CONTRIBUTOR",
    "body": "This was introduced as an optimization in #6612\r\n\r\nDo you have a C++ or core repro that demonstrated this load imbalance? "
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/393510819",
    "html_url": "https://github.com/grpc/grpc/issues/15535#issuecomment-393510819",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15535",
    "id": 393510819,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzUxMDgxOQ==",
    "user": {
      "login": "BusyJay",
      "id": 1701473,
      "node_id": "MDQ6VXNlcjE3MDE0NzM=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1701473?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/BusyJay",
      "html_url": "https://github.com/BusyJay",
      "followers_url": "https://api.github.com/users/BusyJay/followers",
      "following_url": "https://api.github.com/users/BusyJay/following{/other_user}",
      "gists_url": "https://api.github.com/users/BusyJay/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/BusyJay/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/BusyJay/subscriptions",
      "organizations_url": "https://api.github.com/users/BusyJay/orgs",
      "repos_url": "https://api.github.com/users/BusyJay/repos",
      "events_url": "https://api.github.com/users/BusyJay/events{/privacy}",
      "received_events_url": "https://api.github.com/users/BusyJay/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-05-31T12:14:36Z",
    "updated_at": "2018-05-31T12:14:36Z",
    "author_association": "CONTRIBUTOR",
    "body": "In following bench scenario, you can see only two threads are 99%, but other threads keep calm. Change the queue selection method to random can wake up all threads but qps drops a little bit though.\r\n```\r\n            yield _ping_pong_scenario(\r\n                'cpp_generic_async_streaming_qps_1channel_3000req%s' % secstr,\r\n                rpc_type='STREAMING',\r\n                client_type='ASYNC_CLIENT',\r\n                server_type='ASYNC_GENERIC_SERVER',\r\n                unconstrained_client='async',\r\n                use_generic_payload=True,\r\n                outstanding=2000,\r\n                channels=1,\r\n                num_clients=1,\r\n                secure=secure,\r\n                messages_per_stream=10000,\r\n                minimal_stack=not secure,\r\n                categories=smoketest_categories + [SCALABLE])\r\n```"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/415266228",
    "html_url": "https://github.com/grpc/grpc/issues/15535#issuecomment-415266228",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15535",
    "id": 415266228,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQxNTI2NjIyOA==",
    "user": {
      "login": "qiuxiafei",
      "id": 261796,
      "node_id": "MDQ6VXNlcjI2MTc5Ng==",
      "avatar_url": "https://avatars0.githubusercontent.com/u/261796?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/qiuxiafei",
      "html_url": "https://github.com/qiuxiafei",
      "followers_url": "https://api.github.com/users/qiuxiafei/followers",
      "following_url": "https://api.github.com/users/qiuxiafei/following{/other_user}",
      "gists_url": "https://api.github.com/users/qiuxiafei/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/qiuxiafei/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/qiuxiafei/subscriptions",
      "organizations_url": "https://api.github.com/users/qiuxiafei/orgs",
      "repos_url": "https://api.github.com/users/qiuxiafei/repos",
      "events_url": "https://api.github.com/users/qiuxiafei/events{/privacy}",
      "received_events_url": "https://api.github.com/users/qiuxiafei/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-08-23T02:54:29Z",
    "updated_at": "2018-08-23T04:11:04Z",
    "author_association": "NONE",
    "body": "Same issue!\r\n\r\nConsidering multiple concurrent grpc clients in the same process sharing one connection (~~grpc java's default behavior~~), all concurrent requests are handled within only one server thread. This hurts throughput and sounds unreasonable. If these requests can be handled in parallel, throughput may improve a lot.\r\n\r\nAny way to work around saying fall back to random queue selection?\r\n\r\nI have a java client and a rust server (which wraps core c++ server)\r\n```java\r\n// java client\r\n\r\nimport java.util.ArrayList;\r\nimport java.util.List;\r\n\r\nimport com.google.common.util.concurrent.AtomicDouble;\r\nimport com.test.proto.Request;\r\nimport com.test.proto.Response;\r\nimport com.test.proto.TestServiceGrpc;\r\nimport com.test.proto.TestServiceGrpc.TestServiceBlockingStub;\r\nimport io.grpc.ManagedChannel;\r\nimport io.grpc.netty.NegotiationType;\r\nimport io.grpc.netty.NettyChannelBuilder;\r\n\r\npublic class Test {\r\n\r\n    private static final int QUERY_NUM = 1000;\r\n    private static final int THREAD_NUM = 8;\r\n\r\n\r\n    public static void main(String[] args) throws InterruptedException {\r\n        ManagedChannel channel = NettyChannelBuilder.forAddress(\"10.101.212.21\", 8797)\r\n            .negotiationType(NegotiationType.PLAINTEXT)\r\n            .build();\r\n\r\n        AtomicDouble totalQps = new AtomicDouble(0);\r\n        AtomicDouble totalLatency = new AtomicDouble(0);\r\n\r\n        List<Thread> threads = new ArrayList<>();\r\n        for (int i = 0; i < THREAD_NUM; i++) {\r\n            int finalI = i;\r\n            Thread t = new Thread(() -> {\r\n                TestServiceBlockingStub stub = TestServiceGrpc.newBlockingStub(channel);\r\n\r\n                long start = System.currentTimeMillis();\r\n                for (int l = 0; l < QUERY_NUM; l++) {\r\n                    Request req = Request.newBuilder().setData(l).build();\r\n                    Response resp = stub.test(req);\r\n                    //System.out.println(resp.getData());\r\n                    if (l % 100 == 0) {\r\n                        System.out.println(\"Thread \" + finalI + \" process \" + l + \" requests.\");\r\n                    }\r\n                }\r\n                long cost = System.currentTimeMillis() - start;\r\n                double latency = cost * 1.0 / QUERY_NUM;\r\n                double qps = 1000.0 / latency;\r\n\r\n                totalQps.addAndGet(qps);\r\n                totalLatency.addAndGet(latency);\r\n\r\n                System.out.println(\"Thread \" + finalI + \" => qps: \" + qps + \", latency: \" + latency);\r\n            });\r\n            t.start();\r\n            threads.add(t);\r\n        }\r\n\r\n        for (Thread thread : threads) {\r\n            thread.join();\r\n        }\r\n\r\n        System.out.println(\"All \" + \" => qps: \" + totalQps.get() + \", latency: \" + totalLatency.get() / THREAD_NUM);\r\n    }\r\n}\r\n```\r\n\r\n```rust\r\n// rust server\r\nextern crate test;\r\nextern crate grpcio;\r\nextern crate futures;\r\n\r\nuse test::proto::test_grpc::*;\r\nuse test::proto::test::*;\r\nuse grpcio::UnarySink;\r\nuse grpcio::RpcContext;\r\nuse grpcio::ServerBuilder;\r\nuse std::sync::Arc;\r\nuse grpcio::Environment;\r\nuse std::thread;\r\nuse std::time::Duration;\r\nuse futures::future::Future;\r\n\r\n\r\n#[derive(Clone)]\r\nstruct TestServiceImpl {}\r\n\r\nimpl TestService for TestServiceImpl {\r\n    fn test(&self, ctx: RpcContext, req: Request, sink: UnarySink<Response>) {\r\n        let data = req.get_data();\r\n        let mut x = 0_i64;\r\n        for i in 0..50000 {\r\n            x += i;\r\n        }\r\n        let mut resp = Response::new();\r\n        resp.set_data(x as i32);\r\n\r\n        let f = sink.success(resp).map_err(move |e| { eprintln!(\"{:?}\", e); });\r\n        ctx.spawn(f);\r\n    }\r\n}\r\n\r\nfn main() {\r\n    let service = create_test_service(TestServiceImpl {});\r\n    let mut server = ServerBuilder::new(Arc::new(Environment::new(4)))\r\n        .register_service(service)\r\n        .requests_slot_per_cq(1024)\r\n        .bind(\"0.0.0.0\", 8797)\r\n        .build()\r\n        .unwrap();\r\n    server.start();\r\n\r\n    println!(\"started!\");\r\n\r\n    thread::sleep(Duration::from_secs(100000000000));\r\n}\r\n```\r\n\r\n```protobuf\r\n\r\nsyntax = \"proto3\";\r\n\r\n//import \"common.proto\";\r\n\r\noption java_package = \"com.test.proto\";\r\noption java_multiple_files = true;\r\noption java_generate_equals_and_hash = true;\r\noption java_generic_services = true;\r\n\r\nmessage Request {\r\n    int32 data = 1;\r\n}\r\n\r\nmessage Response {\r\n    int32 data = 1;\r\n}\r\n\r\nservice TestService {\r\n    rpc test(Request) returns (Response) {}\r\n}\r\n\r\n\r\n```\r\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/415519746",
    "html_url": "https://github.com/grpc/grpc/issues/15535#issuecomment-415519746",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15535",
    "id": 415519746,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQxNTUxOTc0Ng==",
    "user": {
      "login": "sreecha",
      "id": 2754995,
      "node_id": "MDQ6VXNlcjI3NTQ5OTU=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/2754995?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sreecha",
      "html_url": "https://github.com/sreecha",
      "followers_url": "https://api.github.com/users/sreecha/followers",
      "following_url": "https://api.github.com/users/sreecha/following{/other_user}",
      "gists_url": "https://api.github.com/users/sreecha/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sreecha/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sreecha/subscriptions",
      "organizations_url": "https://api.github.com/users/sreecha/orgs",
      "repos_url": "https://api.github.com/users/sreecha/repos",
      "events_url": "https://api.github.com/users/sreecha/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sreecha/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-08-23T18:23:12Z",
    "updated_at": "2018-08-23T18:35:16Z",
    "author_association": "CONTRIBUTOR",
    "body": "@BusyJay and @qiuxiafei :  Sorry for the late response on this. As Noah pointed out, this change was done in  #6612 to optimize the common case - where a server has multiple incoming channels from several clients.\r\n\r\nBy assigning all calls on a channel to the same completion queue, we try to save a *thread-jump* for all calls on that channel i.e whenever there are incoming bytes on that channel, the thread that is calling `CompletionQueue::Next()` **will be** the *same thread* that will poll, notice the fd is readable, read the bytes and return from `CompletionQueue::Next()`. \r\n\r\nHad we randomly distributed each call to a different completion queue, we end up with a case where all threads calling `CompletionQueue::Next()` (on each of those completion queues which have calls assigned to them that belong to the same channel) end up polling the same channel-fd . Whenever some bytes arrive on that channel (which may correspond to ANY call on that channel), ONLY ONE of those polling threads wakes up, reads the bytes; and if that thread does not correspond to the call (i.e the completion queue) to which those bytes are destined to, then it ends up waking up the \"correct\" thread i.e a *thread jump*. Thread-jumps are expensive and significantly effect latency.\r\n\r\nThe cases you brought up are legitimate (i.e server with single incoming channel - or where number of server completion queues are greater than number of channels will show this behavior. These typically tend to be special cases or some special benchmarks - and not super common in my understanding).\r\n\r\nIf for whatever reason you have less number of incoming channels than the number of completion queues on the server, the workaround I suggest is to create multiple channels (even between the same client and server) and force each channels to create a new underlying connection (you can do that by passing some random but unique channel arg to each channel).  \r\n\r\n---\r\n\r\n\r\nFYI: Here is an example of how to create multiple channels to the same destination (i.e `server_address`) but forcing each channel to use a separate connection\r\n\r\n``` C++\r\n  for (i = 0; i < 5; i++) {\r\n    ::grpc::ChannelArguments args;\r\n\r\n    // Set a dummy (but distinct) channel arg on each channel so that\r\n    // every channel gets its own connection\r\n    args.SetInt(\"test_key\", i);\r\n    std::shared_ptr<::grpc::Channel> channel =\r\n                ::grpc::CreateCustomChannel(server_address,\r\n                                            ::grpc::InsecureChannelCredentials(),\r\n                                            args);\r\n  }\r\n```\r\n\r\n\r\n\r\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/415642891",
    "html_url": "https://github.com/grpc/grpc/issues/15535#issuecomment-415642891",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15535",
    "id": 415642891,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQxNTY0Mjg5MQ==",
    "user": {
      "login": "qiuxiafei",
      "id": 261796,
      "node_id": "MDQ6VXNlcjI2MTc5Ng==",
      "avatar_url": "https://avatars0.githubusercontent.com/u/261796?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/qiuxiafei",
      "html_url": "https://github.com/qiuxiafei",
      "followers_url": "https://api.github.com/users/qiuxiafei/followers",
      "following_url": "https://api.github.com/users/qiuxiafei/following{/other_user}",
      "gists_url": "https://api.github.com/users/qiuxiafei/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/qiuxiafei/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/qiuxiafei/subscriptions",
      "organizations_url": "https://api.github.com/users/qiuxiafei/orgs",
      "repos_url": "https://api.github.com/users/qiuxiafei/repos",
      "events_url": "https://api.github.com/users/qiuxiafei/events{/privacy}",
      "received_events_url": "https://api.github.com/users/qiuxiafei/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-08-24T03:49:29Z",
    "updated_at": "2018-08-24T03:49:29Z",
    "author_association": "NONE",
    "body": "yes, @sreecha , I choose the multi-client-channel workaround either, finally, after comparing it two employing multiple threads on a single completion queue. It's simple.\r\n\r\nBut I think there should be some work-stealing strategy to leverage when workload among completion queues are extremely unbalanced."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/415879228",
    "html_url": "https://github.com/grpc/grpc/issues/15535#issuecomment-415879228",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15535",
    "id": 415879228,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQxNTg3OTIyOA==",
    "user": {
      "login": "sreecha",
      "id": 2754995,
      "node_id": "MDQ6VXNlcjI3NTQ5OTU=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/2754995?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/sreecha",
      "html_url": "https://github.com/sreecha",
      "followers_url": "https://api.github.com/users/sreecha/followers",
      "following_url": "https://api.github.com/users/sreecha/following{/other_user}",
      "gists_url": "https://api.github.com/users/sreecha/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/sreecha/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/sreecha/subscriptions",
      "organizations_url": "https://api.github.com/users/sreecha/orgs",
      "repos_url": "https://api.github.com/users/sreecha/repos",
      "events_url": "https://api.github.com/users/sreecha/events{/privacy}",
      "received_events_url": "https://api.github.com/users/sreecha/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-08-24T20:51:56Z",
    "updated_at": "2018-08-24T20:51:56Z",
    "author_association": "CONTRIBUTOR",
    "body": "Thanks @qiuxiafei . Yeah, we could perhaps think of some work stealing strategy here. That would be a complex change to make (especially considering that we have different types of polling engines in grpc each having slightly threading models). So this item is not in our radar in the near term (moreover, there is a  is a workaround). We certainly welcome any design ideas you have regarding this. Please create a proposal https://github.com/grpc/proposal if you get a chance..\r\n\r\nThanks again for your comment - I am closing this issue for now.\r\n"
  }
]
