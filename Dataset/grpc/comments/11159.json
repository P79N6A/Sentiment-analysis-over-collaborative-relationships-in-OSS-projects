[
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/302243011",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-302243011",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 302243011,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjI0MzAxMQ==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-17T21:56:39Z",
    "updated_at": "2017-05-17T21:56:39Z",
    "author_association": "MEMBER",
    "body": "I haven't yet had time to look at the grpclb code or the tests, but I do have some substantive comments in both pick_first and round_robin that I wanted to let you start thinking about.  I'll try to finish this review tomorrow.\n\n---\n\nReviewed 49 of 56 files at r1.\nReview status: 45 of 52 files reviewed at latest revision, 20 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 380 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN2baomUs3SeR3VSrt:-KkN2baomUs3SeR3VSru:befv32h) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/client_channel.c#L380)):*\n> ```C\n> }\n> \n> static void on_resolver_result_changed_locked(grpc_exec_ctx *exec_ctx,\n> ```\n\nGeneral comment: The structure of the code in this function is pretty hard to follow, and I suspect that it can be simplified a lot now that the whole function is being executed in the combiner instead of having to grab and release a mutex at various points.  We should probably look for ways to improve it, but we can do that in a separate PR.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 404 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN-l_nvrsxhH1SnZUF:-KkN-l_nvrsxhH1SnZUG:byiay8o) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/client_channel.c#L404)):*\n> ```C\n>       lb_policy_name = channel_arg->value.string;\n>     }\n>     // Special case: If all of the addresses are balancer addresses,\n> ```\n\nIt looks like you've accidentally un-done the changes from #10258.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 439 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN0EhCAns4iw7lCjPn:-KkN0EhCAns4iw7lCjPo:b-gmcivd) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/client_channel.c#L439)):*\n>         (chand->info_lb_policy_name == NULL) ||\n>         (strcmp(chand->info_lb_policy_name, lb_policy_name) != 0);\n\nDon't need parens around these two sub-expressions.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 1441 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN1vUceTqFz1ECC59a:-KkN1vUceTqFz1ECC59b:b-cq4aks) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/client_channel.c#L1441)):*\n> ```C\n> typedef struct {\n>   channel_data *chand;\n>   grpc_polling_entity pollent;\n> ```\n\nWhat's the purpose behind changing this from a pollset to a polling entity?\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 1487 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN2D1MxbxYA4J553fC:-KkN2D1MxbxYA4J553fD:bbv0smw) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/client_channel.c#L1487)):*\n> ```C\n> }\n> \n> void grpc_client_lb_channel_watch_connectivity_state(\n> ```\n\nThis looks almost identical to `grpc_client_channel_watch_connectivity_state()`, except for using a pollset_set instead of just a pollset.  As mentioned above, I don't understand why we need to use both, but I'm wondering if we can avoid this duplication.  For example, instead of providing two different functions, could we provide a single function that takes a polling_entity as a parameter, so that the caller can initialize it from either a pollset or a pollset_set, as needed?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy.h, line 78 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkMzXBzu0arABVfYCA0:-KkMzXC-dLiqOd16veZi:b9ssdm) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy.h#L78)):*\n> ```objc\n> } grpc_lb_policy_args;\n> \n> typedef void (*grpc_lb_completion)(void *cb_arg, grpc_subchannel *subchannel,\n> ```\n\nI don't think this is actually used anywhere.  I removed it in a recent PR.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy.h, line 144 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkMzrWRqrc9YhV4MWId:-KkMzrWRqrc9YhV4MWIe:bluyy9f) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy.h#L144)):*\n> ```objc\n>                                         grpc_closure *closure);\n> \n>   bool (*update)(grpc_exec_ctx *exec_ctx, grpc_lb_policy *policy,\n> ```\n\nSuggest calling this `update_locked`.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy.h, line 247 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN--8ENLoHwFYLKLUV:-KkN--8ENLoHwFYLKLUW:bqinlfq) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy.h#L247)):*\n> ```objc\n> \n> /** Update \\a policy with \\a lb_policy_args. Returns true upon success. */\n> bool grpc_lb_policy_update(grpc_exec_ctx *exec_ctx, grpc_lb_policy *policy,\n> ```\n\nSimilarly, suggest calling this `grpc_lb_policy_update_locked`.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy_factory.h, line 57 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN-HqJvjSjAaWit0g4:-KkN-HqJvjSjAaWit0g5:bria6yf) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy_factory.h#L57)):*\n> ```objc\n>     \\a num_addresses addresses.  The \\a user_data_vtable argument may be\n>     NULL if no user data will be added. */\n> grpc_lb_addresses *grpc_lb_addresses_create(\n> ```\n\nIt seems a bit odd to move the definition of the `grpc_lb_addresses` struct to another file but leave the functions that are used to construct it here.  Please either move the struct definition back here or move these functions to `lb_policy.[ch]`.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 185 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNDOx7OF_5QbbTSJrm:-KkNDOx7OF_5QbbTSJrn:b-w0wbbq) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L185)):*\n> ```C\n> }\n> \n> static void start_picking(grpc_exec_ctx *exec_ctx, pick_first_lb_policy *p) {\n> ```\n\nNot related to this PR, but we should probably call this `start_picking_locked`.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 427 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNBE4wBv8JS4Kw0aDy:-KkNBE4wBv8JS4Kw0aDz:b-x57555) ([raw file](https://github.com/grpc/grpc/blob/081c442462a21f326ff08f47ff1cd9a17c6746ea/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L427)):*\n> ```C\n>     if (addresses->addresses[i].user_data != NULL) {\n>       gpr_log(GPR_ERROR,\n>               \"This LB policy doesn't support user data. It will be ignored\");\n> ```\n\nDon't we still want this warning somewhere?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 488 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNBlt5dk2tS_Jb3kAi:-KkNBlt5dk2tS_Jb3kAj:b-w2o2uf) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L488)):*\n> ```C\n>       grpc_subchannel_key *ith_sc_key = grpc_subchannel_key_create(&sc_args[i]);\n>       const bool found_selected =\n>           (grpc_subchannel_key_compare(p->selected_key, ith_sc_key) == 0);\n> ```\n\nNo need for outer parens.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 491 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNEGzkJxsok8VIesTn:-KkNEGzkJxsok8VIesTo:b-7f5amc) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L491)):*\n> ```C\n>       grpc_subchannel_key_destroy(exec_ctx, ith_sc_key);\n>       if (found_selected) {\n>         // The currently selected subchannel is in the update: we are done.\n> ```\n\nIt seems a bit messy to leave p->subchannels holding references to subchannels that may no longer be in the address list.  If nothing else, this could avoid freeing memory when we should be doing so (e.g., if the new list is much smaller than the original one).  Also, I realize that in our current implementation, nothing should actually use any of the other subchannels once we've picked one of them, but I could easily imagine changing the implementation in a way that would do this -- e.g., if we decided that when the selected subchannel fails, we should try the next one instead of immediately failing.\n\nI think we should always update the list of subchannels when we get an update.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 534 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNHQee2R9g926yRAYA:-KkNHQee2R9g926yRAYB:b-cblkrv) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L534)):*\n> ```C\n>   if (p->updating_selected || p->updating_subchannels) {\n>     p->num_new_subchannels = num_new_subchannels;\n>     p->new_subchannels = new_subchannels;\n> ```\n\nWhat happens if we get another update before the previous update has been finished via pf_connectivity_changed_locked()?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 694 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNMAzrsc3rxWpmL7Fs:-KkNMAzrsc3rxWpmL7Ft:bbsf1py) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L694)):*\n> ```C\n>   /* Find the number of backend addresses. We ignore balancer\n>    * addresses, since we don't know how to handle them. We also need to remove\n>    * the LB addresses in order to subchannels connecting to the same server but\n> ```\n\nI think you might be missing a word between \"to\" and \"subchannels\", but I'm not sure what that word should be. :)\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 699 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNMgYLBqlSpHoQsGzI:-KkNMgYLBqlSpHoQsGzJ:b-6jc8zx) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L699)):*\n> ```C\n>       grpc_channel_args_find(args->args, GRPC_ARG_LB_ADDRESSES);\n>   if (arg == NULL || arg->type != GRPC_ARG_POINTER) {\n>     return NULL;\n> ```\n\ns/NULL/false/\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 735 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNNbusHuDX2OBN2rKS:-KkNNbusHuDX2OBN2rKT:b5glhqz) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L735)):*\n> ```C\n>   }\n> \n>   /* 2. Mark known subchannels present in update to ignore them. Move known\n> ```\n\nThe logic here seems much more complex than what I was expecting.  Why do we need to compare the individual subchannels to see which ones are still present?  Won't this happen automatically if we simply (a) construct a list of new subchannels from the new address list and then (b) unref the subchannels from the old address list?  Because of the global subchannel index, any subchannels that are present in both lists should be automatically moved to the new list without any disruption in connectivity.\n\nOne of the reasons I'm uncomfortable with this logic is that it results in a list that is not in the same order as the list that we were originally given.  Right now, we're not actually honoring this order anyway, but based on the internal discussion we're having, it sounds like we might want to do that.  And looking at this code now, I think that going ahead with that change might actually make this code easier, because it would eliminate the need to update the ready list.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 750 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNNSqHFGi-BYvJqcw8:-KkNNSqHFGi-BYvJqcw9:b-w2o2uf) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L750)):*\n> ```C\n>           grpc_subchannel_key_create(&sc_args[j].args);\n>       const bool known_subchannel_in_update =\n>           (grpc_subchannel_key_compare(curr_sc_key, new_sc_key) == 0);\n> ```\n\nNo need for outer parens.\n\n---\n\n*[src/core/ext/filters/client_channel/resolver/fake/fake_resolver.c, line 101 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkMyTNt6_8SdkXGtPqr:-KkMyTNuaQMArizAqCf2:bawt3cn) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/resolver/fake/fake_resolver.c#L101)):*\n> ```C\n>   if (r->next_completion != NULL && r->next_results != NULL) {\n>     *r->target_result =\n>         grpc_channel_args_union(r->next_results, r->channel_args);\n> ```\n\nWhy change from merge to union here?  Isn't merge good enough, especially since the duplicates from r->channel_args will never be returned when searching through the list?\n\n---\n\n*[src/core/lib/channel/channel_args.c, line 146 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkMyDMSyfPYK1P_vJmi:-KkMyDMTd4HxGjZrYsRQ:br1ap3s) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/lib/channel/channel_args.c#L146)):*\n<details><summary><i alt=\"summary\">Quoted 8 lines of code…</i></summary>\n>     bool found = false;\n>     for (size_t j = 0; j < a->num_args; ++j) {\n>       const char *a_key = a->args[j].key;\n>       if (strcmp(a_key, b_key) == 0) {\n>         found = true;\n>         break;\n>       }\n>     }\n</details>\n\nCould probably just use `grpc_channel_args_find()` here.\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159#-:-KkNOt_B3z48zCzvNZ7R:b-nkhvc7)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/302435062",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-302435062",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 302435062,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjQzNTA2Mg==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-18T15:15:03Z",
    "updated_at": "2017-05-18T15:15:03Z",
    "author_association": "MEMBER",
    "body": "Finished my review.  Please let me know if you have any questions about any of this.  Thanks!\n\n---\n\nReviewed 7 of 56 files at r1.\nReview status: all files reviewed at latest revision, 45 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 535 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQvjhI_TpZNvEAlCJs:-KkQvjhI_TpZNvEAlCJt:b9znz4r) ([raw file](https://github.com/grpc/grpc/blob/081c442462a21f326ff08f47ff1cd9a17c6746ea/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L535)):*\n> ```C\n>       grpc_connectivity_state_check(&glb_policy->state_tracker);\n> \n>   /* The new connectivity status is a function of the previous one and the new\n> ```\n\nThis comment is really useful and should probably not go away.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 666 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQw_hHYujK6EQwhqEe:-KkQw_hHYujK6EQwhqEf:b-e16l3a) ([raw file](https://github.com/grpc/grpc/blob/081c442462a21f326ff08f47ff1cd9a17c6746ea/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L666)):*\n> ```C\n>   grpc_lb_policy *new_rr_policy =\n>       create_rr_locked(exec_ctx, glb_policy->serverlist, glb_policy);\n>   if (new_rr_policy == NULL) {\n> ```\n\nIt looks like this conditional is being replaced with an assertion.  Do we have some guarantee that this will never happen?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 781 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQxWK5SqUGlJfmOl8C:-KkQxWK5SqUGlJfmOl8D:b-c2jpy0) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L781)):*\n> ```C\n> /* Returns the channel args for the LB channel, used to create a bidirectional\n>  * stream\n>  * for the reception of load balancing updates.\n> ```\n\nThis can be moved to the previous line.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 827 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQy63obnrZG2RVt72R:-KkQy63pOk-2MFORdAWe:bnvwumt) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L827)):*\n>   for (size_t i = 0; i < num_grpclb_addrs; i++) {\n>     grpc_slice_unref_internal(exec_ctx, targets_info_entries[i].key);\n>   }\n\n`grpc_slice_hash_table_create()` takes ownership of the keys, so there's no need to unref here.  (In fact, doing so is a bug, because it will leave the hash table holding potentially unreffed slices.)\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 908 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQzgQvldHIf0KBQ0yz:-KkQzgQvldHIf0KBQ0z-:baszj9d) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L908)):*\n> ```C\n>   glb_policy->response_generator =\n>       grpc_fake_resolver_response_generator_create();\n>   grpc_slice_hash_table *targets_info = NULL;\n> ```\n\nDoesn't look like this is actually used here.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 986 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR-PNlu7hyTxlzVlW9:-KkR-PNlu7hyTxlzVlWA:b-rlyn5y) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L986)):*\n> ```C\n> \n>   if (glb_policy->lb_channel != NULL) {\n>     grpc_channel_destroy(glb_policy->lb_channel);\n> ```\n\nWhy is this now being done in shutdown instead of destroy?  Don't we have to wait for any remaining callbacks to be invoked before we destroy the LB channel?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1567 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR01mkzNkgGcjcEHMn:-KkR01mkzNkgGcjcEHMo:b-ldh7k8) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1567)):*\n> ```C\n>     query_for_backends_locked(exec_ctx, glb_policy);\n>   }\n>   glb_policy->retry_timer_active = false;\n> ```\n\nJust to be safe, let's move this up to before the \"if\" block above.  That way, if something in the future causes query_for_backends_locked() to restart the retry timer, we won't cause a bug.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1588 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR0z-y07xIFql-qouM:-KkR0z-y07xIFql-qouN:bw0vd7x) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1588)):*\n> ```C\n>   lb_call_destroy_locked(exec_ctx, glb_policy);\n>   if (glb_policy->started_picking && glb_policy->updating) {\n>     GPR_ASSERT(!glb_policy->shutting_down);\n> ```\n\nWhat guarantee do we have that we won't be shut down while an update is in progress?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1591 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR0czOUluSRGdSjHdW:-KkR0czPFuPtPCVRAfdM:b-wqqmf) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1591)):*\n> ```C\n>     if (glb_policy->retry_timer_active) {\n>       grpc_timer_cancel(exec_ctx, &glb_policy->lb_call_retry_timer);\n>       glb_policy->retry_timer_active = false;\n> ```\n\nThis probably shouldn't be done here, because it will happen when the timer callback is invoked, and the timer should still be considered active in between this point and that one.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1624 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR22yOP0hhUddQMdW8:-KkR22yOP0hhUddQMdW9:b5mqzgv) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1624)):*\n> ```C\n> }\n> \n> static void glb_lb_channel_on_connectivity_changed_cb(grpc_exec_ctx *exec_ctx,\n> ```\n\nIs it the case that if the LB channel is in state READY before the update, and the currently chosen address is still in the new address list, that the channel will continue to be in state READY, and this callback will not be invoked?  If so, it's probably worth a comment pointing that out.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1648 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR1KtZznudowhg3OIy:-KkR1KtZznudowhg3OIz:bguoc1x) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1648)):*\n> ```C\n>     }\n>     case GRPC_CHANNEL_IDLE:\n>       // lb channel inactive (probably shutdown prior to update. Restart lb call\n> ```\n\nMissing a `)` before the `.`.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1656 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR1qUC0jqzwgv2jByT:-KkR1qUDxc7JjeppHqgw:b-7on1hl) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1656)):*\n> ```C\n>         glb_policy->updating = true;\n>         grpc_call_cancel(glb_policy->lb_call, NULL);\n>         // lb_on_server_status_received will pick up the cancel a reinit lb_call\n> ```\n\ns/a reinit/and reinit/\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1677 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR3hW5ha-24n7ihMjH:-KkR3hW5ha-24n7ihMjI:birwevx) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1677)):*\n> ```C\n>                               const grpc_lb_policy_args *args) {\n>   glb_lb_policy *glb_policy = (glb_lb_policy *)policy;\n>   // 1. Propagate update to lb_channel (pick first).\n> ```\n\nI think you can remove the `1.`, since there is no `2`. :)\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1681 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR3MwuM0Cw2SFmqaYQ:-KkR3MwuM0Cw2SFmqaYR:b-6jc8zx) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1681)):*\n> ```C\n>       grpc_channel_args_find(args->args, GRPC_ARG_LB_ADDRESSES);\n>   if (arg == NULL || arg->type != GRPC_ARG_POINTER) {\n>     return NULL;\n> ```\n\ns/NULL/false/\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1684 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR428IL_dqxQypDIal:-KkR428IL_dqxQypDIam:blfxqzk) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1684)):*\n> ```C\n>   }\n>   const grpc_lb_addresses *addresses = arg->value.pointer.p;\n>   if (glb_policy->lb_channel != NULL) {\n> ```\n\nCan't this be an assertion?  The lb_channel gets set in glb_create(), so it should never be NULL.  And the code here is not actually trying to handle the case where it's not NULL anyway.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1693 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR4gnfLTC_paTEqJu7:-KkR4gnfLTC_paTEqJu8:bmm4ple) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1693)):*\n>   glb_policy->lb_channel_connectivity = grpc_channel_check_connectivity_state(\n>       glb_policy->lb_channel, true /* try_to_connect */);\n>   grpc_channel_element *client_channel_elem = grpc_channel_stack_last_element(\n>       grpc_channel_get_channel_stack(glb_policy->lb_channel));\n>   GPR_ASSERT(client_channel_elem->filter == &grpc_client_channel_filter);\n\nShouldn't this be moved into the `if` block below?  If we are already watching the LB channel (i.e., because there is already another update in flight), then we presumably don't want to overwrite `glb_policy->lb_channel_connectivity`, right?\n\nActually, if there is already an update in flight, don't we need some way to handle that in `glb_lb_channel_on_connectivity_changed_cb()`?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel.c, line 59 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQvNBb9uvQfSjbv956:-KkQvNBb9uvQfSjbv957:b-bx4zrg) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel.c#L59)):*\n> ```C\n>   const grpc_arg to_add[] = {\n>       grpc_fake_resolver_response_generator_arg(response_generator)};\n>   /* We strip out the channel arg for the LB policy name, since we want\n> ```\n\nPlease update this comment to indicate why we're removing GRPC_ARG_FAKE_RESOLVER_RESPONSE_GENERATOR.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel_secure.c, line 86 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQvTt2ws-mxFMVhKde:-KkQvTt2ws-mxFMVhKdf:b-bx4zrg) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel_secure.c#L86)):*\n> ```C\n>       grpc_lb_targets_info_create_channel_arg(targets_info),\n>       grpc_fake_resolver_response_generator_arg(response_generator)};\n>   /* We strip out the channel arg for the LB policy name, since we want\n> ```\n\nPlease update this comment to indicate why we're removing GRPC_ARG_FAKE_RESOLVER_RESPONSE_GENERATOR.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 117 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQqqYatTKy3BZh0YJW:-KkQqqYatTKy3BZh0YJX:b-pghjul) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/client_lb_end2end_test.cc#L117)):*\n>     bool is_balancer;\n>     grpc::string balancer_name;\n\nThese two fields are never used in this test, so the AddressData struct can probably be replaced with a single int.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 149 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQr92tFsEwzv1CkMu5:-KkMwjINly9J8nn3hsNJ:ba54ryr) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/client_lb_end2end_test.cc#L149)):*\n> ```cpp\n>   void ResetStub(const grpc::string& lb_policy_name) {\n>     ChannelArguments args;\n>     args.SetLoadBalancingPolicyName(lb_policy_name);\n> ```\n\nIt's probably worth only doing this if `lb_policy_name` is not empty, so that we can also test that we get pick_first by default.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 259 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQtMMMYt34Xub2ZjKH:-KkQtMMMYt34Xub2ZjKI:bn78u1f) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/client_lb_end2end_test.cc#L259)):*\n> ```cpp\n>   std::vector<AddressData> addresses;\n> \n>   // Perform one RPC against the first server.\n> ```\n\nPlease also test the case where the resolver returns a new list that includes the original address, and make sure that pick_first sticks with the original address.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 265 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQrpNL7ENpYMZJqtup:-KkQrpNL7ENpYMZJqtuq:b-l0c605) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/client_lb_end2end_test.cc#L265)):*\n> ```cpp\n>   SendRpc();\n> \n>   // An empty update doesn't trigger a policy update. The RPC will go to\n> ```\n\nHmmm... Is this the right behavior?  I could argue that if the resolver returns an empty list, that means that there are no servers, in which case we should update with an empty list, just like we would have done if the original resolver result was empty.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 280 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQu7F8xA3Xwpdswmu3:-KkQu7F8xA3Xwpdswmu4:b-o0vyar) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/client_lb_end2end_test.cc#L280)):*\n> ```cpp\n>   gpr_log(GPR_INFO, \"****** SET [1] *******\");\n>   for (int i = 0; i < 10; ++i) SendRpc();\n>   EXPECT_GT(servers_[1]->service_.request_count(), 0);\n> ```\n\nWould the logic be simpler in these update tests if we had a method to reset the counters in the servers?  That way, after any transition, we could wait for the new server to get its first RPC, then reset all the counters, and then run a new test to make sure that the balancing is correct.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 322 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQu2D-eUqbRu61Fln4:-KkQu2D-eUqbRu61Fln5:b7qke0j) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/client_lb_end2end_test.cc#L322)):*\n> ```cpp\n> }\n> ```\n\nPlease remove extra blank line.\n\n---\n\n*[test/cpp/end2end/grpclb_end2end_test.cc, line 670 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR685Y8R1iglw3duZL:-KkR685Y8R1iglw3duZM:b-7wqe10) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/grpclb_end2end_test.cc#L670)):*\n> ```cpp\n>   } while (backend_servers_[1].service_->request_count() == 0);\n> \n>   const int second_backend_base_request_count =\n> ```\n\nConsider adding a method to reset the counters, so that the tests don't need to track how many requests or responses were already seen.\n\n---\n\n*[test/cpp/end2end/grpclb_end2end_test.cc, line 719 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR6U5tYriLJF4DvIV5:-KkR6U5tYriLJF4DvIV6:blot2zh) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/grpclb_end2end_test.cc#L719)):*\n> ```cpp\n>   EXPECT_EQ(10, backend_servers_[0].service_->request_count());\n> \n>   // Kill backend 0\n> ```\n\ns/backend/balancer/\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159#-:-KkR70ArUWgTDtrrB8jl:bmolle)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/302844373",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-302844373",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 302844373,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjg0NDM3Mw==",
    "user": {
      "login": "grpc-kokoro",
      "id": 12722905,
      "node_id": "MDQ6VXNlcjEyNzIyOTA1",
      "avatar_url": "https://avatars3.githubusercontent.com/u/12722905?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/grpc-kokoro",
      "html_url": "https://github.com/grpc-kokoro",
      "followers_url": "https://api.github.com/users/grpc-kokoro/followers",
      "following_url": "https://api.github.com/users/grpc-kokoro/following{/other_user}",
      "gists_url": "https://api.github.com/users/grpc-kokoro/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/grpc-kokoro/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/grpc-kokoro/subscriptions",
      "organizations_url": "https://api.github.com/users/grpc-kokoro/orgs",
      "repos_url": "https://api.github.com/users/grpc-kokoro/repos",
      "events_url": "https://api.github.com/users/grpc-kokoro/events{/privacy}",
      "received_events_url": "https://api.github.com/users/grpc-kokoro/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-20T02:10:47Z",
    "updated_at": "2017-05-20T02:10:47Z",
    "author_association": "NONE",
    "body": "```\nPerformance differences noted:\nBenchmark                                                                                                    allocs_per_iteration    atm_add_per_iteration    atm_cas_per_iteration\n-----------------------------------------------------------------------------------------------------------  ----------------------  -----------------------  -----------------------\nBM_PumpStreamClientToServer<InProcessCHTTP2>/128M                                                                                                             +881%\nBM_PumpStreamClientToServer<InProcessCHTTP2>/16M                                                                                                              +1141%\nBM_PumpStreamClientToServer<InProcessCHTTP2>/256k                                                                                                             +16%\nBM_PumpStreamClientToServer<InProcessCHTTP2>/2M                                                                                                               +78%\nBM_PumpStreamClientToServer<SockPair>/128M                                                                                                                    +97%\nBM_PumpStreamClientToServer<SockPair>/16M                                                                                            +5%                      +110%\nBM_PumpStreamClientToServer<SockPair>/256k                                                                                                                    +33%\nBM_PumpStreamClientToServer<SockPair>/2M                                                                                                                      +43%\nBM_PumpStreamClientToServer<SockPair>/32k                                                                                                                     +5%\nBM_PumpStreamClientToServer<TCP>/128M                                                                                                                         +800%\nBM_PumpStreamClientToServer<TCP>/16M                                                                                                                          +537%\nBM_PumpStreamClientToServer<TCP>/256k                                                                                                                         +34%\nBM_PumpStreamClientToServer<TCP>/2M                                                                                                                           +170%\nBM_PumpStreamClientToServer<TCP>/32k                                                                                                                          +5%\nBM_PumpStreamClientToServer<UDS>/128M                                                                                                                         +68%\nBM_PumpStreamClientToServer<UDS>/16M                                                                                                                          +83%\nBM_PumpStreamClientToServer<UDS>/256k                                                                                                                         +22%\nBM_PumpStreamClientToServer<UDS>/2M                                                                                                                           +44%\nBM_PumpStreamClientToServer<UDS>/32k                                                                                                                          +5%\nBM_PumpStreamServerToClient<InProcessCHTTP2>/128M                                                                                                             +887%\nBM_PumpStreamServerToClient<InProcessCHTTP2>/16M                                                                                                              +1150%\nBM_PumpStreamServerToClient<InProcessCHTTP2>/256k                                                                                                             +19%\nBM_PumpStreamServerToClient<InProcessCHTTP2>/2M                                                                                                               +84%\nBM_PumpStreamServerToClient<SockPair>/128M                                                                                                                    +103%\nBM_PumpStreamServerToClient<SockPair>/16M                                                                                            +5%                      +110%\nBM_PumpStreamServerToClient<SockPair>/256k                                                                                                                    +29%\nBM_PumpStreamServerToClient<SockPair>/2M                                                                                                                      +53%\nBM_PumpStreamServerToClient<SockPair>/32k                                                                                                                     +4%\nBM_PumpStreamServerToClient<TCP>/128M                                                                                                                         +940%\nBM_PumpStreamServerToClient<TCP>/16M                                                                                                 +5%                      +589%\nBM_PumpStreamServerToClient<TCP>/256k                                                                                                                         +14%\nBM_PumpStreamServerToClient<TCP>/2M                                                                                                                           +174%\nBM_PumpStreamServerToClient<TCP>/32k                                                                                                                          +6%\nBM_PumpStreamServerToClient<UDS>/128M                                                                                                                         +97%\nBM_PumpStreamServerToClient<UDS>/16M                                                                                                 +4%                      +90%\nBM_PumpStreamServerToClient<UDS>/256k                                                                                                                         +21%\nBM_PumpStreamServerToClient<UDS>/2M                                                                                                                           +41%\nBM_StreamingPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/1                                                                                        +1963%\nBM_StreamingPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/2                                                                                        +1166%\nBM_StreamingPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/1                                        +4%                     +5%                      +657%\nBM_StreamingPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/2                                        +5%                     +5%                      +738%\nBM_StreamingPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/1                                                                                        +7%\nBM_StreamingPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/2                                                                                        +14%\nBM_StreamingPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/1                                                                                          +104%\nBM_StreamingPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/2                                                                                          +103%\nBM_StreamingPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/0                                                                                        +4%\nBM_StreamingPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/1                                                                                     +1965%\nBM_StreamingPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/2                                                                                     +1263%\nBM_StreamingPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/1                                     +7%                     +7%                      +697%\nBM_StreamingPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/2                                     +5%                     +5%                      +800%\nBM_StreamingPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/1                                                                                     +8%\nBM_StreamingPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/2                                                                                     +14%\nBM_StreamingPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/1                                                                                       +66%\nBM_StreamingPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/2                                                                                       +94%\nBM_StreamingPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/32k/1                                                                                      +4%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/0                                                                                                    +4%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/1                                                                                                    +4%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/2                                                                                                    +4%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/1/1                                                                                                    +5%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/1/2                                                                                                    +4%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/128M/1                                                                        +28%                     +1238%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/128M/2                                                                        +19%                     +837%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/16M/1                                                                                                  +405%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/16M/2                                                                         +5%                      +607%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/256k/1                                                                                                 +21%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/256k/2                                                                                                 +26%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/2M/1                                                                                                   +185%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/2M/2                                                                                                   +161%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/32k/1                                                                                                  +6%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/32k/2                                                                                                  +7%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/4k/1                                                                                                   +5%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/4k/2                                                                                                   +4%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/512/1                                                                                                  +4%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/512/2                                                                                                  +4%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/64/1                                                                                                   +4%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/64/2                                                                                                   +4%\nBM_StreamingPingPong<MinTCP, NoOpMutator, NoOpMutator>/8/1                                                                                                    +4%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/0/0                                                                                                       +5%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/1/2                                                                                                       +4%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/128M/1                                                                           +25%                     +1237%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/128M/2                                                                           +16%                     +850%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/16M/1                                                                                                     +268%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/16M/2                                                                            +8%                      +568%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/256k/1                                                                                                    +21%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/256k/2                                                                                                    +21%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/2M/1                                                                                                      +65%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/2M/2                                                                                                      +177%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/32k/1                                                                                                     +6%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/32k/2                                                                                                     +6%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/4k/1                                                                                                      +4%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/4k/2                                                                                                      +5%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/512/1                                                                                                     +4%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/512/2                                                                                                     +4%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/64/1                                                                                                      +4%\nBM_StreamingPingPong<TCP, NoOpMutator, NoOpMutator>/8/1                                                                                                       +4%\nBM_StreamingPingPongMsgs<InProcessCHTTP2, NoOpMutator, NoOpMutator>/128M                                     +5%                     +4%                      +2007%\nBM_StreamingPingPongMsgs<InProcessCHTTP2, NoOpMutator, NoOpMutator>/16M                                      +5%                     +5%                      +672%\nBM_StreamingPingPongMsgs<InProcessCHTTP2, NoOpMutator, NoOpMutator>/256k                                                                                      +11%\nBM_StreamingPingPongMsgs<InProcessCHTTP2, NoOpMutator, NoOpMutator>/2M                                                                                        +142%\nBM_StreamingPingPongMsgs<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/128M                                                                                   +1966%\nBM_StreamingPingPongMsgs<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/16M                                   +5%                     +5%                      +719%\nBM_StreamingPingPongMsgs<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/256k                                                                                   +13%\nBM_StreamingPingPongMsgs<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/2M                                                                                     +127%\nBM_StreamingPingPongMsgs<MinTCP, NoOpMutator, NoOpMutator>/128M                                                                      +23%                     +1206%\nBM_StreamingPingPongMsgs<MinTCP, NoOpMutator, NoOpMutator>/16M                                                                       +7%                      +641%\nBM_StreamingPingPongMsgs<MinTCP, NoOpMutator, NoOpMutator>/256k                                                                                               +36%\nBM_StreamingPingPongMsgs<MinTCP, NoOpMutator, NoOpMutator>/2M                                                                                                 +263%\nBM_StreamingPingPongMsgs<MinTCP, NoOpMutator, NoOpMutator>/32k                                                                                                +9%\nBM_StreamingPingPongMsgs<MinTCP, NoOpMutator, NoOpMutator>/4k                                                                                                 +4%\nBM_StreamingPingPongMsgs<MinTCP, NoOpMutator, NoOpMutator>/512                                                                                                +4%\nBM_StreamingPingPongMsgs<TCP, NoOpMutator, NoOpMutator>/128M                                                                         +31%                     +1223%\nBM_StreamingPingPongMsgs<TCP, NoOpMutator, NoOpMutator>/16M                                                                                                   +473%\nBM_StreamingPingPongMsgs<TCP, NoOpMutator, NoOpMutator>/256k                                                                                                  +32%\nBM_StreamingPingPongMsgs<TCP, NoOpMutator, NoOpMutator>/2M                                                                                                    +181%\nBM_StreamingPingPongMsgs<TCP, NoOpMutator, NoOpMutator>/32k                                                                                                   +8%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/0/0                                                                        +4%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/0/1                                                                        +4%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/1/1                                                                        +4%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/1/0                    +12%                    +11%                     +2114%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/1/1                    +10%                    +9%                      +2057%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/2/0                    +7%                     +5%                      +1246%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/2/1                    +5%                     +4%                      +1255%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/1/0                     +7%                     +7%                      +731%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/1/1                     +5%                     +4%                      +748%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/2/0                     +7%                     +7%                      +727%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/2/1                     +4%                     +4%                      +755%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/1/0                                                                     +9%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/1/1                                                                     +12%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/2/0                                                                     +14%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/2/1                                                                     +18%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/1/0                                                                       +94%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/1/1                                                                       +109%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/2/0                                                                       +119%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/2/1                                                                       +121%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/32k/1/1                                                                      +4%\nBM_StreamingPingPongWithCoalescingApi<InProcessCHTTP2, NoOpMutator, NoOpMutator>/512/1/1                                                                      +4%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/0/0                                                                     +4%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/0/1                                                                     +4%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/1/0                 +10%                    +9%                      +2039%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/1/1                 +12%                    +10%                     +2090%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/2/0                 +5%                     +4%                      +1256%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/2/1                 +5%                     +4%                      +1243%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/1/0                  +4%                                              +556%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/1/1                                                                   +631%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/2/0                                                                   +578%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/2/1                  +7%                     +6%                      +1034%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/1/0                                                                  +7%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/1/1                                                                  +10%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/2/0                                                                  +15%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/2/1                                                                  +13%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/1/0                                                                    +100%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/1/1                                                                    +120%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/2/0                                                                    +140%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/2/1                                                                    +138%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/32k/1/0                                                                   +4%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/32k/1/1                                                                   +4%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/32k/2/1                                                                   +4%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/4k/1/1                                                                    +4%\nBM_StreamingPingPongWithCoalescingApi<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/512/1/1                                                                   +4%\nBM_UnaryPingPong<InProcessCHTTP2, Client_AddMetadata<RandomAsciiMetadata<100>, 1>, NoOpMutator>/0/0                                                           +5%\nBM_UnaryPingPong<InProcessCHTTP2, Client_AddMetadata<RandomAsciiMetadata<10>, 1>, NoOpMutator>/0/0                                                            +5%\nBM_UnaryPingPong<InProcessCHTTP2, Client_AddMetadata<RandomAsciiMetadata<31>, 1>, NoOpMutator>/0/0                                                            +5%\nBM_UnaryPingPong<InProcessCHTTP2, Client_AddMetadata<RandomBinaryMetadata<100>, 1>, NoOpMutator>/0/0                                                          +6%\nBM_UnaryPingPong<InProcessCHTTP2, Client_AddMetadata<RandomBinaryMetadata<100>, 2>, NoOpMutator>/0/0                                                          +6%\nBM_UnaryPingPong<InProcessCHTTP2, Client_AddMetadata<RandomBinaryMetadata<10>, 1>, NoOpMutator>/0/0                                                           +5%\nBM_UnaryPingPong<InProcessCHTTP2, Client_AddMetadata<RandomBinaryMetadata<10>, 2>, NoOpMutator>/0/0                                                           +5%\nBM_UnaryPingPong<InProcessCHTTP2, Client_AddMetadata<RandomBinaryMetadata<31>, 1>, NoOpMutator>/0/0                                                           +5%\nBM_UnaryPingPong<InProcessCHTTP2, Client_AddMetadata<RandomBinaryMetadata<31>, 2>, NoOpMutator>/0/0                                                           +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/0                                                                                               +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/1                                                                                               +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/128M                                                                                            +937%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/16M                                                                                             +323%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/256k                                                                                            +9%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/32k                                                                                             +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/4k                                                                                              +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/512                                                                                             +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/64                                                                                              +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/0/8                                                                                               +6%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/1/0                                                                                               +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/1/1                                                                                               +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/0                                           +16%                    +21%                     +1619%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/128M                                        +6%                     +5%                      +2159%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/0                                                                    +5%                      +766%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/16M                                                                                           +540%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/0                                                                                            +14%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/256k                                                                                         +15%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/0                                                                                              +10%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/2M                                                                    +4%                      +108%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/32k/0                                                                                             +6%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/32k/32k                                                                                           +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/4k/0                                                                                              +6%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/4k/4k                                                                                             +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/512/0                                                                                             +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/512/512                                                                                           +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/64/0                                                                                              +6%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/64/64                                                                                             +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/8/0                                                                                               +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, NoOpMutator>/8/8                                                                                               +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, Server_AddInitialMetadata<RandomAsciiMetadata<100>, 1>>/0/0                                                    +6%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, Server_AddInitialMetadata<RandomAsciiMetadata<10>, 100>>/0/0                                                   +21%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, Server_AddInitialMetadata<RandomAsciiMetadata<10>, 1>>/0/0                                                     +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, Server_AddInitialMetadata<RandomAsciiMetadata<31>, 1>>/0/0                                                     +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, Server_AddInitialMetadata<RandomBinaryMetadata<100>, 1>>/0/0                                                   +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, Server_AddInitialMetadata<RandomBinaryMetadata<10>, 1>>/0/0                                                    +5%\nBM_UnaryPingPong<InProcessCHTTP2, NoOpMutator, Server_AddInitialMetadata<RandomBinaryMetadata<31>, 1>>/0/0                                                    +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/0                                                                                            +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/1                                                                                            +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/128M                                                                                         +866%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/16M                                         +7%                     +6%                      +549%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/256k                                                                                         +9%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/2M                                          +4%                     +4%                      +29%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/32k                                                                                          +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/4k                                                                                           +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/512                                                                                          +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/64                                                                                           +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/0/8                                                                                            +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/1/0                                                                                            +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/1/1                                                                                            +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/0                                        +5%                     +10%                     +1312%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/128M/128M                                                                                      +2110%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/0                                                                                          +585%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/16M/16M                                                                                        +629%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/0                                                                                         +16%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/256k/256k                                                                                      +13%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/0                                                                                           +72%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/2M/2M                                                                                          +164%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/32k/0                                                                                          +7%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/32k/32k                                                                                        +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/4k/0                                                                                           +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/4k/4k                                                                                          +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/512/0                                                                                          +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/512/512                                                                                        +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/64/0                                                                                           +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/64/64                                                                                          +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/8/0                                                                                            +6%\nBM_UnaryPingPong<MinInProcessCHTTP2, NoOpMutator, NoOpMutator>/8/8                                                                                            +5%\nBM_UnaryPingPong<MinSockPair, NoOpMutator, NoOpMutator>/0/0                                                                                                   +5%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/0                                                                                                        +6%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/1                                                                                                        +7%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/128M                                                                                                     +642%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/16M                                                                                                      +215%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/256k                                                                                                     +12%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/2M                                                                                                       +96%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/32k                                                                                                      +9%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/4k                                                                                                       +7%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/512                                                                                                      +6%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/64                                                                                                       +6%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/0/8                                                                                                        +6%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/1/0                                                                                                        +7%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/1/1                                                                                                        +6%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/128M/0                                                                                                     +288%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/128M/128M                                                                         +5%                      +639%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/16M/0                                                                                                      +289%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/16M/16M                                                                                                    +273%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/256k/0                                                                                                     +12%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/256k/256k                                                                                                  +34%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/2M/0                                                                                                       +136%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/2M/2M                                                                                                      +190%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/32k/0                                                                                                      +9%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/32k/32k                                                                                                    +10%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/4k/0                                                                                                       +7%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/4k/4k                                                                                                      +8%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/512/0                                                                                                      +6%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/512/512                                                                                                    +7%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/64/0                                                                                                       +7%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/64/64                                                                                                      +7%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/8/0                                                                                                        +6%\nBM_UnaryPingPong<MinTCP, NoOpMutator, NoOpMutator>/8/8                                                                                                        +6%\nBM_UnaryPingPong<MinUDS, NoOpMutator, NoOpMutator>/0/0                                                                                                        +6%\nBM_UnaryPingPong<SockPair, NoOpMutator, NoOpMutator>/0/0                                                                                                      +5%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/0/0                                                                                                           +6%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/0/1                                                                                                           +6%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/0/128M                                                                                                        +742%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/0/16M                                                                                                         +192%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/0/256k                                                                                                        +15%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/0/2M                                                                                                          +39%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/0/32k                                                                                                         +8%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/0/4k                                                                                                          +7%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/0/512                                                                                                         +7%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/0/64                                                                                                          +6%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/0/8                                                                                                           +6%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/1/0                                                                                                           +6%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/1/1                                                                                                           +6%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/128M/0                                                                                                        +610%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/128M/128M                                                                            +6%                      +597%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/16M/0                                                                                                         +268%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/16M/16M                                                                                                       +167%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/256k/0                                                                                                        +15%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/256k/256k                                                                                                     +27%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/2M/0                                                                                                          +62%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/2M/2M                                                                                                         +195%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/32k/0                                                                                                         +8%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/32k/32k                                                                                                       +11%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/4k/0                                                                                                          +7%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/4k/4k                                                                                                         +8%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/512/0                                                                                                         +7%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/512/512                                                                                                       +6%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/64/0                                                                                                          +6%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/64/64                                                                                                         +6%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/8/0                                                                                                           +6%\nBM_UnaryPingPong<TCP, NoOpMutator, NoOpMutator>/8/8                                                                                                           +6%\nBM_UnaryPingPong<UDS, NoOpMutator, NoOpMutator>/0/0                                                                                                           +5%\n```"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/302846907",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-302846907",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 302846907,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjg0NjkwNw==",
    "user": {
      "login": "dgquintas",
      "id": 120217,
      "node_id": "MDQ6VXNlcjEyMDIxNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/120217?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dgquintas",
      "html_url": "https://github.com/dgquintas",
      "followers_url": "https://api.github.com/users/dgquintas/followers",
      "following_url": "https://api.github.com/users/dgquintas/following{/other_user}",
      "gists_url": "https://api.github.com/users/dgquintas/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dgquintas/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dgquintas/subscriptions",
      "organizations_url": "https://api.github.com/users/dgquintas/orgs",
      "repos_url": "https://api.github.com/users/dgquintas/repos",
      "events_url": "https://api.github.com/users/dgquintas/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dgquintas/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-20T03:09:40Z",
    "updated_at": "2017-05-20T03:09:40Z",
    "author_association": "NONE",
    "body": "@ctiller the performance numbers don't look too good... is there a way to run those benchmarks locally to verify?"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/302848227",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-302848227",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 302848227,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjg0ODIyNw==",
    "user": {
      "login": "ctiller",
      "id": 10120821,
      "node_id": "MDQ6VXNlcjEwMTIwODIx",
      "avatar_url": "https://avatars3.githubusercontent.com/u/10120821?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ctiller",
      "html_url": "https://github.com/ctiller",
      "followers_url": "https://api.github.com/users/ctiller/followers",
      "following_url": "https://api.github.com/users/ctiller/following{/other_user}",
      "gists_url": "https://api.github.com/users/ctiller/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ctiller/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ctiller/subscriptions",
      "organizations_url": "https://api.github.com/users/ctiller/orgs",
      "repos_url": "https://api.github.com/users/ctiller/repos",
      "events_url": "https://api.github.com/users/ctiller/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ctiller/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-20T03:43:15Z",
    "updated_at": "2017-05-20T03:43:15Z",
    "author_association": "MEMBER",
    "body": "Yeah... tools/profiling/microbenchmarks/bm_diff.py -b bm_fullstack_unary -d\nsome_sha1_hash\n\nOn Fri, May 19, 2017 at 8:09 PM David G. Quintas <notifications@github.com>\nwrote:\n\n> @ctiller <https://github.com/ctiller> the performance numbers don't look\n> too good... is there a way to run those benchmarks locally to verify?\n>\n> —\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/grpc/grpc/pull/11159#issuecomment-302846907>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/AJpudX9_hL3jykUzg4ctIThW8JmPGkbCks5r7ll9gaJpZM4Nc-KW>\n> .\n>\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/302848245",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-302848245",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 302848245,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjg0ODI0NQ==",
    "user": {
      "login": "ctiller",
      "id": 10120821,
      "node_id": "MDQ6VXNlcjEwMTIwODIx",
      "avatar_url": "https://avatars3.githubusercontent.com/u/10120821?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ctiller",
      "html_url": "https://github.com/ctiller",
      "followers_url": "https://api.github.com/users/ctiller/followers",
      "following_url": "https://api.github.com/users/ctiller/following{/other_user}",
      "gists_url": "https://api.github.com/users/ctiller/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ctiller/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ctiller/subscriptions",
      "organizations_url": "https://api.github.com/users/ctiller/orgs",
      "repos_url": "https://api.github.com/users/ctiller/repos",
      "events_url": "https://api.github.com/users/ctiller/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ctiller/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-20T03:43:50Z",
    "updated_at": "2017-05-20T03:43:50Z",
    "author_association": "MEMBER",
    "body": "Make sure everything is committed... some_sha1_hash will be what you're\ncomparing against (and will be checked out briefly during the comparison).\n\nOn Fri, May 19, 2017 at 8:42 PM Craig Tiller <ctiller@google.com> wrote:\n\n> Yeah... tools/profiling/microbenchmarks/bm_diff.py -b bm_fullstack_unary\n> -d some_sha1_hash\n>\n> On Fri, May 19, 2017 at 8:09 PM David G. Quintas <notifications@github.com>\n> wrote:\n>\n>> @ctiller <https://github.com/ctiller> the performance numbers don't look\n>> too good... is there a way to run those benchmarks locally to verify?\n>>\n>> —\n>> You are receiving this because you were mentioned.\n>>\n>>\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/grpc/grpc/pull/11159#issuecomment-302846907>, or mute\n>> the thread\n>> <https://github.com/notifications/unsubscribe-auth/AJpudX9_hL3jykUzg4ctIThW8JmPGkbCks5r7ll9gaJpZM4Nc-KW>\n>> .\n>>\n>\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/303160516",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-303160516",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 303160516,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzE2MDUxNg==",
    "user": {
      "login": "dgquintas",
      "id": 120217,
      "node_id": "MDQ6VXNlcjEyMDIxNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/120217?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dgquintas",
      "html_url": "https://github.com/dgquintas",
      "followers_url": "https://api.github.com/users/dgquintas/followers",
      "following_url": "https://api.github.com/users/dgquintas/following{/other_user}",
      "gists_url": "https://api.github.com/users/dgquintas/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dgquintas/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dgquintas/subscriptions",
      "organizations_url": "https://api.github.com/users/dgquintas/orgs",
      "repos_url": "https://api.github.com/users/dgquintas/repos",
      "events_url": "https://api.github.com/users/dgquintas/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dgquintas/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-22T17:04:12Z",
    "updated_at": "2017-05-22T17:04:12Z",
    "author_association": "NONE",
    "body": "\n\n\n\nReview status: 35 of 58 files reviewed at latest revision, 46 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 380 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN2baomUs3SeR3VSrt:-KkYURq897lXnqRUvdX_:b-pbdqz3) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/client_channel.c#L380)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nGeneral comment: The structure of the code in this function is pretty hard to follow, and I suspect that it can be simplified a lot now that the whole function is being executed in the combiner instead of having to grab and release a mutex at various points.  We should probably look for ways to improve it, but we can do that in a separate PR.\n</blockquote></details>\n\nAgreed.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 404 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN-l_nvrsxhH1SnZUF:-KkSyI3vLv4Oc5US_fga:b-dty2lb) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/client_channel.c#L404)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIt looks like you've accidentally un-done the changes from #10258.\n</blockquote></details>\n\nWoops. Fixed.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 439 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN0EhCAns4iw7lCjPn:-KkSyQBITWPvcmfBBJl_:b13yv1e) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/client_channel.c#L439)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>         (chand->info_lb_policy_name == NULL) ||\n>         (strcmp(chand->info_lb_policy_name, lb_policy_name) != 0);\n\nDon't need parens around these two sub-expressions.\n</blockquote></details>\n\nTrue, but in cases like this, multiline, I believe it helps readability.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 1441 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN1vUceTqFz1ECC59a:-KkSzorqBWFVhbt1vI_I:b-yw4yya) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/client_channel.c#L1441)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nWhat's the purpose behind changing this from a pollset to a polling entity?\n</blockquote></details>\n\ngrpclb needs to watch the connectivity of the LB channel. In order to do that, it passes in its `glb_policy->interested_parties`, which is a pollset_set. Because pre-existing code is already using this function with a pollset as an argument, it was changed to polling entity to accommodate both cases.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 1487 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN2D1MxbxYA4J553fC:-KkT-9EQlUPqwC8TRauJ:bxx8t9s) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/client_channel.c#L1487)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThis looks almost identical to `grpc_client_channel_watch_connectivity_state()`, except for using a pollset_set instead of just a pollset.  As mentioned above, I don't understand why we need to use both, but I'm wondering if we can avoid this duplication.  For example, instead of providing two different functions, could we provide a single function that takes a polling_entity as a parameter, so that the caller can initialize it from either a pollset or a pollset_set, as needed?\n</blockquote></details>\n\nYes. The duplicate is a left over from early experimentation.mak Removed.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy.h, line 78 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkMzXBzu0arABVfYCA0:-KkT3rm8DMHhgu74T1Nk:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy.h#L78)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nI don't think this is actually used anywhere.  I removed it in a recent PR.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy.h, line 144 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkMzrWRqrc9YhV4MWId:-KkT3uYztdxz_wI9BlW1:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy.h#L144)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nSuggest calling this `update_locked`.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy.h, line 247 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN--8ENLoHwFYLKLUV:-KkTAUksy6wvxy0_KE6O:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy.h#L247)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nSimilarly, suggest calling this `grpc_lb_policy_update_locked`.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy_factory.h, line 57 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkN-HqJvjSjAaWit0g4:-KkTDFNhhOk1oBSatphN:b-x2rs1g) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy_factory.h#L57)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIt seems a bit odd to move the definition of the `grpc_lb_addresses` struct to another file but leave the functions that are used to construct it here.  Please either move the struct definition back here or move these functions to `lb_policy.[ch]`.\n</blockquote></details>\n\nAt some point a reason existed to have moved those. Whatever it was, it doesn't exist anymore. Moved it all back. Sorry for the noise.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 535 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQvjhI_TpZNvEAlCJs:-KkTE-_WtY7mxkQu5Ojt:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/081c442462a21f326ff08f47ff1cd9a17c6746ea/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L535)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThis comment is really useful and should probably not go away.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 666 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQw_hHYujK6EQwhqEe:-KkTEGoSdiWsPXp7o2FO:bcz1wv3) ([raw file](https://github.com/grpc/grpc/blob/081c442462a21f326ff08f47ff1cd9a17c6746ea/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L666)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIt looks like this conditional is being replaced with an assertion.  Do we have some guarantee that this will never happen?\n</blockquote></details>\n\nThis PR is a mess, I'm sorry about the amount of leftover experimental code left laying around... the name of the branch ends in \"5\", to give an idea of the number of iterations things went through, let alone merges.\n\nRegardless, thanks for being so vigilant. Restored.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 781 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQxWK5SqUGlJfmOl8C:-KkTEXdcKyAkPBsWp-yJ:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L781)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThis can be moved to the previous line.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 827 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQy63obnrZG2RVt72R:-KkTErEZrZ3cfBwKVYJk:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L827)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>   for (size_t i = 0; i < num_grpclb_addrs; i++) {\n>     grpc_slice_unref_internal(exec_ctx, targets_info_entries[i].key);\n>   }\n\n`grpc_slice_hash_table_create()` takes ownership of the keys, so there's no need to unref here.  (In fact, doing so is a bug, because it will leave the hash table holding potentially unreffed slices.)\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 908 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQzgQvldHIf0KBQ0yz:-KkTEyG8dXGUrcNMpfrw:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L908)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nDoesn't look like this is actually used here.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 986 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR-PNlu7hyTxlzVlW9:-KkTI6iPs468uUBJU5p1:b8hzaom) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L986)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nWhy is this now being done in shutdown instead of destroy?  Don't we have to wait for any remaining callbacks to be invoked before we destroy the LB channel?\n</blockquote></details>\n\nIt moved because we now watch the connectivity of the channel via`glb_lb_channel_on_connectivity_changed_cb`. This function needs a valid`glb_policy` instance. If we were to destroy the LB channel in glb_destroy, this callback would likely be invoked after the `glb_policy` is gone, causing a use-after-free. \n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1567 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR01mkzNkgGcjcEHMn:-KkTMEYdQ6ZkSUifYyUE:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1567)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nJust to be safe, let's move this up to before the \"if\" block above.  That way, if something in the future causes query_for_backends_locked() to restart the retry timer, we won't cause a bug.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1588 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR0z-y07xIFql-qouM:-KkTP2itULJ4SWR2Oidz:bjtx8fb) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1588)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nWhat guarantee do we have that we won't be shut down while an update is in progress?\n</blockquote></details>\n\nReplaced with an `if`.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1591 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR0czOUluSRGdSjHdW:-KkTPya1qsaMR0Kgw7xH:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1591)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThis probably shouldn't be done here, because it will happen when the timer callback is invoked, and the timer should still be considered active in between this point and that one.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1624 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR22yOP0hhUddQMdW8:-KkTQDIlQy6LJVaC0p6A:bn0im5h) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1624)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIs it the case that if the LB channel is in state READY before the update, and the currently chosen address is still in the new address list, that the channel will continue to be in state READY, and this callback will not be invoked?  If so, it's probably worth a comment pointing that out.\n</blockquote></details>\n\nIt was the case. I've changed the behavior to _always_ end up invoking the cb, as the last step of the update.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1648 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR1KtZznudowhg3OIy:-KkTQnb1LxhftGtVZFKu:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1648)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nMissing a `)` before the `.`.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1656 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR1qUC0jqzwgv2jByT:-KkTQrOaJceLQt8HBYeC:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1656)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\ns/a reinit/and reinit/\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1677 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR3hW5ha-24n7ihMjH:-KkTSFbfBWOOpOJeIrHH:b9cydtv) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1677)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nI think you can remove the `1.`, since there is no `2`. :)\n</blockquote></details>\n\nStill technically correct. Done.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1681 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR3MwuM0Cw2SFmqaYQ:-KkTSQd1K2EoW_INmecs:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1681)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\ns/NULL/false/\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1684 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR428IL_dqxQypDIal:-KkTSdJTOwt7MSthyC2j:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1684)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nCan't this be an assertion?  The lb_channel gets set in glb_create(), so it should never be NULL.  And the code here is not actually trying to handle the case where it's not NULL anyway.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1693 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR4gnfLTC_paTEqJu7:-KkXTofE1WFIHu8E9HOL:b-gcp3v) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1693)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>   glb_policy->lb_channel_connectivity = grpc_channel_check_connectivity_state(\n>       glb_policy->lb_channel, true /* try_to_connect */);\n>   grpc_channel_element *client_channel_elem = grpc_channel_stack_last_element(\n>       grpc_channel_get_channel_stack(glb_policy->lb_channel));\n>   GPR_ASSERT(client_channel_elem->filter == &grpc_client_channel_filter);\n\nShouldn't this be moved into the `if` block below?  If we are already watching the LB channel (i.e., because there is already another update in flight), then we presumably don't want to overwrite `glb_policy->lb_channel_connectivity`, right?\n\nActually, if there is already an update in flight, don't we need some way to handle that in `glb_lb_channel_on_connectivity_changed_cb()`?\n</blockquote></details>\n\nDone and done. Updates arriving while another one is still being processed are ignored. The definition of \"in progress update\" is from the moment glb_update is called to the invocation of `glb_lb_channel_on_connectivity_changed_cb()` which now always happens because we watch changes from `GRPC_CHANNEL_INIT`.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel.c, line 59 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQvNBb9uvQfSjbv956:-KkXY1_bCItSff5FOVJd:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel.c#L59)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nPlease update this comment to indicate why we're removing GRPC_ARG_FAKE_RESOLVER_RESPONSE_GENERATOR.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel_secure.c, line 86 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQvTt2ws-mxFMVhKde:-KkXY3qJyxxahA1cti74:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel_secure.c#L86)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nPlease update this comment to indicate why we're removing GRPC_ARG_FAKE_RESOLVER_RESPONSE_GENERATOR.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 185 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNDOx7OF_5QbbTSJrm:-KkXYBvZih-5Ogc1TCkS:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L185)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nNot related to this PR, but we should probably call this `start_picking_locked`.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 427 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNBE4wBv8JS4Kw0aDy:-KkX_HjbGM1G6yXPr6he:b-j64con) ([raw file](https://github.com/grpc/grpc/blob/081c442462a21f326ff08f47ff1cd9a17c6746ea/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L427)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nDon't we still want this warning somewhere?\n</blockquote></details>\n\nYes. Restored. Lost in refactoring.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 488 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNBlt5dk2tS_Jb3kAi:-KkX_TnO7THN-0aLMVH3:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L488)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nNo need for outer parens.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 491 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNEGzkJxsok8VIesTn:-KkXc20B88G3VITUg6qV:b-52l500) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L491)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIt seems a bit messy to leave p->subchannels holding references to subchannels that may no longer be in the address list.  If nothing else, this could avoid freeing memory when we should be doing so (e.g., if the new list is much smaller than the original one).  Also, I realize that in our current implementation, nothing should actually use any of the other subchannels once we've picked one of them, but I could easily imagine changing the implementation in a way that would do this -- e.g., if we decided that when the selected subchannel fails, we should try the next one instead of immediately failing.\n\nI think we should always update the list of subchannels when we get an update.\n</blockquote></details>\n\nIf we are here, p->selected != NULL, which implies p->subchannels is empty and NULL (see `destroy_subchannels_locked()`, which in invoked right after p->selected is set). Going out of this branch leaves no references to subchannels behind, because 1) none exist except for p->selected we aren't changing and 2) no new subchannels have been created yet, only their subchannel args in order to calculate their subchannel keys.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 534 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNHQee2R9g926yRAYA:-KkXnFOTMyxUsk54Xaq6:bipxkc) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L534)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nWhat happens if we get another update before the previous update has been finished via pf_connectivity_changed_locked()?\n</blockquote></details>\n\nI now use `p->updating_subchannels` as a sentinel. \n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 694 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNMAzrsc3rxWpmL7Fs:-KkXnzfmxV83j9jpMTkr:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L694)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nI think you might be missing a word between \"to\" and \"subchannels\", but I'm not sure what that word should be. :)\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 699 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNMgYLBqlSpHoQsGzI:-KkXngBIxbx86vh3zjmn:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L699)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\ns/NULL/false/\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 735 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNNbusHuDX2OBN2rKS:-Kkl6J7JMYpRMPWJawah:btvblqz) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L735)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThe logic here seems much more complex than what I was expecting.  Why do we need to compare the individual subchannels to see which ones are still present?  Won't this happen automatically if we simply (a) construct a list of new subchannels from the new address list and then (b) unref the subchannels from the old address list?  Because of the global subchannel index, any subchannels that are present in both lists should be automatically moved to the new list without any disruption in connectivity.\n\nOne of the reasons I'm uncomfortable with this logic is that it results in a list that is not in the same order as the list that we were originally given.  Right now, we're not actually honoring this order anyway, but based on the internal discussion we're having, it sounds like we might want to do that.  And looking at this code now, I think that going ahead with that change might actually make this code easier, because it would eliminate the need to update the ready list.\n</blockquote></details>\n\nI'm giving your suggestion a go on a separate commit, following the one addressing the rest of the comments.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 750 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNNSqHFGi-BYvJqcw8:-KkXo5Bpf33gTp__Qeiz:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L750)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nNo need for outer parens.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/resolver/fake/fake_resolver.c, line 101 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkMyTNt6_8SdkXGtPqr:-KkXvzo2KkLj4MS58DvK:bwj978k) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/resolver/fake/fake_resolver.c#L101)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nWhy change from merge to union here?  Isn't merge good enough, especially since the duplicates from r->channel_args will never be returned when searching through the list?\n</blockquote></details>\n\nMerge will result in duplicated args. Updates shouldn't result in args accumulating in `r->target_result`. \n\n---\n\n*[src/core/lib/channel/channel_args.c, line 146 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkMyDMSyfPYK1P_vJmi:-KkXyy7Gyl1XDqr5eQkw:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/lib/channel/channel_args.c#L146)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>     bool found = false;\n>     for (size_t j = 0; j < a->num_args; ++j) {\n>       const char *a_key = a->args[j].key;\n>       if (strcmp(a_key, b_key) == 0) {\n>         found = true;\n>         break;\n>       }\n>     }\n\nCould probably just use `grpc_channel_args_find()` here.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 117 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQqqYatTKy3BZh0YJW:-KkXqFDl-NZ4x4exYNIP:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/client_lb_end2end_test.cc#L117)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>     bool is_balancer;\n>     grpc::string balancer_name;\n\nThese two fields are never used in this test, so the AddressData struct can probably be replaced with a single int.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 149 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQr92tFsEwzv1CkMu5:-KkXzWw-WA9bXuDl2406:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/client_lb_end2end_test.cc#L149)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIt's probably worth only doing this if `lb_policy_name` is not empty, so that we can also test that we get pick_first by default.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 259 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQtMMMYt34Xub2ZjKH:-KkY7Hs-7GNZNjD2nop8:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/client_lb_end2end_test.cc#L259)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nPlease also test the case where the resolver returns a new list that includes the original address, and make sure that pick_first sticks with the original address.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 265 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQrpNL7ENpYMZJqtup:-KkYMBoKrhmoiOrNUtU-:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/client_lb_end2end_test.cc#L265)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nHmmm... Is this the right behavior?  I could argue that if the resolver returns an empty list, that means that there are no servers, in which case we should update with an empty list, just like we would have done if the original resolver result was empty.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 280 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQu7F8xA3Xwpdswmu3:-KkYUOxcIqCMAeIUgWRV:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/client_lb_end2end_test.cc#L280)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nWould the logic be simpler in these update tests if we had a method to reset the counters in the servers?  That way, after any transition, we could wait for the new server to get its first RPC, then reset all the counters, and then run a new test to make sure that the balancing is correct.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 322 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkQu2D-eUqbRu61Fln4:-KkY7QdTGAwfFj_xEMn-:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/client_lb_end2end_test.cc#L322)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nPlease remove extra blank line.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/grpclb_end2end_test.cc, line 670 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR685Y8R1iglw3duZL:-KkY8niGHgmRr0R4QDQ4:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/grpclb_end2end_test.cc#L670)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nConsider adding a method to reset the counters, so that the tests don't need to track how many requests or responses were already seen.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/grpclb_end2end_test.cc, line 719 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR6U5tYriLJF4DvIV5:-KkY7ZZZXj1eCyw8dfRd:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/test/cpp/end2end/grpclb_end2end_test.cc#L719)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\ns/backend/balancer/\n</blockquote></details>\n\nDone.\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/303165066",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-303165066",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 303165066,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzE2NTA2Ng==",
    "user": {
      "login": "dgquintas",
      "id": 120217,
      "node_id": "MDQ6VXNlcjEyMDIxNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/120217?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dgquintas",
      "html_url": "https://github.com/dgquintas",
      "followers_url": "https://api.github.com/users/dgquintas/followers",
      "following_url": "https://api.github.com/users/dgquintas/following{/other_user}",
      "gists_url": "https://api.github.com/users/dgquintas/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dgquintas/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dgquintas/subscriptions",
      "organizations_url": "https://api.github.com/users/dgquintas/orgs",
      "repos_url": "https://api.github.com/users/dgquintas/repos",
      "events_url": "https://api.github.com/users/dgquintas/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dgquintas/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-22T17:21:55Z",
    "updated_at": "2017-05-22T17:21:55Z",
    "author_association": "NONE",
    "body": "@ctiller Running `tools/profiling/microbenchmarks/bm_diff.py -b bm_fullstack_unary_ping_pong -d 081c442462a21f326ff08f47ff1cd9a17c6746ea` (the commit is the latest before any of this PR's changes) results in `No significant performance differences` message at the end. I've verified this on two different machines several times."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/303220837",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-303220837",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 303220837,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzIyMDgzNw==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-22T21:09:28Z",
    "updated_at": "2017-05-22T21:09:28Z",
    "author_association": "MEMBER",
    "body": "Overall, this is still looking very reasonable, but I think there's still a problem with how we're handling the case where we receive a new update while we're still processing the preceding one.\n\nPlease let me know if you want to chat about this further.  Thanks!\n\n---\n\nReviewed 1 of 56 files at r1, 23 of 23 files at r2.\nReview status: all files reviewed at latest revision, 28 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 443 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kkl8cWf7fBJnEZljdAv:-Kkl8cWf7fBJnEZljdAw:b-ml01pq) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/client_channel.c#L443)):*\n> ```C\n>       // update\n>       lb_policy_updated = true;\n>       lb_policy_update_ok = grpc_lb_policy_update_locked(\n> ```\n\nHmmm... Should grpc_lb_policy_update_locked() return a `grpc_error*` instead of a `bool`, so that when it fails, it can report a more detailed error?\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 450 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kkl8MeO2WvQHsOpXmie:-Kkl8MePNZvBCEMypciO:b-f0kn4p) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/client_channel.c#L450)):*\n> ```C\n>             exec_ctx, chand->lb_policy, &state_error);\n>       } else {\n>         // it'll keep its default GRPC_CHANNEL_TRANSIENT_FAILURE value.\n> ```\n\nWhat does \"it\" refer to here?  (I'm guessing it refers to the \"state\" variable, but the wording is unclear.)\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy.h, line 48 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kkl9jaPC1tIuAgISQ6O:-Kkl9jaPC1tIuAgISQ6P:b6dlxy7) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy.h#L48)):*\n> ```objc\n> \n> /** Arguments passed to LB policies. */\n> typedef struct grpc_lb_policy_args {\n> ```\n\nShould this be moved back to lb_policy_factory.h as well?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 986 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR-PNlu7hyTxlzVlW9:-KklvK7rk4fjrx-D3REC:b-i210lk) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L986)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nIt moved because we now watch the connectivity of the channel via`glb_lb_channel_on_connectivity_changed_cb`. This function needs a valid`glb_policy` instance. If we were to destroy the LB channel in glb_destroy, this callback would likely be invoked after the `glb_policy` is gone, causing a use-after-free. \n</blockquote></details>\n\nIsn't this backwards?  shutdown comes before destroy, so we're actually destroying it earlier than we were before.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1693 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR4gnfLTC_paTEqJu7:-KklwYVYnIV7ZtRhFdBO:b1tgypr) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1693)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nDone and done. Updates arriving while another one is still being processed are ignored. The definition of \"in progress update\" is from the moment glb_update is called to the invocation of `glb_lb_channel_on_connectivity_changed_cb()` which now always happens because we watch changes from `GRPC_CHANNEL_INIT`.\n</blockquote></details>\n\nSee my comment in pick_first about this.  I don't think ignoring subsequent updates is the right behavior.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1675 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kklw-i9ekgKteDDZeZk:-Kklw-iAdzgMpmi5zz5H:bbkkl2h) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1675)):*\n> ```C\n>                                                       grpc_error *error) {\n>   glb_lb_policy *glb_policy = arg;\n>   gpr_log(GPR_INFO, \"LOL glb_lb_channel_on_connectivity_changed_cb %d\",\n> ```\n\n\"LOL\"?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel.c, line 66 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklwqBEP1HQegbm2U8X:-KklwqBEP1HQegbm2U8Y:bqnzu81) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel.c#L66)):*\n> ```C\n>    * - The channel arg for the resolved addresses, since that will be generated\n>    *   by the name resolver used in the LB channel.  Note that the LB channel\n>    *   will use the sockaddr resolver, so this won't actually generate a query\n> ```\n\ns/sockaddr/fake/\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel.c, line 68 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklwwMiC07bnjunQXvd:-KklwwMiC07bnjunQXve:bqnzu81) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel.c#L68)):*\n> ```C\n>    *   will use the sockaddr resolver, so this won't actually generate a query\n>    *   to DNS (or some other name service).  However, the addresses returned by\n>    *   the sockaddr resolver will have is_balancer=false, whereas our own\n> ```\n\ns/sockaddr/fake/\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel_secure.c, line 93 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklxAMJHgj_hjaWnp5s:-KklxAMJHgj_hjaWnp5t:bqnzu81) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel_secure.c#L93)):*\n> ```C\n>    * - The channel arg for the resolved addresses, since that will be generated\n>    *   by the name resolver used in the LB channel.  Note that the LB channel\n>    *   will use the sockaddr resolver, so this won't actually generate a query\n> ```\n\ns/sockaddr/fake/\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel_secure.c, line 95 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklxBoQVDdBqvaEduQE:-KklxBoQVDdBqvaEduQF:bqnzu81) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel_secure.c#L95)):*\n> ```C\n>    *   will use the sockaddr resolver, so this won't actually generate a query\n>    *   to DNS (or some other name service).  However, the addresses returned by\n>    *   the sockaddr resolver will have is_balancer=false, whereas our own\n> ```\n\ns/sockaddr/fake/\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 491 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNEGzkJxsok8VIesTn:-Kklp7PuPmB3kJNRjIQi:b-2jq58d) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L491)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nIf we are here, p->selected != NULL, which implies p->subchannels is empty and NULL (see `destroy_subchannels_locked()`, which in invoked right after p->selected is set). Going out of this branch leaves no references to subchannels behind, because 1) none exist except for p->selected we aren't changing and 2) no new subchannels have been created yet, only their subchannel args in order to calculate their subchannel keys.\n</blockquote></details>\n\nOh, I didn't realize the existing code did that.  I think that's the wrong behavior, for the reasons I outlined in my original comment.  But since it's a pre-existing problem, I guess we can leave this for a different PR if you'd prefer.  It just seems a shame to make this code even more complex than it already is, when ideally I think it should look a lot more like round_robin, just without ever changing the index into the subchannel list.\n\nIn fact, we might even want to explore actually combining this with round_robin, since the only differences between the two should be (a) pick_first doesn't change the index into the subchannel list on each pick and (b) when doing an update, if the originally chosen subchannel is in the new list, we continue using that subchannel instead of resetting the index into the subchannel list to 0.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 534 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNHQee2R9g926yRAYA:-KklFNrF8uFgEXeA94_H:b-z2fw7e) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L534)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nI now use `p->updating_subchannels` as a sentinel. \n</blockquote></details>\n\nIt looks like we're using this new field to simply ignore any new update that comes in while the preceding one is in progress.  I don't think this is the right behavior.  If we get two new address lists from the resolver in rapid succession, then we need to eventually wind  up in a state where we're using the most recent list, since that's the current one.\n\nFor example, let's say that the resolver returns the following the following two updates, in order:\n\nupdate 0: backend A, backend B\nupdate 1: backend C, backend D\n\nIf we're in the process of processing update 0 when update 1 comes in, we don't want to ignore update 1, or else it will leave us talking to the wrong backend.\n\nI think the right way to do this is to save the new update when it comes in so that we can start to process it once the currently in-progress update is done processing.  (We don't need a full queue here, just a single pending update, because if multiple pending updates come in while we are still processing the previous one, we only need to save the most recent new update.)\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 735 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNNbusHuDX2OBN2rKS:-KklurVJFcF-PO6KDRk3:bxy33zy) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L735)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nI'm giving your suggestion a go on a separate commit, following the one addressing the rest of the comments.\n</blockquote></details>\n\nSounds good.  You should probably merge in the changes from #11227, so that you don't wind up doing this twice.\n\nBTW, note that we have the same issue here as in pick_first and grpclb w.r.t. handling a new update while we're still processing the preceding one.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 693 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklDcCuPQ1fS8jbQKUz:-KklDcCuPQ1fS8jbQKV-:bke38ec) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L693)):*\n> We also need to remove the LB addresses\n>    * in order to be able to compare the subchannel keys of subchannels\n>    * connecting to the same server but from a different batch of addresses.\n\nSuggest moving this sentence down to line 718, where we actually remove the arg.\n\n---\n\n*[src/core/ext/filters/client_channel/resolver/fake/fake_resolver.c, line 101 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkMyTNt6_8SdkXGtPqr:-KklynTm50mykoEqgkq_:b-k4z29s) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/resolver/fake/fake_resolver.c#L101)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nMerge will result in duplicated args. Updates shouldn't result in args accumulating in `r->target_result`. \n</blockquote></details>\n\nIs there ever any case where we want duplicates to accumulate?  Consider checking the other callers of merge and changing them to use union, in which case we can just eliminate merge.\n\n---\n\n*[src/core/lib/security/transport/lb_targets_info.c, line 48 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklCT5suc4mNcwvPryJ:-KklCT5suc4mNcwvPryK:b-s40wlx) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/security/transport/lb_targets_info.c#L48)):*\n> ```C\n> }\n> static int targets_info_cmp(void *a, void *b) {\n>   return grpc_slice_hash_table_eq(a, b) ? 0 : 1;\n> ```\n\nWill we have any sorting problems due to the fact that when they're not equal, we always return 1, even though we don't actually know which of the two tables is \"larger\"?\n\nI've suggested elsewhere changing \"cmp\" to \"equals\" for the slice_hash_table API.  But maybe it would be better to make it a real \"cmp\"-style interface (i.e., change the function to return less than, equal to, or greater than 0 instead of just a bool).\n\n---\n\n*[src/core/lib/slice/slice_hash_table.c, line 131 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkkjbUJG9Sd5GNgn9UE:-KkkjbUKiKQ09r4k6-o7:b-a5t1a) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/slice/slice_hash_table.c#L131)):*\n> ```C\n>                               const grpc_slice_hash_table* b) {\n>   bool (*cmp_value_fn_a)(void* a, void* b) = a->cmp_value;\n>   bool (*cmp_value_fn_b)(void* a, void* b) = a->cmp_value;\n> ```\n\ns/a->cmp_value/b->cmp_value/\n\n---\n\n*[src/core/lib/slice/slice_hash_table.c, line 132 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklBH7jXwyKjHQa2kay:-KklBH7jXwyKjHQa2kaz:bfunya4) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/slice/slice_hash_table.c#L132)):*\n>   GPR_ASSERT(cmp_value_fn_a != NULL);\n>   GPR_ASSERT(cmp_value_fn_b != NULL);\n\nAnother option here would be that if no comparison function is specified, we default to using `GPR_ICMP()`.\n\n---\n\n*[src/core/lib/slice/slice_hash_table.h, line 62 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklA_8it7vZC5RoYpMg:-KklA_8it7vZC5RoYpMh:bjo8yw9) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/slice/slice_hash_table.h#L62)):*\n> ```objc\n>     size_t num_entries, grpc_slice_hash_table_entry *entries,\n>     void (*destroy_value)(grpc_exec_ctx *exec_ctx, void *value),\n>     bool (*cmp_value)(void *a, void *b));\n> ```\n\nThis isn't really a `cmp`-style function, since it returns a `bool` instead of an `int` whose value is less than, equal to, or greater than zero.  Suggest calling this something like `value_equals`.\n\nSame thing in the .c file.\n\n---\n\n*[src/core/lib/slice/slice_hash_table.h, line 62 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklAsRaGUbnPAYiB4zC:-KklAsRbWWaai-BJysVG:b-ub6o84) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/slice/slice_hash_table.h#L62)):*\n> ```objc\n>     size_t num_entries, grpc_slice_hash_table_entry *entries,\n>     void (*destroy_value)(grpc_exec_ctx *exec_ctx, void *value),\n>     bool (*cmp_value)(void *a, void *b));\n> ```\n\nPlease document this parameter, including the fact that it may be NULL.\n\n---\n\n*[test/core/slice/slice_hash_table_test.c, line 82 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklBcr9elEyolCGTkkJ:-KklBcr9elEyolCGTkkK:b-sds819) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/core/slice/slice_hash_table_test.c#L82)):*\n> ```C\n> static grpc_slice_hash_table* create_table_from_entries(\n>     const test_entry* test_entries, size_t num_test_entries,\n>     bool (*value_cmp_fn)(void*, void*)) {\n> ```\n\nSame comment as elsewhere w.r.t. \"cmp\" vs. \"equals\".\n\n---\n\n*[test/core/slice/slice_hash_table_test.c, line 169 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklBvR6zKR5ujeAr1dO:-KklBvR6zKR5ujeAr1dP:b8jg7o9) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/core/slice/slice_hash_table_test.c#L169)):*\n> ```C\n> }\n> \n> static void test_slice_hash_table_eq_failure() {\n> ```\n\nThis should also test the case where the comparison functions are different.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 210 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkluIWF676fYueOSDIM:-KkluIWF676fYueOSDIN:bxb5q7j) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/cpp/end2end/client_lb_end2end_test.cc#L210)):*\n> ```cpp\n>   };\n> \n>   int WaitForServer(size_t server_idx, int start_count) {\n> ```\n\nThere's probably no need for the `start_count` parameter, since we can just check the value of the server's counter before we start the loop.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 308 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkltYuebvdhMiZ69Sni:-KkltYuebvdhMiZ69Snj:b-l1nu10) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/cpp/end2end/client_lb_end2end_test.cc#L308)):*\n> ```cpp\n> \n>   // Perform one RPC against the first server.\n>   ports.emplace_back(servers_[0]->port_);\n> ```\n\nMight want to make this `servers_[1]`, since that proves that the second update doesn't cause us to pick the first element in the list.\n\n---\n\n*[test/cpp/end2end/grpclb_end2end_test.cc, line 703 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklxcYcH86ty1yNxUfZ:-KklxcYcH86ty1yNxUf_:bj8jyax) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/cpp/end2end/grpclb_end2end_test.cc#L703)):*\n> ```cpp\n> }\n> \n> // Send and update with the same set of LBs as the one in SetUp() in order to\n> ```\n\ns/and/an/\n\n---\n\n*[test/cpp/end2end/grpclb_end2end_test.cc, line 707 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kkly9usUfIRnYeS1XWD:-Kkly9usUfIRnYeS1XWE:b-9ufjs8) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/cpp/end2end/grpclb_end2end_test.cc#L707)):*\n>   const std::vector<int> first_backend{GetBackendPorts()[0]};\n>   const std::vector<int> second_backend{GetBackendPorts()[0]};\n\nAre these two backends supposed to be the same, or different?\n\n---\n\n*[test/cpp/end2end/grpclb_end2end_test.cc, line 757 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklyVvQ8h9bsp-ChMoK:-KklyVvQ8h9bsp-ChMoL:b-cscx59) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/cpp/end2end/grpclb_end2end_test.cc#L757)):*\n> ```cpp\n>   } while (gpr_time_cmp(gpr_now(GPR_CLOCK_REALTIME), deadline) < 0);\n>   // grpclb continued using the original LB call to the first balancer, which\n>   // doesn't assign the second balancer.\n> ```\n\ns/balancer/backend/\n\n---\n\n*[test/cpp/end2end/grpclb_end2end_test.cc, line 780 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklyYFd8TjT8hsBXa4S:-KklyYFd8TjT8hsBXa4T:b870oag) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/cpp/end2end/grpclb_end2end_test.cc#L780)):*\n> ```cpp\n>   } while (gpr_time_cmp(gpr_now(GPR_CLOCK_REALTIME), deadline) < 0);\n>   // grpclb continued using the original LB call to the first balancer, which\n>   // doesn't assign the second balancer.\n> ```\n\ns/second balancer/first backend/ ?\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159#-:-KklzJD_6WxNeAN14ttR:bjm04dv)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/304179607",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-304179607",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 304179607,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDE3OTYwNw==",
    "user": {
      "login": "dgquintas",
      "id": 120217,
      "node_id": "MDQ6VXNlcjEyMDIxNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/120217?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dgquintas",
      "html_url": "https://github.com/dgquintas",
      "followers_url": "https://api.github.com/users/dgquintas/followers",
      "following_url": "https://api.github.com/users/dgquintas/following{/other_user}",
      "gists_url": "https://api.github.com/users/dgquintas/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dgquintas/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dgquintas/subscriptions",
      "organizations_url": "https://api.github.com/users/dgquintas/orgs",
      "repos_url": "https://api.github.com/users/dgquintas/repos",
      "events_url": "https://api.github.com/users/dgquintas/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dgquintas/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-26T03:10:15Z",
    "updated_at": "2017-05-26T03:10:15Z",
    "author_association": "NONE",
    "body": "\n\n\n\nReview status: 23 of 57 files reviewed at latest revision, 27 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 443 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kkl8cWf7fBJnEZljdAv:-Kl1KJYsFd-Gi_zEA_RW:b-lwkab6) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/client_channel.c#L443)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nHmmm... Should grpc_lb_policy_update_locked() return a `grpc_error*` instead of a `bool`, so that when it fails, it can report a more detailed error?\n</blockquote></details>\n\nYes. In fact, it shouldn't return anything but instead take an `on_complete` callback to be invoked upon completion, given that the update is an intrinsically asynchronous event. See my other comment (and added TODO) about this.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 450 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kkl8MeO2WvQHsOpXmie:-Kl1M5oIora7uJmhSjPb:b-29rzg2) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/client_channel.c#L450)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nWhat does \"it\" refer to here?  (I'm guessing it refers to the \"state\" variable, but the wording is unclear.)\n</blockquote></details>\n\nSorry, another leftover logging statement. Removed. \n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy.h, line 48 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kkl9jaPC1tIuAgISQ6O:-Kl1K3K6JwIdutcUY9zu:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy.h#L48)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nShould this be moved back to lb_policy_factory.h as well?\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 986 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR-PNlu7hyTxlzVlW9:-KkmS8Rwm4l0gZwjQS-z:b-gxgull) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L986)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIsn't this backwards?  shutdown comes before destroy, so we're actually destroying it earlier than we were before.\n</blockquote></details>\n\nshutdown comes first, while `glb_policy` is still valid. The channel destruction _in glb_shutdown_ triggers the watching callback, which needs a valid `glb_policy` instance, which exists because all this is happening _before_ glb_shutdown. \n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1693 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR4gnfLTC_paTEqJu7:-Kl1JB_yMZqceRnPluZx:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1693)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nSee my comment in pick_first about this.  I don't think ignoring subsequent updates is the right behavior.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1675 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kklw-i9ekgKteDDZeZk:-Kl1JGQt80i6aUTfsMPE:b-pvytxj) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1675)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n\"LOL\"?\n</blockquote></details>\n\nIt's funny code alright. Removed.\n\nObviously, it's a leftover debugging log marker.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel.c, line 66 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklwqBEP1HQegbm2U8X:-Kl1Lpj9zUHhbWeaz9Zs:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel.c#L66)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\ns/sockaddr/fake/\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel.c, line 68 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklwwMiC07bnjunQXvd:-Kl1LpJa7wBnWe1-isN9:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel.c#L68)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\ns/sockaddr/fake/\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel_secure.c, line 93 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklxAMJHgj_hjaWnp5s:-Kl1LitRAo6tJdQIhIPj:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel_secure.c#L93)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\ns/sockaddr/fake/\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel_secure.c, line 95 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklxBoQVDdBqvaEduQE:-Kl1LjOVZrVHVPm901UL:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb_channel_secure.c#L95)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\ns/sockaddr/fake/\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 534 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNHQee2R9g926yRAYA:-Kl107e9_d_abMHvSdvL:b-ef2lki) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L534)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIt looks like we're using this new field to simply ignore any new update that comes in while the preceding one is in progress.  I don't think this is the right behavior.  If we get two new address lists from the resolver in rapid succession, then we need to eventually wind  up in a state where we're using the most recent list, since that's the current one.\n\nFor example, let's say that the resolver returns the following the following two updates, in order:\n\nupdate 0: backend A, backend B\nupdate 1: backend C, backend D\n\nIf we're in the process of processing update 0 when update 1 comes in, we don't want to ignore update 1, or else it will leave us talking to the wrong backend.\n\nI think the right way to do this is to save the new update when it comes in so that we can start to process it once the currently in-progress update is done processing.  (We don't need a full queue here, just a single pending update, because if multiple pending updates come in while we are still processing the previous one, we only need to save the most recent new update.)\n</blockquote></details>\n\nChanges made. Not too happy about them. As I mention in a TODO: \n\"instead of returning a bool, add an \\a on_complete closure to be\n * invoked once the update has been processed. Note that only the last update\n * of a sequence arriving while still processing a previous update will take\n * effect. All others will be discarded and their associated \\a on_complete\n * invoked with their error set to GRPC_ERROR_CANCELLED */\"\n\nBasically, the deferred application of the update is an asynchronous event, whereas up to this point the update function was (incorrectly, for both pf and grpclb the update only completes in a separate callback) considered asynchronous. Changing the grpc_lb_policy_update to take an \"on_complete\" closure is non-trivial and I'd rather wrap this already-way-too-long PR first.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 735 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNNbusHuDX2OBN2rKS:-Kl0e8KfR6Z8_r35Kf7X:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L735)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nSounds good.  You should probably merge in the changes from #11227, so that you don't wind up doing this twice.\n\nBTW, note that we have the same issue here as in pick_first and grpclb w.r.t. handling a new update while we're still processing the preceding one.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 693 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklDcCuPQ1fS8jbQKUz:-Kl0e5HQp-b5K8Sat7w_:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L693)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n> We also need to remove the LB addresses\n>    * in order to be able to compare the subchannel keys of subchannels\n>    * connecting to the same server but from a different batch of addresses.\n\nSuggest moving this sentence down to line 718, where we actually remove the arg.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/resolver/fake/fake_resolver.c, line 101 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkMyTNt6_8SdkXGtPqr:-Kl1bFsSYQlBPA9o8165:b-ebn6id) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/resolver/fake/fake_resolver.c#L101)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIs there ever any case where we want duplicates to accumulate?  Consider checking the other callers of merge and changing them to use union, in which case we can just eliminate merge.\n</blockquote></details>\n\nNone. In fact, `grpc_channel_args_merge` wasn't being used at all throughout the codebase. Removed.\n\n---\n\n*[src/core/lib/security/transport/lb_targets_info.c, line 48 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklCT5suc4mNcwvPryJ:-Kl1dCiS4lOwkQ2tAGad:b4968t) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/security/transport/lb_targets_info.c#L48)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nWill we have any sorting problems due to the fact that when they're not equal, we always return 1, even though we don't actually know which of the two tables is \"larger\"?\n\nI've suggested elsewhere changing \"cmp\" to \"equals\" for the slice_hash_table API.  But maybe it would be better to make it a real \"cmp\"-style interface (i.e., change the function to return less than, equal to, or greater than 0 instead of just a bool).\n</blockquote></details>\n\nNo. We don't care about sorting order in this case. And even if we did, the only consequence is that the \"sorted\" result would cluster equal elements together. \n\nThere's no simple way to define a comparison between tables. Even if we wanted to (something we don't currently need), it'd necessitate adding extra methods to the slice_hash_table API, for more (unnecessary) complexity.\n\n---\n\n*[src/core/lib/slice/slice_hash_table.c, line 131 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkkjbUJG9Sd5GNgn9UE:-Kl1b_MF9N04tYrG8fpd:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/slice/slice_hash_table.c#L131)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\ns/a->cmp_value/b->cmp_value/\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/lib/slice/slice_hash_table.c, line 132 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklBH7jXwyKjHQa2kay:-Kl1bkGkd9wKEHbOaCvB:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/slice/slice_hash_table.c#L132)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>   GPR_ASSERT(cmp_value_fn_a != NULL);\n>   GPR_ASSERT(cmp_value_fn_b != NULL);\n\nAnother option here would be that if no comparison function is specified, we default to using `GPR_ICMP()`.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/lib/slice/slice_hash_table.h, line 62 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklA_8it7vZC5RoYpMg:-Kl1eiMBW5hKPWhorMI1:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/slice/slice_hash_table.h#L62)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThis isn't really a `cmp`-style function, since it returns a `bool` instead of an `int` whose value is less than, equal to, or greater than zero.  Suggest calling this something like `value_equals`.\n\nSame thing in the .c file.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/lib/slice/slice_hash_table.h, line 62 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklAsRaGUbnPAYiB4zC:-Kl1f2xcmZkyqqgfvb5d:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/slice/slice_hash_table.h#L62)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nPlease document this parameter, including the fact that it may be NULL.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/core/slice/slice_hash_table_test.c, line 82 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklBcr9elEyolCGTkkJ:-Kl1fWje5k-sh-jUSOio:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/core/slice/slice_hash_table_test.c#L82)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nSame comment as elsewhere w.r.t. \"cmp\" vs. \"equals\".\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/core/slice/slice_hash_table_test.c, line 169 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklBvR6zKR5ujeAr1dO:-Kl1hko8gUNdya4Sg1uq:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/core/slice/slice_hash_table_test.c#L169)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThis should also test the case where the comparison functions are different.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 210 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkluIWF676fYueOSDIM:-Kl1GIr96hmWOKFrdaa8:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/cpp/end2end/client_lb_end2end_test.cc#L210)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThere's probably no need for the `start_count` parameter, since we can just check the value of the server's counter before we start the loop.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 308 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkltYuebvdhMiZ69Sni:-Kl1GgO2WZCpbyaduHyQ:b-8953hk) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/cpp/end2end/client_lb_end2end_test.cc#L308)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nMight want to make this `servers_[1]`, since that proves that the second update doesn't cause us to pick the first element in the list.\n</blockquote></details>\n\nAs the test is right now, it first connects to 0 and then gets an update that creates a subchannel against a _different_ address (servers_[1]) and yet, continues to use servers_[0]. Following your suggestion exercises less code, because the update will finish without creating both subchannels.\n\n---\n\n*[test/cpp/end2end/grpclb_end2end_test.cc, line 703 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklxcYcH86ty1yNxUfZ:-Kl1IZI7_uhG3mhMj9e8:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/cpp/end2end/grpclb_end2end_test.cc#L703)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\ns/and/an/\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/grpclb_end2end_test.cc, line 707 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kkly9usUfIRnYeS1XWD:-Kl1IfEvmfQhn6X_MEcb:b-c5ig7l) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/cpp/end2end/grpclb_end2end_test.cc#L707)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>   const std::vector<int> first_backend{GetBackendPorts()[0]};\n>   const std::vector<int> second_backend{GetBackendPorts()[0]};\n\nAre these two backends supposed to be the same, or different?\n</blockquote></details>\n\nSame. See the test name :)\n\n---\n\n*[test/cpp/end2end/grpclb_end2end_test.cc, line 757 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklyVvQ8h9bsp-ChMoK:-Kl1IptGPQgrm6TvXWfw:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/cpp/end2end/grpclb_end2end_test.cc#L757)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\ns/balancer/backend/\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/grpclb_end2end_test.cc, line 780 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklyYFd8TjT8hsBXa4S:-Kl1IuEZUCgryWawqyZX:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/cpp/end2end/grpclb_end2end_test.cc#L780)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\ns/second balancer/first backend/ ?\n</blockquote></details>\n\nDone.\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/304184235",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-304184235",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 304184235,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDE4NDIzNQ==",
    "user": {
      "login": "grpc-kokoro",
      "id": 12722905,
      "node_id": "MDQ6VXNlcjEyNzIyOTA1",
      "avatar_url": "https://avatars3.githubusercontent.com/u/12722905?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/grpc-kokoro",
      "html_url": "https://github.com/grpc-kokoro",
      "followers_url": "https://api.github.com/users/grpc-kokoro/followers",
      "following_url": "https://api.github.com/users/grpc-kokoro/following{/other_user}",
      "gists_url": "https://api.github.com/users/grpc-kokoro/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/grpc-kokoro/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/grpc-kokoro/subscriptions",
      "organizations_url": "https://api.github.com/users/grpc-kokoro/orgs",
      "repos_url": "https://api.github.com/users/grpc-kokoro/repos",
      "events_url": "https://api.github.com/users/grpc-kokoro/events{/privacy}",
      "received_events_url": "https://api.github.com/users/grpc-kokoro/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-26T03:51:50Z",
    "updated_at": "2017-05-26T03:51:50Z",
    "author_association": "NONE",
    "body": "```\nNo significant performance differences\n```"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/304336967",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-304336967",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 304336967,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDMzNjk2Nw==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-26T17:07:30Z",
    "updated_at": "2017-05-26T17:07:30Z",
    "author_association": "MEMBER",
    "body": "The pick_first code looks pretty good, but I think there are still some problems in round_robin.  I'm happy to discuss further in person (or via VC) if you'd like.\n\n---\n\nReviewed 1 of 56 files at r1, 1 of 5 files at r3, 32 of 34 files at r4, 2 of 2 files at r5.\nReview status: all files reviewed at latest revision, 23 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 443 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kkl8cWf7fBJnEZljdAv:-Kl4ErtBWFuwydraUKsF:beli2oq) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/client_channel.c#L443)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nYes. In fact, it shouldn't return anything but instead take an `on_complete` callback to be invoked upon completion, given that the update is an intrinsically asynchronous event. See my other comment (and added TODO) about this.\n</blockquote></details>\n\nIt looks like the only thing we're using the return status for is to decide if we should check the connectivity state, and (as per my comment below) I'm not sure we actually need to do that.  If not, we can probably just change this to return void.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 447 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4D-uiDrDn3CrisCNX:-Kl4D-ujwgPCTA9qJGD8:b7b3k6f) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/client_channel.c#L447)):*\n> ```C\n>       if (lb_policy_update_ok) {\n>         GRPC_ERROR_UNREF(state_error);\n>         state = grpc_lb_policy_check_connectivity_locked(\n> ```\n\nHmm... Do we actually need to do this?\n\nPrior to this PR, the code here would always create a new LB policy, and it would call `grpc_lb_policy_check_connectivity_locked()` to get the policy's initial state.  Then the code down on line 538 would set the client channel's initial connectivity state and start a watch on the LB policy.\n\nHowever, in the update case, we are not replacing the LB policy instance, which means that the watch we created when we first instantiated the LB policy should still be in effect.  Given that, I think we don't need to get the state, because it's already being reported by the existing watch.  We also don't need to start the watch, because it's already in flight.  (In fact, the way this code is right now, it's probably a bug that we're calling `watch_lb_policy_locked()` in the update case, since that callback is already being used for the same watch.)\n\nSo, I think we should (a) not call `grpc_lb_policy_check_connectivity_state()` here, and (b) skip the block on line 538 in the update case.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy_factory.h, line 121 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4A0eNo32uYd6MFOei:-Kl4A0eNo32uYd6MFOej:b-vvceto) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy_factory.h#L121)):*\n> ```objc\n> \n> /** Arguments passed to LB policies. */\n> struct grpc_lb_policy_args {\n> ```\n\nWhy is this no longer a typedef?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 986 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR-PNlu7hyTxlzVlW9:-Kl4ch7NbxD1-TXra5Vi:bycnb8i) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L986)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nshutdown comes first, while `glb_policy` is still valid. The channel destruction _in glb_shutdown_ triggers the watching callback, which needs a valid `glb_policy` instance, which exists because all this is happening _before_ glb_shutdown. \n</blockquote></details>\n\nOkay, thanks, I think I get it now.\n\nIs it safe to destroy the channel while a call on that channel still exists?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1818 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4ec6AJRUvZiGxdk2U:-Kl4ec6AJRUvZiGxdk2V:bl2fhos) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1818)):*\n> ```C\n>         if (glb_policy->pending_update_args != NULL) {\n>           glb_update_locked(exec_ctx, &glb_policy->base,\n>                             glb_policy->pending_update_args);\n> ```\n\nSame comment as in pick_first: if `pending_update_args` is set, `glb_update_locked()` will first free it and then use the passed-in `args`.  However, in this case, `args` is equal to `glb_policy->pending_update_args`, so it will be a use-after-free bug.  I think we need to reset `glb_policy->pending_update_args` to NULL before calling `glb_update_locked()`.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 534 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNHQee2R9g926yRAYA:-Kl4_FY-WkLcFI1XBEIN:b8f4irq) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L534)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nChanges made. Not too happy about them. As I mention in a TODO: \n\"instead of returning a bool, add an \\a on_complete closure to be\n * invoked once the update has been processed. Note that only the last update\n * of a sequence arriving while still processing a previous update will take\n * effect. All others will be discarded and their associated \\a on_complete\n * invoked with their error set to GRPC_ERROR_CANCELLED */\"\n\nBasically, the deferred application of the update is an asynchronous event, whereas up to this point the update function was (incorrectly, for both pf and grpclb the update only completes in a separate callback) considered asynchronous. Changing the grpc_lb_policy_update to take an \"on_complete\" closure is non-trivial and I'd rather wrap this already-way-too-long PR first.\n</blockquote></details>\n\nAs per my comment in client_channel.c, I think this may not be an issue.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 355 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4aY_GnX6hUSIVJ4vf:-Kl4aY_GnX6hUSIVJ4vg:b9eq30o) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L355)):*\n> ```C\n>     }\n>     if (p->pending_update_args != NULL) {\n>       gpr_free(p->pending_update_args);\n> ```\n\nDon't we also need to call `grpc_channel_args_destroy(p->pending_update_args->args)`?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 355 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4bNBRb8gKvCno4ohT:-Kl4bNBRb8gKvCno4ohU:b-yhmso8) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L355)):*\n> ```C\n>     }\n>     if (p->pending_update_args != NULL) {\n>       gpr_free(p->pending_update_args);\n> ```\n\nIf we're called from `pf_connectivity_changed_locked()`, then won't `p->pending_update_args` and `args` be the same thing?  If so, this will free the struct and then try copying from it.\n\nI think we should change `pf_connectivity_changed_locked()` to do something like this:\n\n```\ngrpc_lb_policy_args *args = p->pending_update_args;\np->pending_update_args = NULL;\npf_update_locked(exec_ctx, p, args);\n```\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 475 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4XQ78lXgLQbAX4NEw:-Kl4XQ78lXgLQbAX4NEx:bqnhpf8) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L475)):*\n> ```C\n>     }\n>     if (p->pending_update_args != NULL) {\n>       // TODO(dgq): this discards the returned value. Change once LB updates\n> ```\n\nMight not be necessary, as per my comment in client_channel.c.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 428 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4KZAard0nZsALSdsP:-Kl4KZAard0nZsALSdsQ:bipwhpp) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L428)):*\n>       // Note that sd->subchannel could be NULL if the subchannel had shutdown\n>       // between the setting of sd->shutting_down and the invocation of this\n>       // callback. In that case, sd->user_data is already destroyed.\n\nHow can this happen?  The code that sets sd->subchannel to NULL is right here in this callback, and both the code that sets sd->shutting_down and this callback run in the same combiner.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 437 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4L9Hki-WKEPVTWlB4:-Kl4L9Hki-WKEPVTWlB5:b88s9bd) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L437)):*\n>     // If sd->shutting_down is true, the subchannel has already\n>     // been unsubscribed from connectivity updates: it's safe to free \"sd\".\n\nIt's not entirely accurate to say that it has already been unsubscribed.  Actually, this callback is the result of that subscription, and the very last thing it does is to renew the subscription.  So it would probably be more accurate to say \"If sd->shutting_down is true, we are not going to renew the connectivity update subscription, so it's safe to free sd.\"\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 460 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4SMoSWY8X3Ge9FP2g:-Kl4SMoSWY8X3Ge9FP2h:b-ymfsx0) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L460)):*\n>   update_state_counters_locked(sd);\n>   sd->prev_connectivity_state = sd->curr_connectivity_state;\n>   grpc_connectivity_state new_connectivity_state =\n>       update_lb_connectivity_status_locked(exec_ctx, sd, GRPC_ERROR_REF(error));\n\nI think this aggregation code won't work exactly right while an update is in flight.  In particular, we need to make sure that the counters in `update_state_counters_locked()` will reflect whichever subchannel list is currently in use (see my comment below about not switching to the new list until at least one subchannel is in READY state).\n\nI suspect that this will be a lot easier if we had a separate struct to contain the list of subchannel_data structs.  For example:\n\n```\ntypedef struct {\n  /** backpointer to owning policy */\n  round_robin_lb_policy *policy;\n\n    /** total number of addresses received at creation time */\n  size_t num_addresses;\n\n  /** all our subchannels */\n  size_t num_subchannels;\n  subchannel_data *subchannels;\n\n  /** how many subchannels are in state READY */\n  size_t num_ready;\n  /** how many subchannels are in state TRANSIENT_FAILURE */\n  size_t num_transient_failures;\n  /** how many subchannels are in state IDLE */\n  size_t num_idle;\n\n  /** There will be one ref for each entry in subchannels for which there is a pending\n   * connectivity state watcher callback. */\n  gpr_refcount refs;\n\n  bool shutdown;\n} rr_subchannel_list;\n```\n\nThe round_robin_lb_policy struct would contain a pointer to this struct instead of directly containing the fields in this struct.  The subchannel_data struct would have a backpointer to the rr_subchannel_list that contains it, rather than a backpointer directly to the round_robin_lb_policy itself.\n\nThe connectivity state callback would do the aggregation across only the subchannels within the rr_subchannel_list.  When the aggregate state goes to READY and the subchannel list is not being shut down, we can swap out the rr_subchannel_list held by the RR policy, update the policy's connectivity state, and shut down the old rr_subchannel_list (if any).\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 572 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4LsB3ie1Cy1-0kTmB:-Kl4LsB4ZJArX0UIcwvN:b5jp8rf) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L572)):*\n> ```C\n>     if (!addresses->addresses[i].is_balancer) ++num_addrs;\n>   }\n> ```\n\nNit: Please remove blank lines within functions.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 612 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4NIXta4DW8cPAEwP1:-Kl4NIXta4DW8cPAEwP2:b-4wp70f) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L612)):*\n<details><summary><i alt=\"summary\">Quoted 22 lines of code…</i></summary>\n>       // Look for subchannel in current p->subchannels to increase its refcount.\n>       subchannel_data *curr_sd = NULL;\n>       for (size_t j = 0; j < p->num_subchannels; ++j) {\n>         if (p->subchannels[j]->subchannel == subchannel) {\n>           curr_sd = p->subchannels[j];\n>           gpr_ref(&curr_sd->refcount);\n>           break;  // there should be only one.\n>         }\n>       }\n>       if (curr_sd != NULL) {\n>         update_subchannels[subchannel_index] = curr_sd;\n>         if (GRPC_TRACER_ON(grpc_lb_round_robin_trace)) {\n>           char *address_uri =\n>               grpc_sockaddr_to_uri(&addresses->addresses[i].address);\n>           gpr_log(GPR_DEBUG, \"Reusing subchannel %p for address uri %s\",\n>                   (void *)subchannel, address_uri);\n>           gpr_free(address_uri);\n>         }\n>         // Discount the subchannel ref from the unnecessary creation in step 1\n>         // of the subchannel being resused.\n>         GRPC_SUBCHANNEL_UNREF(exec_ctx, subchannel, \"rr_update_sc_known\");\n>       }\n</details>\n\nWhy is any of this necessary?  Why not simply create a new subchannel_data struct for each address in the update list, regardless of whether or not it exists in the previous list, and then unref the ones in the previous list?  If there are subchannels that exist in both lists, the refcounts should be taken care of automatically, without any need to do manual work here.\n\nNote that `grpc_client_channel_factory_create_subchannel()` returns a new reference to every subchannel it returns, regardless of whether it created a new one or returned an existing one from the subchannel index, so if the subchannel exists in both lists, its refcount will be increased automatically when we create the new subchannel_data struct and decreased when we shut down the old subchannel_data struct.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 671 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4QVWV6wRpkimiq70-:-Kl4QVWV6wRpkimiq700:b8k41bf) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L671)):*\n> ```C\n>   for (size_t i = 0; i < p->num_subchannels; ++i) {\n>     subchannel_data *sd = p->subchannels[i];\n>     if (sd->subchannel != NULL) {\n> ```\n\nIf we don't invoke the connectivity callback when the subchannel is NULL, then how will the subchannel_data struct get freed?  It seems like this will result in a memory leak.\n\nI suspect that this would be easier if we had a separate struct that contained an array of subchannel_data structs, rather than allocating each subchannel_data struct individually, as I described above.  That way, we could refcount the overall struct instead of the individual subchannel_data structs, which would mean that when a subchannel shuts down, we could immediately decrement the refcount on the overall struct, thus avoiding the need to deal with that case here.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 676 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4QBejGU2gYFmeSnbq:-Kl4QBekMuCd5lrCN1IX:bx5wbja) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L676)):*\n> ```C\n>                 (void *)sd->subchannel);\n>       }\n>       if (gpr_unref(&sd->refcount)) {\n> ```\n\nThere's probably no need to refcount the individual subchannel_data structs.  As mentioned above, I think we can just recreate the subchannel_data structs for the elements that overlap, in which case we know unconditionally that we want to shut down all of the elements of the old list.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 693 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4RXxEcOplXMdMyOcX:-Kl4RXxEcOplXMdMyOcY:b-eg2o20) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L693)):*\n> ```C\n>   }\n> \n>   if (subchannel_index > 0) {\n> ```\n\nIf we get an empty update, don't we want to use that instead of failing?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 695 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4RvU8tGRqz5fw0eI-:-Kl4RvU8tGRqz5fw0eI0:bfux1x3) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L695)):*\n> ```C\n>   if (subchannel_index > 0) {\n>     if (p->subchannels != NULL) gpr_free(p->subchannels);\n>     p->subchannels = update_subchannels;\n> ```\n\nI think we should wait for at least one subchannel on the new list to be READY before we switch over to the new list.  See my comment above about having a separate struct to contain the list of subchannel_data structs.\n\n---\n\n*[src/core/ext/filters/client_channel/resolver/fake/fake_resolver.h, line 39 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4A9Cktx7XTtpN50Ba:-Kl4A9Cktx7XTtpN50Bb:b-vk26ek) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/resolver/fake/fake_resolver.h#L39)):*\n> ```objc\n> #include \"src/core/lib/channel/channel_args.h\"\n> \n> #include \"test/core/util/test_config.h\"\n> ```\n\nShould we be depending on something in test/ from code in src/ ?\n\n---\n\n*[src/core/lib/security/transport/lb_targets_info.c, line 48 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklCT5suc4mNcwvPryJ:-Kl4AQhBqAuIyRp_axtw:bft35xc) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/security/transport/lb_targets_info.c#L48)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nNo. We don't care about sorting order in this case. And even if we did, the only consequence is that the \"sorted\" result would cluster equal elements together. \n\nThere's no simple way to define a comparison between tables. Even if we wanted to (something we don't currently need), it'd necessitate adding extra methods to the slice_hash_table API, for more (unnecessary) complexity.\n</blockquote></details>\n\nThe channel_args vtable uses a cmp function, not an equality function, which implies that it does actually care about sorting.  Even if there's no code that currently does sorting, if someone were to add that later, they would look at this vtable, see the cmp function, and assume that they already have what they need to do sorting.  Given that, I think we should be consistent here.\n\nI don't think there's any significant increased complexity here.  We're already adding a bunch of \"equals\" methods; all we need to do is make them \"cmp\" methods instead.\n\n---\n\n*[test/core/slice/slice_hash_table_test.c, line 164 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4BcH02UpWjOST-GoP:-Kl4BcH02UpWjOST-GoQ:bnagnyd) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/core/slice/slice_hash_table_test.c#L164)):*\n> ```C\n>   GPR_ASSERT(grpc_slice_hash_table_eq(table_a, table_b));\n>   grpc_exec_ctx exec_ctx = GRPC_EXEC_CTX_INIT;\n>   grpc_slice_hash_table_unref(&exec_ctx, table_a);\n> ```\n\nWhy don't we need to unref the tables?\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 254 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4gRqDtlIkFMVzWqYI:-Kl4gRqDtlIkFMVzWqYJ:b-hiep4r) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/test/cpp/end2end/client_lb_end2end_test.cc#L254)):*\n> ```cpp\n> }\n> \n> TEST_F(ClientLbEnd2endTest, PickFirstMany) {\n> ```\n\nSuggest calling this `PickFirstManyUpdates`.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 269 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4gac-vVfPSX7d9oDA:-Kl4gac085frnYiMIjGZ:b-v9wp67) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/test/cpp/end2end/client_lb_end2end_test.cc#L269)):*\n> ```cpp\n>   }\n> \n>   // for (size_t i = 0; i < servers_.size(); ++i) {\n> ```\n\nThis commented code is probably not needed.\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159#-:-Kl4iB3DAydxT9qZHH29:bh6emg5)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/304344848",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-304344848",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 304344848,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDM0NDg0OA==",
    "user": {
      "login": "dgquintas",
      "id": 120217,
      "node_id": "MDQ6VXNlcjEyMDIxNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/120217?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dgquintas",
      "html_url": "https://github.com/dgquintas",
      "followers_url": "https://api.github.com/users/dgquintas/followers",
      "following_url": "https://api.github.com/users/dgquintas/following{/other_user}",
      "gists_url": "https://api.github.com/users/dgquintas/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dgquintas/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dgquintas/subscriptions",
      "organizations_url": "https://api.github.com/users/dgquintas/orgs",
      "repos_url": "https://api.github.com/users/dgquintas/repos",
      "events_url": "https://api.github.com/users/dgquintas/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dgquintas/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-26T17:39:15Z",
    "updated_at": "2017-05-26T17:39:15Z",
    "author_association": "NONE",
    "body": "\n\n\n\nReview status: 56 of 57 files reviewed at latest revision, 22 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy_factory.h, line 121 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4A0eNo32uYd6MFOei:-Kl4k1Kles09eQhkU6rx:b-m36un4) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy_factory.h#L121)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nWhy is this no longer a typedef?\n</blockquote></details>\n\nBecause we have a \"forward declaration\" in lb_policy.h (new update function references grpc_lb_policy_args*). If this were a typedef, there'd be a double definition for the type.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 986 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkR-PNlu7hyTxlzVlW9:-Kl4kDyDryJ40p1AFgGK:b-t9h8bq) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L986)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nOkay, thanks, I think I get it now.\n\nIs it safe to destroy the channel while a call on that channel still exists?\n</blockquote></details>\n\nYes. It triggers a cancellation on the call.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 428 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4KZAard0nZsALSdsP:-Kl4karxM4y_JDg6Ra35:b-2xg7b5) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L428)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>       // Note that sd->subchannel could be NULL if the subchannel had shutdown\n>       // between the setting of sd->shutting_down and the invocation of this\n>       // callback. In that case, sd->user_data is already destroyed.\n\nHow can this happen?  The code that sets sd->subchannel to NULL is right here in this callback, and both the code that sets sd->shutting_down and this callback run in the same combiner.\n</blockquote></details>\n\nA server closes the connection, this callback is invoked, it goes to the SHUTDOWN branch and sd->subchannel is set to NULL. The policy then wants to, for example, shutdown: it unsubscribes and sets sd->shutting_down. At that point it'd go through here and find sd->subchannel being NULL.\n\nBasically the state of the subchannel isn't fully controlled by the client.\n\n---\n\n*[src/core/ext/filters/client_channel/resolver/fake/fake_resolver.h, line 39 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4A9Cktx7XTtpN50Ba:-Kl4pFgtWqnPJj-W9ux3:b-pyrbqy) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/resolver/fake/fake_resolver.h#L39)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nShould we be depending on something in test/ from code in src/ ?\n</blockquote></details>\n\nNo. Removed.\n\n---\n\n*[src/core/lib/security/transport/lb_targets_info.c, line 48 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklCT5suc4mNcwvPryJ:-Kl4nK81Pk6Hj8rT0BBi:b12ksi0) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/security/transport/lb_targets_info.c#L48)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThe channel_args vtable uses a cmp function, not an equality function, which implies that it does actually care about sorting.  Even if there's no code that currently does sorting, if someone were to add that later, they would look at this vtable, see the cmp function, and assume that they already have what they need to do sorting.  Given that, I think we should be consistent here.\n\nI don't think there's any significant increased complexity here.  We're already adding a bunch of \"equals\" methods; all we need to do is make them \"cmp\" methods instead.\n</blockquote></details>\n\nFair enough. My point about complexity isn't about cmp or equals methods, but rather about extending the slice_hash_tables API to at a minimum provide a get_size() function to allow a minimally meaningful ordering between tables.\n\n---\n\n*[test/core/slice/slice_hash_table_test.c, line 164 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4BcH02UpWjOST-GoP:-Kl4p20ZHOfUd1NkYOv7:bycmfg4) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/test/core/slice/slice_hash_table_test.c#L164)):*\n>   grpc_exec_ctx exec_ctx = GRPC_EXEC_CTX_INIT;\n>   grpc_slice_hash_table_unref(&exec_ctx, table_a);\n>   grpc_slice_hash_table_unref(&exec_ctx, table_b);\n>   grpc_slice_hash_table_unref(&exec_ctx, table_c);\n>   grpc_slice_hash_table_unref(&exec_ctx, table_d);\n>   grpc_exec_ctx_finish(&exec_ctx);\nWe do. The git repo on my laptop was out of sync. Restored.\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/304353075",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-304353075",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 304353075,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDM1MzA3NQ==",
    "user": {
      "login": "grpc-kokoro",
      "id": 12722905,
      "node_id": "MDQ6VXNlcjEyNzIyOTA1",
      "avatar_url": "https://avatars3.githubusercontent.com/u/12722905?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/grpc-kokoro",
      "html_url": "https://github.com/grpc-kokoro",
      "followers_url": "https://api.github.com/users/grpc-kokoro/followers",
      "following_url": "https://api.github.com/users/grpc-kokoro/following{/other_user}",
      "gists_url": "https://api.github.com/users/grpc-kokoro/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/grpc-kokoro/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/grpc-kokoro/subscriptions",
      "organizations_url": "https://api.github.com/users/grpc-kokoro/orgs",
      "repos_url": "https://api.github.com/users/grpc-kokoro/repos",
      "events_url": "https://api.github.com/users/grpc-kokoro/events{/privacy}",
      "received_events_url": "https://api.github.com/users/grpc-kokoro/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-26T18:15:15Z",
    "updated_at": "2017-05-26T18:15:15Z",
    "author_association": "NONE",
    "body": "```\nNo significant performance differences\n```"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/304353624",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-304353624",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 304353624,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDM1MzYyNA==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-26T18:17:46Z",
    "updated_at": "2017-05-26T18:17:46Z",
    "author_association": "MEMBER",
    "body": "\n\n\n\nReviewed 1 of 56 files at r1, 2 of 2 files at r7.\nReview status: 56 of 57 files reviewed at latest revision, 19 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy_factory.h, line 121 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4A0eNo32uYd6MFOei:-Kl4uPrpy4jCdUxTWCOp:bx4ta6b) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy_factory.h#L121)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nBecause we have a \"forward declaration\" in lb_policy.h (new update function references grpc_lb_policy_args*). If this were a typedef, there'd be a double definition for the type.\n</blockquote></details>\n\nAh, I see.  In that case, I retract my earlier suggestion: It probably would make sense to move this to lb_policy.h.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 428 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4KZAard0nZsALSdsP:-Kl4uaI1igwVkWXYkzQI:b-t2eh61) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L428)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nA server closes the connection, this callback is invoked, it goes to the SHUTDOWN branch and sd->subchannel is set to NULL. The policy then wants to, for example, shutdown: it unsubscribes and sets sd->shutting_down. At that point it'd go through here and find sd->subchannel being NULL.\n\nBasically the state of the subchannel isn't fully controlled by the client.\n</blockquote></details>\n\nThe way that we get to this callback is via `grpc_subchannel_notify_on_state_change()`, and that function needs to be passed a subchannel.  Once the subchannel is set to NULL, we can't possibly get back into this function.\n\n---\n\n*[src/core/lib/security/transport/lb_targets_info.c, line 48 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklCT5suc4mNcwvPryJ:-Kl4yO_wWiuvjJSpyhoO:b-nx7ukq) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/security/transport/lb_targets_info.c#L48)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nFair enough. My point about complexity isn't about cmp or equals methods, but rather about extending the slice_hash_tables API to at a minimum provide a get_size() function to allow a minimally meaningful ordering between tables.\n</blockquote></details>\n\nI don't think we need to expose a get_size() method.  The size can be checked directly by `grpc_slice_hash_table_cmp()`.\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/304358326",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-304358326",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 304358326,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDM1ODMyNg==",
    "user": {
      "login": "dgquintas",
      "id": 120217,
      "node_id": "MDQ6VXNlcjEyMDIxNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/120217?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dgquintas",
      "html_url": "https://github.com/dgquintas",
      "followers_url": "https://api.github.com/users/dgquintas/followers",
      "following_url": "https://api.github.com/users/dgquintas/following{/other_user}",
      "gists_url": "https://api.github.com/users/dgquintas/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dgquintas/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dgquintas/subscriptions",
      "organizations_url": "https://api.github.com/users/dgquintas/orgs",
      "repos_url": "https://api.github.com/users/dgquintas/repos",
      "events_url": "https://api.github.com/users/dgquintas/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dgquintas/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-26T18:38:32Z",
    "updated_at": "2017-05-26T18:38:32Z",
    "author_association": "NONE",
    "body": "\n\n\n\nReview status: 56 of 57 files reviewed at latest revision, 19 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy_factory.h, line 121 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4A0eNo32uYd6MFOei:-Kl52A8bf2AczKLDzXzo:b9cw3s1) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy_factory.h#L121)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nAh, I see.  In that case, I retract my earlier suggestion: It probably would make sense to move this to lb_policy.h.\n</blockquote></details>\n\nI actually think this is fine, nothing wrong with forward declarations. \n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/305614556",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-305614556",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 305614556,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTYxNDU1Ng==",
    "user": {
      "login": "dgquintas",
      "id": 120217,
      "node_id": "MDQ6VXNlcjEyMDIxNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/120217?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dgquintas",
      "html_url": "https://github.com/dgquintas",
      "followers_url": "https://api.github.com/users/dgquintas/followers",
      "following_url": "https://api.github.com/users/dgquintas/following{/other_user}",
      "gists_url": "https://api.github.com/users/dgquintas/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dgquintas/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dgquintas/subscriptions",
      "organizations_url": "https://api.github.com/users/dgquintas/orgs",
      "repos_url": "https://api.github.com/users/dgquintas/repos",
      "events_url": "https://api.github.com/users/dgquintas/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dgquintas/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-01T20:43:21Z",
    "updated_at": "2017-06-01T20:43:21Z",
    "author_association": "NONE",
    "body": "Thanks for the very in-depth review. I believe I've addressed all the comments. PTAL.\n\n---\n\nReview status: 45 of 59 files reviewed at latest revision, 19 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 443 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kkl8cWf7fBJnEZljdAv:-Kl_K5sRPLiw_OE8z9zq:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/ext/filters/client_channel/client_channel.c#L443)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIt looks like the only thing we're using the return status for is to decide if we should check the connectivity state, and (as per my comment below) I'm not sure we actually need to do that.  If not, we can probably just change this to return void.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/client_channel.c, line 447 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4D-uiDrDn3CrisCNX:-Kl_K76hQW21RQQB2D2N:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/client_channel.c#L447)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nHmm... Do we actually need to do this?\n\nPrior to this PR, the code here would always create a new LB policy, and it would call `grpc_lb_policy_check_connectivity_locked()` to get the policy's initial state.  Then the code down on line 538 would set the client channel's initial connectivity state and start a watch on the LB policy.\n\nHowever, in the update case, we are not replacing the LB policy instance, which means that the watch we created when we first instantiated the LB policy should still be in effect.  Given that, I think we don't need to get the state, because it's already being reported by the existing watch.  We also don't need to start the watch, because it's already in flight.  (In fact, the way this code is right now, it's probably a bug that we're calling `watch_lb_policy_locked()` in the update case, since that callback is already being used for the same watch.)\n\nSo, I think we should (a) not call `grpc_lb_policy_check_connectivity_state()` here, and (b) skip the block on line 538 in the update case.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1818 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4ec6AJRUvZiGxdk2U:-Kl_K9use0cna-VIfsAz:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1818)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nSame comment as in pick_first: if `pending_update_args` is set, `glb_update_locked()` will first free it and then use the passed-in `args`.  However, in this case, `args` is equal to `glb_policy->pending_update_args`, so it will be a use-after-free bug.  I think we need to reset `glb_policy->pending_update_args` to NULL before calling `glb_update_locked()`.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 534 at r1](https://reviewable.io:443/reviews/grpc/grpc/11159#-KkNHQee2R9g926yRAYA:-Kl_MwYjR8YLiVPyJD5i:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/367a9ad1efd1f23c9c27dd4011c8846b5f5a2cef/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L534)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nAs per my comment in client_channel.c, I think this may not be an issue.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 355 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4aY_GnX6hUSIVJ4vf:-Kl_MqsEQEUYdde3aLAy:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L355)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nDon't we also need to call `grpc_channel_args_destroy(p->pending_update_args->args)`?\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 355 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4bNBRb8gKvCno4ohT:-Kl_Mt4V3eh0p06xVVCs:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L355)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIf we're called from `pf_connectivity_changed_locked()`, then won't `p->pending_update_args` and `args` be the same thing?  If so, this will free the struct and then try copying from it.\n\nI think we should change `pf_connectivity_changed_locked()` to do something like this:\n\n```\ngrpc_lb_policy_args *args = p->pending_update_args;\np->pending_update_args = NULL;\npf_update_locked(exec_ctx, p, args);\n```\n\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 475 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4XQ78lXgLQbAX4NEw:-Kl_MuwYzAjBWCVYtm2h:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L475)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nMight not be necessary, as per my comment in client_channel.c.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 428 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4KZAard0nZsALSdsP:-Kl_N1ucu7-y3BBaY1N3:b-rpxv55) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L428)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThe way that we get to this callback is via `grpc_subchannel_notify_on_state_change()`, and that function needs to be passed a subchannel.  Once the subchannel is set to NULL, we can't possibly get back into this function.\n</blockquote></details>\n\nYou were right. It's not applicable anymore regardless.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 437 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4L9Hki-WKEPVTWlB4:-Kl_N6s7LmwI3HU9Or6b:bqi6y0s) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L437)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>     // If sd->shutting_down is true, the subchannel has already\n>     // been unsubscribed from connectivity updates: it's safe to free \"sd\".\n\nIt's not entirely accurate to say that it has already been unsubscribed.  Actually, this callback is the result of that subscription, and the very last thing it does is to renew the subscription.  So it would probably be more accurate to say \"If sd->shutting_down is true, we are not going to renew the connectivity update subscription, so it's safe to free sd.\"\n</blockquote></details>\n\nN/A anymore\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 460 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4SMoSWY8X3Ge9FP2g:-Kl_N8lrbKry9HHlnPiA:b-sm213l) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L460)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>   update_state_counters_locked(sd);\n>   sd->prev_connectivity_state = sd->curr_connectivity_state;\n>   grpc_connectivity_state new_connectivity_state =\n>       update_lb_connectivity_status_locked(exec_ctx, sd, GRPC_ERROR_REF(error));\n\nI think this aggregation code won't work exactly right while an update is in flight.  In particular, we need to make sure that the counters in `update_state_counters_locked()` will reflect whichever subchannel list is currently in use (see my comment below about not switching to the new list until at least one subchannel is in READY state).\n\nI suspect that this will be a lot easier if we had a separate struct to contain the list of subchannel_data structs.  For example:\n\n```\ntypedef struct {\n  /** backpointer to owning policy */\n  round_robin_lb_policy *policy;\n\n    /** total number of addresses received at creation time */\n  size_t num_addresses;\n\n  /** all our subchannels */\n  size_t num_subchannels;\n  subchannel_data *subchannels;\n\n  /** how many subchannels are in state READY */\n  size_t num_ready;\n  /** how many subchannels are in state TRANSIENT_FAILURE */\n  size_t num_transient_failures;\n  /** how many subchannels are in state IDLE */\n  size_t num_idle;\n\n  /** There will be one ref for each entry in subchannels for which there is a pending\n   * connectivity state watcher callback. */\n  gpr_refcount refs;\n\n  bool shutdown;\n} rr_subchannel_list;\n```\n\nThe round_robin_lb_policy struct would contain a pointer to this struct instead of directly containing the fields in this struct.  The subchannel_data struct would have a backpointer to the rr_subchannel_list that contains it, rather than a backpointer directly to the round_robin_lb_policy itself.\n\nThe connectivity state callback would do the aggregation across only the subchannels within the rr_subchannel_list.  When the aggregate state goes to READY and the subchannel list is not being shut down, we can swap out the rr_subchannel_list held by the RR policy, update the policy's connectivity state, and shut down the old rr_subchannel_list (if any).\n</blockquote></details>\n\nDone. Very true.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 572 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4LsB3ie1Cy1-0kTmB:-Kl_NBB_wj3CV3cUsIGF:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L572)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nNit: Please remove blank lines within functions.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 612 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4NIXta4DW8cPAEwP1:-Kl_NCAN_KtmWsoRumBW:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L612)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>       // Look for subchannel in current p->subchannels to increase its refcount.\n>       subchannel_data *curr_sd = NULL;\n>       for (size_t j = 0; j < p->num_subchannels; ++j) {\n>         if (p->subchannels[j]->subchannel == subchannel) {\n>           curr_sd = p->subchannels[j];\n>           gpr_ref(&curr_sd->refcount);\n>           break;  // there should be only one.\n>         }\n>       }\n>       if (curr_sd != NULL) {\n>         update_subchannels[subchannel_index] = curr_sd;\n>         if (GRPC_TRACER_ON(grpc_lb_round_robin_trace)) {\n>           char *address_uri =\n>               grpc_sockaddr_to_uri(&addresses->addresses[i].address);\n>           gpr_log(GPR_DEBUG, \"Reusing subchannel %p for address uri %s\",\n>                   (void *)subchannel, address_uri);\n>           gpr_free(address_uri);\n>         }\n>         // Discount the subchannel ref from the unnecessary creation in step 1\n>         // of the subchannel being resused.\n>         GRPC_SUBCHANNEL_UNREF(exec_ctx, subchannel, \"rr_update_sc_known\");\n>       }\n\nWhy is any of this necessary?  Why not simply create a new subchannel_data struct for each address in the update list, regardless of whether or not it exists in the previous list, and then unref the ones in the previous list?  If there are subchannels that exist in both lists, the refcounts should be taken care of automatically, without any need to do manual work here.\n\nNote that `grpc_client_channel_factory_create_subchannel()` returns a new reference to every subchannel it returns, regardless of whether it created a new one or returned an existing one from the subchannel index, so if the subchannel exists in both lists, its refcount will be increased automatically when we create the new subchannel_data struct and decreased when we shut down the old subchannel_data struct.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 671 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4QVWV6wRpkimiq70-:-Kl_O6VUKEWji-WOBBw9:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L671)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIf we don't invoke the connectivity callback when the subchannel is NULL, then how will the subchannel_data struct get freed?  It seems like this will result in a memory leak.\n\nI suspect that this would be easier if we had a separate struct that contained an array of subchannel_data structs, rather than allocating each subchannel_data struct individually, as I described above.  That way, we could refcount the overall struct instead of the individual subchannel_data structs, which would mean that when a subchannel shuts down, we could immediately decrement the refcount on the overall struct, thus avoiding the need to deal with that case here.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 676 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4QBejGU2gYFmeSnbq:-Kl_O8cHOtoDKmY2FMYw:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L676)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThere's probably no need to refcount the individual subchannel_data structs.  As mentioned above, I think we can just recreate the subchannel_data structs for the elements that overlap, in which case we know unconditionally that we want to shut down all of the elements of the old list.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 693 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4RXxEcOplXMdMyOcX:-Kl_OAIKVmmPz1ABtZIp:byvyago) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L693)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIf we get an empty update, don't we want to use that instead of failing?\n</blockquote></details>\n\nYes. Done.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 695 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4RvU8tGRqz5fw0eI-:-Kl_ODcbildlu2S-EgbJ:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L695)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nI think we should wait for at least one subchannel on the new list to be READY before we switch over to the new list.  See my comment above about having a separate struct to contain the list of subchannel_data structs.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/lib/security/transport/lb_targets_info.c, line 48 at r2](https://reviewable.io:443/reviews/grpc/grpc/11159#-KklCT5suc4mNcwvPryJ:-Kl_OG_BPU8cGdUCbxJw:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/f81bca7984b9a583cbb899de23d37c3b726e280a/src/core/lib/security/transport/lb_targets_info.c#L48)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nI don't think we need to expose a get_size() method.  The size can be checked directly by `grpc_slice_hash_table_cmp()`.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 254 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4gRqDtlIkFMVzWqYI:-Kl_OItGsU7cEuLREKTc:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/test/cpp/end2end/client_lb_end2end_test.cc#L254)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nSuggest calling this `PickFirstManyUpdates`.\n</blockquote></details>\n\nDone.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 269 at r5](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kl4gac-vVfPSX7d9oDA:-Kl_OJNwc7h4UnDgJxZ1:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/e5dad07837ccc78bfbfb53fe9b2d4d4fa86c52aa/test/cpp/end2end/client_lb_end2end_test.cc#L269)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThis commented code is probably not needed.\n</blockquote></details>\n\nDone.\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159#-:-Kl_OMJ3iEsh456d82pR:bb7gn0r)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/305620738",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-305620738",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 305620738,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTYyMDczOA==",
    "user": {
      "login": "grpc-kokoro",
      "id": 12722905,
      "node_id": "MDQ6VXNlcjEyNzIyOTA1",
      "avatar_url": "https://avatars3.githubusercontent.com/u/12722905?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/grpc-kokoro",
      "html_url": "https://github.com/grpc-kokoro",
      "followers_url": "https://api.github.com/users/grpc-kokoro/followers",
      "following_url": "https://api.github.com/users/grpc-kokoro/following{/other_user}",
      "gists_url": "https://api.github.com/users/grpc-kokoro/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/grpc-kokoro/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/grpc-kokoro/subscriptions",
      "organizations_url": "https://api.github.com/users/grpc-kokoro/orgs",
      "repos_url": "https://api.github.com/users/grpc-kokoro/repos",
      "events_url": "https://api.github.com/users/grpc-kokoro/events{/privacy}",
      "received_events_url": "https://api.github.com/users/grpc-kokoro/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-01T21:07:27Z",
    "updated_at": "2017-06-01T21:07:27Z",
    "author_association": "NONE",
    "body": "```\nNo significant performance differences\n```"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/305648099",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-305648099",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 305648099,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTY0ODA5OQ==",
    "user": {
      "login": "grpc-kokoro",
      "id": 12722905,
      "node_id": "MDQ6VXNlcjEyNzIyOTA1",
      "avatar_url": "https://avatars3.githubusercontent.com/u/12722905?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/grpc-kokoro",
      "html_url": "https://github.com/grpc-kokoro",
      "followers_url": "https://api.github.com/users/grpc-kokoro/followers",
      "following_url": "https://api.github.com/users/grpc-kokoro/following{/other_user}",
      "gists_url": "https://api.github.com/users/grpc-kokoro/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/grpc-kokoro/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/grpc-kokoro/subscriptions",
      "organizations_url": "https://api.github.com/users/grpc-kokoro/orgs",
      "repos_url": "https://api.github.com/users/grpc-kokoro/repos",
      "events_url": "https://api.github.com/users/grpc-kokoro/events{/privacy}",
      "received_events_url": "https://api.github.com/users/grpc-kokoro/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-01T23:28:13Z",
    "updated_at": "2017-06-01T23:28:13Z",
    "author_association": "NONE",
    "body": "```\nNo significant performance differences\n```"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/305666376",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-305666376",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 305666376,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTY2NjM3Ng==",
    "user": {
      "login": "grpc-kokoro",
      "id": 12722905,
      "node_id": "MDQ6VXNlcjEyNzIyOTA1",
      "avatar_url": "https://avatars3.githubusercontent.com/u/12722905?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/grpc-kokoro",
      "html_url": "https://github.com/grpc-kokoro",
      "followers_url": "https://api.github.com/users/grpc-kokoro/followers",
      "following_url": "https://api.github.com/users/grpc-kokoro/following{/other_user}",
      "gists_url": "https://api.github.com/users/grpc-kokoro/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/grpc-kokoro/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/grpc-kokoro/subscriptions",
      "organizations_url": "https://api.github.com/users/grpc-kokoro/orgs",
      "repos_url": "https://api.github.com/users/grpc-kokoro/repos",
      "events_url": "https://api.github.com/users/grpc-kokoro/events{/privacy}",
      "received_events_url": "https://api.github.com/users/grpc-kokoro/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-02T01:44:01Z",
    "updated_at": "2017-06-02T01:44:01Z",
    "author_association": "NONE",
    "body": "```\nNo significant performance differences\n```"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/305838333",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-305838333",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 305838333,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTgzODMzMw==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-02T16:26:48Z",
    "updated_at": "2017-06-02T16:26:48Z",
    "author_association": "MEMBER",
    "body": "This is looking really good!  There's only one substantive comment left, which is the one about making sure we get the latest update in round_robin.\n\n---\n\nReviewed 1 of 56 files at r1, 10 of 14 files at r8, 23 of 24 files at r9, 6 of 6 files at r10.\nReview status: all files reviewed at latest revision, 12 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1751 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kldbhnl-7_LU5n9bn9k:-Kldbhnl-7_LU5n9bn9l:bqb5kiw) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1751)):*\n> ```C\n>       grpc_channel_args_find(args->args, GRPC_ARG_LB_ADDRESSES);\n>   if (arg == NULL || arg->type != GRPC_ARG_POINTER) {\n>     grpc_connectivity_state_set(\n> ```\n\nSame comment as in round_robin -- we should probably treat this as an empty list.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 290 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldacZQUnyC6lgvg-Xe:-KldacZRrUPmVi6_vKMV:b-4ab7ce) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L290)):*\n> ```C\n>       grpc_channel_args_find(args->args, GRPC_ARG_LB_ADDRESSES);\n>   if (arg == NULL || arg->type != GRPC_ARG_POINTER) {\n>     grpc_connectivity_state_set(\n> ```\n\nSame comment as in round_robin -- we probably want to treat this as an empty list.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 306 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldahyxG_fbB2ixW_xe:-KldahyxG_fbB2ixW_xf:b2s8g2l) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L306)):*\n> ```C\n>         GRPC_ERROR_CREATE_FROM_STATIC_STRING(\"Empty update\"),\n>         \"pf_update_empty\");\n>     return;\n> ```\n\nSame comment as in round_robin -- we probably want to unref the existing subchannels in this case.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 405 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kldb2p8Ubct0456hQ-o:-Kldb2p8Ubct0456hQ-p:bn7ho8b) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L405)):*\n> ```C\n>         GRPC_ERROR_CREATE_FROM_STATIC_STRING(\"No valid addresses in update\"),\n>         \"pf_update_no_valid_addresses\");\n>     return;\n> ```\n\nSame here -- probably need to unref the existing subchannels.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 159 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldRyPjfZqzit2BFe7i:-KldRyPjfZqzit2BFe7j:blxaxcc) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L159)):*\n>   for (size_t i = 0; i < subchannel_list->num_subchannels; i++) {\n<details><summary><i alt=\"summary\">Quoted 10 lines of code…</i></summary>\n>     subchannel_data *sd = &subchannel_list->subchannels[i];\n>     if (sd->subchannel != NULL) {\n>       GRPC_SUBCHANNEL_UNREF(exec_ctx, sd->subchannel,\n>                             \"rr_subchannel_list_destroy\");\n>     }\n>     sd->subchannel = NULL;\n>     if (sd->user_data != NULL) {\n>       GPR_ASSERT(sd->user_data_vtable != NULL);\n>       sd->user_data_vtable->destroy(exec_ctx, sd->user_data);\n>     }\n</details>\n>   }\n\nWould it make more sense to do this for each individual subchannel in `rr_connectivity_changed_locked()` instead of here, so that we don't have to go through the loop here?\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 199 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldOoDJs0XE9cPfP7en:-KldOoDJs0XE9cPfP7eo:brcoo87) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L199)):*\n> ```C\n>     rr_subchannel_list_destroy(exec_ctx, subchannel_list);\n>   }\n>   return;\n> ```\n\nNot needed, since this is the end of the function.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 515 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldZwlQpUqt59K-KDId:-KldZwlQpUqt59K-KDIe:b5jp8rf) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L515)):*\n> ```C\n>     return;\n>   }\n> ```\n\nNit: Please remove blank lines within functions.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 566 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kld_2WwLjVdqkPP7lKH:-Kld_2WwLjVdqkPP7lKI:b5f6ord) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L566)):*\n> ```C\n>   } else {  // sd not in SHUTDOWN\n>     if (sd->curr_connectivity_state == GRPC_CHANNEL_READY) {\n>       // promote sd->subchannel_list to p->subchannel_list\n> ```\n\nWhat happens if we get two updates in quick succession?  How do we ensure that the second one is the one that we wind up using, given that the connectivity callbacks for the first list could come in *after* the callbacks for the second list?\n\nOne possible solution (although there may be alternatives) would be to add a `pending_subchannel_list` member to the RR policy, which we set to the new list whenever we get an update.  When the callback comes in, we can check whether `sd->subchannel_list` is equal to `p->subchannel_list` or `p->pending_subchannel_list` (or neither) to decide how to handle it.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 666 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldUnVk8egMnPBdm1M1:-KldUnVk8egMnPBdm1M2:b-n3461r) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L666)):*\n>   if (arg == NULL || arg->type != GRPC_ARG_POINTER) {\n<details><summary><i alt=\"summary\">Quoted 5 lines of code…</i></summary>\n>     grpc_connectivity_state_set(\n>         exec_ctx, &p->state_tracker, GRPC_CHANNEL_TRANSIENT_FAILURE,\n>         GRPC_ERROR_CREATE_FROM_STATIC_STRING(\"Missing update in args\"),\n>         \"rr_update_missing\");\n>     return;\n</details>\n>   }\n\nSetting the connectivity state to TRANSIENT_FAILURE doesn't seem like quite the right thing to do, since we're going to continue using the previous address list.\n\nIt might be better to just log an error and treat this as an empty address list.\n\n(I also wouldn't object to just making this an assertion, but you had changed this to avoid using an assertion back in #9838, so I assume that we need to continue to avoid that.)\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 683 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldVFuRgr98Gfsj8h7i:-KldVFuRgr98Gfsj8h7j:b-akw2wb) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L683)):*\n> ```C\n>         GRPC_ERROR_CREATE_FROM_STATIC_STRING(\"Empty update\"),\n>         \"rr_update_empty\");\n>     return;\n> ```\n\nDon't we also want to clear the existing subchannel list?  There's no reason to continue holding refs to those subchannels if we're supposed to be switching to an empty list.\n\n---\n\n*[src/core/lib/slice/slice_hash_table.c, line 144 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldNCPznNCZSBHnOD5e:-KldNCPznNCZSBHnOD5f:bnrgced) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/lib/slice/slice_hash_table.c#L144)):*\n<details><summary><i alt=\"summary\">Quoted 12 lines of code…</i></summary>\n>     const gpr_slice a_key =\n>         is_empty(&a->entries[i]) ? grpc_empty_slice() : a->entries[i].key;\n>     const gpr_slice b_key =\n>         is_empty(&b->entries[i]) ? grpc_empty_slice() : b->entries[i].key;\n>     const int key_cmp = grpc_slice_cmp(a_key, b_key);\n>     if (key_cmp != 0) return key_cmp;\n>     // keys can be equal because both entries are empty. In that case, continue\n>     // (no values for empty entries).\n>     if (GRPC_SLICE_IS_EMPTY(a_key) && GRPC_SLICE_IS_EMPTY(b_key)) continue;\n>     const int value_cmp =\n>         value_cmp_fn_a(a->entries[i].value, b->entries[i].value);\n>     if (value_cmp != 0) return value_cmp;\n</details>\n\nI don't think there's anything stopping someone from using an empty slice as a real key, in which case this might falsely compare as equal when it shouldn't.  I suggest explicitly checking whether the entries are empty first (which I think is also a bit clearer to read):\n\n```\nif (is_empty(&a->entries[i])) {\n  if (!is_empty(&b->entries[i])) {\n    return -1;  // a empty but b non-empty\n  }\n  continue;  // both empty, no need to check key or value\n} else if (is_empty(&b->entries[i])) {\n  return 1;  // a non-empty but b empty\n}\n// neither empty\nconst int key_cmp = grpc_slice_cmp(a->entries[i].key, b->entries[i].key);\nif (key_cmp != 0) return key_cmp;\nconst int value_cmp = value_cmp_fn_a(a->entries[i].value, b->entries[i].value);\nif (value_cmp != 0) return value_cmp;\n```\n\n---\n\n*[src/core/lib/slice/slice_hash_table.h, line 80 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldLEEJSUvnLeuvBbOC:-KldLEEJSUvnLeuvBbOD:b-h63vlm) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/lib/slice/slice_hash_table.h#L80)):*\n> ```objc\n>  *  - GPR_ICMP(a->value_cmp, b->value_cmp) < 1 (resp. > 1),\n>  *  - else, it contains fewer (resp. more) entries,\n>  *  - else, if (a_key, a_value) < (b_key, b_value) (resp >), which is defined as\n> ```\n\nWhy are `a_value` and `b_value` mentioned in this bullet?\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159#-:-KldcDy7JUOdTaXvKv0-:byq3bu1)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/305923062",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-305923062",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 305923062,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTkyMzA2Mg==",
    "user": {
      "login": "grpc-kokoro",
      "id": 12722905,
      "node_id": "MDQ6VXNlcjEyNzIyOTA1",
      "avatar_url": "https://avatars3.githubusercontent.com/u/12722905?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/grpc-kokoro",
      "html_url": "https://github.com/grpc-kokoro",
      "followers_url": "https://api.github.com/users/grpc-kokoro/followers",
      "following_url": "https://api.github.com/users/grpc-kokoro/following{/other_user}",
      "gists_url": "https://api.github.com/users/grpc-kokoro/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/grpc-kokoro/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/grpc-kokoro/subscriptions",
      "organizations_url": "https://api.github.com/users/grpc-kokoro/orgs",
      "repos_url": "https://api.github.com/users/grpc-kokoro/repos",
      "events_url": "https://api.github.com/users/grpc-kokoro/events{/privacy}",
      "received_events_url": "https://api.github.com/users/grpc-kokoro/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-02T22:25:47Z",
    "updated_at": "2017-06-02T22:25:47Z",
    "author_association": "NONE",
    "body": "```\nNo significant performance differences\n```"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/305932139",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-305932139",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 305932139,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTkzMjEzOQ==",
    "user": {
      "login": "grpc-kokoro",
      "id": 12722905,
      "node_id": "MDQ6VXNlcjEyNzIyOTA1",
      "avatar_url": "https://avatars3.githubusercontent.com/u/12722905?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/grpc-kokoro",
      "html_url": "https://github.com/grpc-kokoro",
      "followers_url": "https://api.github.com/users/grpc-kokoro/followers",
      "following_url": "https://api.github.com/users/grpc-kokoro/following{/other_user}",
      "gists_url": "https://api.github.com/users/grpc-kokoro/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/grpc-kokoro/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/grpc-kokoro/subscriptions",
      "organizations_url": "https://api.github.com/users/grpc-kokoro/orgs",
      "repos_url": "https://api.github.com/users/grpc-kokoro/repos",
      "events_url": "https://api.github.com/users/grpc-kokoro/events{/privacy}",
      "received_events_url": "https://api.github.com/users/grpc-kokoro/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-02T23:36:55Z",
    "updated_at": "2017-06-02T23:36:55Z",
    "author_association": "NONE",
    "body": "```\nNo significant performance differences\n```"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/305932552",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-305932552",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 305932552,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTkzMjU1Mg==",
    "user": {
      "login": "dgquintas",
      "id": 120217,
      "node_id": "MDQ6VXNlcjEyMDIxNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/120217?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dgquintas",
      "html_url": "https://github.com/dgquintas",
      "followers_url": "https://api.github.com/users/dgquintas/followers",
      "following_url": "https://api.github.com/users/dgquintas/following{/other_user}",
      "gists_url": "https://api.github.com/users/dgquintas/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dgquintas/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dgquintas/subscriptions",
      "organizations_url": "https://api.github.com/users/dgquintas/orgs",
      "repos_url": "https://api.github.com/users/dgquintas/repos",
      "events_url": "https://api.github.com/users/dgquintas/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dgquintas/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-02T23:40:46Z",
    "updated_at": "2017-06-02T23:40:46Z",
    "author_association": "NONE",
    "body": "Thanks! PTAL.\n\n---\n\nReview status: 55 of 62 files reviewed at latest revision, 13 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c, line 1751 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kldbhnl-7_LU5n9bn9k:-Kle6GRFZ7Sxfnf64WtH:b-3u57gm) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1751)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nSame comment as in round_robin -- we should probably treat this as an empty list.\n</blockquote></details>\n\nRather than an empty update, this is an error: the arg is malformed. We'll continue using the previous state if available, go TRANSIENT_FAILURE otherwise.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 290 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldacZQUnyC6lgvg-Xe:-KldzoPBD3G1rGc7mtCH:b-3u57gm) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L290)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nSame comment as in round_robin -- we probably want to treat this as an empty list.\n</blockquote></details>\n\nRather than an empty update, this is an error: the arg is malformed. We'll continue using the previous state if available, go TRANSIENT_FAILURE otherwise.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 306 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldahyxG_fbB2ixW_xe:-KleaoGsL1bkQ5aYEiud:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L306)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nSame comment as in round_robin -- we probably want to unref the existing subchannels in this case.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c, line 405 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kldb2p8Ubct0456hQ-o:-KleoPaGcf1pZox8Mg2b:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c#L405)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nSame here -- probably need to unref the existing subchannels.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 159 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldRyPjfZqzit2BFe7i:-Kldr1FLuRpA8KmMcKnl:b89x22m) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L159)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>   for (size_t i = 0; i < subchannel_list->num_subchannels; i++) {\n>     subchannel_data *sd = &subchannel_list->subchannels[i];\n>     if (sd->subchannel != NULL) {\n>       GRPC_SUBCHANNEL_UNREF(exec_ctx, sd->subchannel,\n>                             \"rr_subchannel_list_destroy\");\n>     }\n>     sd->subchannel = NULL;\n>     if (sd->user_data != NULL) {\n>       GPR_ASSERT(sd->user_data_vtable != NULL);\n>       sd->user_data_vtable->destroy(exec_ctx, sd->user_data);\n>     }\n>   }\n\nWould it make more sense to do this for each individual subchannel in `rr_connectivity_changed_locked()` instead of here, so that we don't have to go through the loop here?\n</blockquote></details>\n\nI think it's cleaner to keep them here. I find the code more easily to understand in its current form.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 199 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldOoDJs0XE9cPfP7en:-KldrHZm3g73onRbJyDl:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L199)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nNot needed, since this is the end of the function.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 515 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldZwlQpUqt59K-KDId:-KldrPa3OGItxJ73uqar:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L515)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nNit: Please remove blank lines within functions.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 566 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-Kld_2WwLjVdqkPP7lKH:-KlelB2hfY_XsvidiACk:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L566)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nWhat happens if we get two updates in quick succession?  How do we ensure that the second one is the one that we wind up using, given that the connectivity callbacks for the first list could come in *after* the callbacks for the second list?\n\nOne possible solution (although there may be alternatives) would be to add a `pending_subchannel_list` member to the RR policy, which we set to the new list whenever we get an update.  When the callback comes in, we can check whether `sd->subchannel_list` is equal to `p->subchannel_list` or `p->pending_subchannel_list` (or neither) to decide how to handle it.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 666 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldUnVk8egMnPBdm1M1:-KldzjOwN_eHV4MJprdz:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L666)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>   if (arg == NULL || arg->type != GRPC_ARG_POINTER) {\n>     grpc_connectivity_state_set(\n>         exec_ctx, &p->state_tracker, GRPC_CHANNEL_TRANSIENT_FAILURE,\n>         GRPC_ERROR_CREATE_FROM_STATIC_STRING(\"Missing update in args\"),\n>         \"rr_update_missing\");\n>     return;\n>   }\n\nSetting the connectivity state to TRANSIENT_FAILURE doesn't seem like quite the right thing to do, since we're going to continue using the previous address list.\n\nIt might be better to just log an error and treat this as an empty address list.\n\n(I also wouldn't object to just making this an assertion, but you had changed this to avoid using an assertion back in #9838, so I assume that we need to continue to avoid that.)\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 683 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldVFuRgr98Gfsj8h7i:-KldsnPT7NFTShiycCWB:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L683)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nDon't we also want to clear the existing subchannel list?  There's no reason to continue holding refs to those subchannels if we're supposed to be switching to an empty list.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/lib/slice/slice_hash_table.c, line 144 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldNCPznNCZSBHnOD5e:-KlegzjWcCXiwOIeF2ZE:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/lib/slice/slice_hash_table.c#L144)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\n>     const gpr_slice a_key =\n>         is_empty(&a->entries[i]) ? grpc_empty_slice() : a->entries[i].key;\n>     const gpr_slice b_key =\n>         is_empty(&b->entries[i]) ? grpc_empty_slice() : b->entries[i].key;\n>     const int key_cmp = grpc_slice_cmp(a_key, b_key);\n>     if (key_cmp != 0) return key_cmp;\n>     // keys can be equal because both entries are empty. In that case, continue\n>     // (no values for empty entries).\n>     if (GRPC_SLICE_IS_EMPTY(a_key) && GRPC_SLICE_IS_EMPTY(b_key)) continue;\n>     const int value_cmp =\n>         value_cmp_fn_a(a->entries[i].value, b->entries[i].value);\n>     if (value_cmp != 0) return value_cmp;\n\nI don't think there's anything stopping someone from using an empty slice as a real key, in which case this might falsely compare as equal when it shouldn't.  I suggest explicitly checking whether the entries are empty first (which I think is also a bit clearer to read):\n\n```\nif (is_empty(&a->entries[i])) {\n  if (!is_empty(&b->entries[i])) {\n    return -1;  // a empty but b non-empty\n  }\n  continue;  // both empty, no need to check key or value\n} else if (is_empty(&b->entries[i])) {\n  return 1;  // a non-empty but b empty\n}\n// neither empty\nconst int key_cmp = grpc_slice_cmp(a->entries[i].key, b->entries[i].key);\nif (key_cmp != 0) return key_cmp;\nconst int value_cmp = value_cmp_fn_a(a->entries[i].value, b->entries[i].value);\nif (value_cmp != 0) return value_cmp;\n```\n\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/lib/slice/slice_hash_table.h, line 80 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldLEEJSUvnLeuvBbOC:-KldsylXNv5IOywsieVs:b7qmi9p) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/lib/slice/slice_hash_table.h#L80)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nWhy are `a_value` and `b_value` mentioned in this bullet?\n</blockquote></details>\n\nThe (foo, bar) was meant to represent an entry, but I can see it being confusing. Rewritten.\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159#-:-Klelu3AldwnkAbpIfEd:b4e8i7m)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/306217628",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-306217628",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 306217628,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjIxNzYyOA==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-05T15:25:48Z",
    "updated_at": "2017-06-05T15:25:48Z",
    "author_association": "MEMBER",
    "body": "This looks really good!  There are no more really substantive issues here -- all comments are relatively minor.  Hopefully, we can get this ready to merge after one more pass.\n\n---\n\nReviewed 7 of 7 files at r11.\nReview status: all files reviewed at latest revision, 9 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 159 at r10](https://reviewable.io:443/reviews/grpc/grpc/11159#-KldRyPjfZqzit2BFe7i:-Klsgc7tT7O5-MBGLQHa:b-jqux36) ([raw file](https://github.com/grpc/grpc/blob/d714b56c8d907c6a9752e1ac2f62cc2ebd62550f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L159)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\nI think it's cleaner to keep them here. I find the code more easily to understand in its current form.\n</blockquote></details>\n\nOkay.  Just note that this means that we may hold on to subchannel refs longer than we really need to.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 103 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlshYhNM2JFQMxmFeeU:-KlshYhNM2JFQMxmFeeV:bopoq72) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L103)):*\n> ```C\n>    * Subchannel connectivity callbacks will only promote updated subchannel\n>    * lists if they equal \\a latest_subchannel_list. In other words, racing\n>    * callbacks that reference outdated subchannel lists will be dropped. */\n> ```\n\nThis last sentence seems a little misleading.  We don't drop the callbacks; it's just that the callbacks will do something different in this case.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 104 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlshJVWtHajff-mYqdD:-KlshJVWtHajff-mYqdE:b8r0z7e) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L104)):*\n> ```C\n>    * lists if they equal \\a latest_subchannel_list. In other words, racing\n>    * callbacks that reference outdated subchannel lists will be dropped. */\n>   rr_subchannel_list *latest_subchannel_list;\n> ```\n\nMight be better to call this `pending_subchannel_list` or perhaps `latest_pending_subchannel_list`.  Without the word \"pending\", it's not clear that this is not the list that's actually currently in use.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 215 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlsiQXVAmLC3Zm1t2HF:-KlsiQXVAmLC3Zm1t2HG:b-e6o3io) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L215)):*\n> ```C\n>     subchannel_data *sd = &subchannel_list->subchannels[i];\n>     if (sd->subchannel != NULL) {  // if subchannel isn't shutdown, unsubscribe.\n>       grpc_subchannel_notify_on_state_change(exec_ctx, sd->subchannel, NULL,\n> ```\n\nJust occurred to me: Is this safe to do if there is already a pending connectivity callback?  It seems like it's reusing both the subchannel connectivity reporter and the closure that we're using to call it, when the existing callback may already be scheduled.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 585 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlsjBLbV85Hk-K9LNbf:-KlsjBLbV85Hk-K9LNbg:brei7ga) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L585)):*\n> ```C\n>         rr_subchannel_list_unref(exec_ctx, p->subchannel_list,\n>                                  \"sl_shutdown_rr_update_connectivity\");\n>         p->subchannel_list = sd->subchannel_list;\n> ```\n\nLet's also set `p->latest_subchannel_list = NULL`, since we've moved it here.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 693 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlslWgj3N_uOGurJ2t_:-KlslWgj3N_uOGurJ2ta:b-mubjm4) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L693)):*\n> ```C\n>         GRPC_ERROR_CREATE_FROM_STATIC_STRING(\"Empty update\"),\n>         \"rr_update_empty\");\n>     rr_subchannel_list_shutdown(exec_ctx, p->subchannel_list,\n> ```\n\nIf this is the first update, then `p->subchannel_list` will be NULL.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 693 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-Klsn2uc8rlHC5ENUXd4:-Klsn2uc8rlHC5ENUXd5:b-evqzh0) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L693)):*\n> ```C\n>         GRPC_ERROR_CREATE_FROM_STATIC_STRING(\"Empty update\"),\n>         \"rr_update_empty\");\n>     rr_subchannel_list_shutdown(exec_ctx, p->subchannel_list,\n> ```\n\nRight after this, I think we also need to call `rr_subchannel_list_unref()` and then set `p->subchannel_list = NULL`.\n\n(As a general rule, I think we *always* need to call `rr_subchannel_list_unref()` right after calling `rr_subchannel_list_shutdown()`.  Otherwise, we risk leaking memory, since we will never get a callback if all of the list's subchannels are already shut down.)\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 704 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlshyYE1JVbrkmLrNdc:-KlshyYFgF0xLrreONWy:b-pikvdf) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L704)):*\n> ```C\n>   subchannel_list->num_subchannels = num_addrs;\n>   gpr_ref_init(&subchannel_list->refcount, 1);\n>   p->latest_subchannel_list = subchannel_list;\n> ```\n\nI think we need to add the following right before this line:\n\n```\nif (p->latest_subchannel_list != NULL) {\n  rr_subchannel_list_shutdown(exec_ctx, p->latest_subchannel_list, \"rr_update_locked\");\n  rr_subchannel_list_unref(exec_ctx, p->subchannel_list, \"rr_update_locked\");\n}\n```\n\nThat way, when the connectivity callbacks are invoked for the previous update, they will properly shut down the list.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 372 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlsqjPEgQxq3zkSQ_kL:-KlsqjPEgQxq3zkSQ_kM:b6c060r) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/test/cpp/end2end/client_lb_end2end_test.cc#L372)):*\n> ```cpp\n> }\n> \n> TEST_F(ClientLbEnd2endTest, RoundRobinUpdates) {\n> ```\n\nPlease add an empty update in this test, so that we make sure we're handling that case properly.\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159#-:-Klsqz_CILK6M19La1QI:b-d36yk2)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/306310554",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-306310554",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 306310554,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjMxMDU1NA==",
    "user": {
      "login": "dgquintas",
      "id": 120217,
      "node_id": "MDQ6VXNlcjEyMDIxNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/120217?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dgquintas",
      "html_url": "https://github.com/dgquintas",
      "followers_url": "https://api.github.com/users/dgquintas/followers",
      "following_url": "https://api.github.com/users/dgquintas/following{/other_user}",
      "gists_url": "https://api.github.com/users/dgquintas/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dgquintas/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dgquintas/subscriptions",
      "organizations_url": "https://api.github.com/users/dgquintas/orgs",
      "repos_url": "https://api.github.com/users/dgquintas/repos",
      "events_url": "https://api.github.com/users/dgquintas/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dgquintas/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-05T21:07:28Z",
    "updated_at": "2017-06-05T21:07:28Z",
    "author_association": "NONE",
    "body": "\n\n\n\nReview status: 60 of 62 files reviewed at latest revision, 8 unresolved discussions, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 103 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlshYhNM2JFQMxmFeeU:-KltMeLHhhmDIERZyNvl:b9dfuu3) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L103)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nThis last sentence seems a little misleading.  We don't drop the callbacks; it's just that the callbacks will do something different in this case.\n</blockquote></details>\n\nRephrased.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 104 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlshJVWtHajff-mYqdD:-KltMpfBP2jkVu9ShHSf:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L104)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nMight be better to call this `pending_subchannel_list` or perhaps `latest_pending_subchannel_list`.  Without the word \"pending\", it's not clear that this is not the list that's actually currently in use.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 215 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlsiQXVAmLC3Zm1t2HF:-KltOdQMiXN_PTIeNQFT:bfxl5qk) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L215)):*\n> : Is this safe to do if there is already a pending connectivity callback?  It seems lik\nYes. In that case, all pending callbacks will be invoked with `error == GRPC_ERROR_CANCELLED`. More importantly in this case, all these invocations go out the `if (sd->subchannel_list->shutting_down)` branch.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 585 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlsjBLbV85Hk-K9LNbf:-KltPd0wuzXWzTUrseh4:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L585)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nLet's also set `p->latest_subchannel_list = NULL`, since we've moved it here.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 693 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlslWgj3N_uOGurJ2t_:-KltPzX2N0USQPbua0qa:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L693)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nIf this is the first update, then `p->subchannel_list` will be NULL.\n</blockquote></details>\n\nDone.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 693 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-Klsn2uc8rlHC5ENUXd4:-KltRcZ0H1EftDWVGIKI:b-ty9vne) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L693)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nRight after this, I think we also need to call `rr_subchannel_list_unref()` and then set `p->subchannel_list = NULL`.\n\n(As a general rule, I think we *always* need to call `rr_subchannel_list_unref()` right after calling `rr_subchannel_list_shutdown()`.  Otherwise, we risk leaking memory, since we will never get a callback if all of the list's subchannels are already shut down.)\n</blockquote></details>\n\nI've moved the unref to the shutdown function itself. Actually, it used to be there (that's why there's a `reason` argument that wasn't being used in the latest revision. It's found its purpose again).\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 704 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlshyYE1JVbrkmLrNdc:-KltTAs0rWuSgaX46sCt:bo68zsd) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L704)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nI think we need to add the following right before this line:\n\n```\nif (p->latest_subchannel_list != NULL) {\n  rr_subchannel_list_shutdown(exec_ctx, p->latest_subchannel_list, \"rr_update_locked\");\n  rr_subchannel_list_unref(exec_ctx, p->subchannel_list, \"rr_update_locked\");\n}\n```\n\nThat way, when the connectivity callbacks are invoked for the previous update, they will properly shut down the list.\n</blockquote></details>\n\n`p->latest_subchannel_list` is just a pointer copy of `subchannel_list`. The memory management happens independently over `subchannel_list`.\n\n---\n\n*[test/cpp/end2end/client_lb_end2end_test.cc, line 372 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlsqjPEgQxq3zkSQ_kL:-Klu18NaGUX-bjGbLgRz:b-896fix) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/test/cpp/end2end/client_lb_end2end_test.cc#L372)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nPlease add an empty update in this test, so that we make sure we're handling that case properly.\n</blockquote></details>\n\nDone.\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/306316522",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-306316522",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 306316522,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjMxNjUyMg==",
    "user": {
      "login": "grpc-kokoro",
      "id": 12722905,
      "node_id": "MDQ6VXNlcjEyNzIyOTA1",
      "avatar_url": "https://avatars3.githubusercontent.com/u/12722905?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/grpc-kokoro",
      "html_url": "https://github.com/grpc-kokoro",
      "followers_url": "https://api.github.com/users/grpc-kokoro/followers",
      "following_url": "https://api.github.com/users/grpc-kokoro/following{/other_user}",
      "gists_url": "https://api.github.com/users/grpc-kokoro/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/grpc-kokoro/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/grpc-kokoro/subscriptions",
      "organizations_url": "https://api.github.com/users/grpc-kokoro/orgs",
      "repos_url": "https://api.github.com/users/grpc-kokoro/repos",
      "events_url": "https://api.github.com/users/grpc-kokoro/events{/privacy}",
      "received_events_url": "https://api.github.com/users/grpc-kokoro/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-05T21:32:58Z",
    "updated_at": "2017-06-05T21:32:58Z",
    "author_association": "NONE",
    "body": "```\nNo significant performance differences\n```"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/306320511",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-306320511",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 306320511,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjMyMDUxMQ==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-05T21:49:25Z",
    "updated_at": "2017-06-05T21:49:25Z",
    "author_association": "MEMBER",
    "body": "Just one comment remaining.\n\n---\n\nReviewed 2 of 2 files at r12.\nReview status: all files reviewed at latest revision, 1 unresolved discussion, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 704 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlshyYE1JVbrkmLrNdc:-KluCewuuB9I6j-bEwkk:b-p77y7k) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L704)):*\n<details><summary><i>Previously, dgquintas (David G. Quintas) wrote…</i></summary><blockquote>\n\n`p->latest_subchannel_list` is just a pointer copy of `subchannel_list`. The memory management happens independently over `subchannel_list`.\n</blockquote></details>\n\nSure, but if we get a second update when the previous one was still in flight (i.e., the previous one has not yet been swapped from `p->latest_pending_subchannel_list` to `p->subchannel_list`), don't we need to make sure that the previous update gets shut down?  Otherwise, won't we just leak the subchannel list for the previous update?\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159#-:-KluDwbIP0iTohukJTnG:bf5nlqa)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/306362684",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-306362684",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 306362684,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjM2MjY4NA==",
    "user": {
      "login": "dgquintas",
      "id": 120217,
      "node_id": "MDQ6VXNlcjEyMDIxNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/120217?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dgquintas",
      "html_url": "https://github.com/dgquintas",
      "followers_url": "https://api.github.com/users/dgquintas/followers",
      "following_url": "https://api.github.com/users/dgquintas/following{/other_user}",
      "gists_url": "https://api.github.com/users/dgquintas/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dgquintas/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dgquintas/subscriptions",
      "organizations_url": "https://api.github.com/users/dgquintas/orgs",
      "repos_url": "https://api.github.com/users/dgquintas/repos",
      "events_url": "https://api.github.com/users/dgquintas/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dgquintas/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-06T02:19:37Z",
    "updated_at": "2017-06-06T02:19:37Z",
    "author_association": "NONE",
    "body": "\n\n\n\nReview status: all files reviewed at latest revision, 1 unresolved discussion, some commit checks failed.\n\n---\n\n*[src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c, line 704 at r11](https://reviewable.io:443/reviews/grpc/grpc/11159#-KlshyYE1JVbrkmLrNdc:-KluxwykZ8xHaVTaLEGM:b-jysops) ([raw file](https://github.com/grpc/grpc/blob/a5783bcdf174e30004e867bb9d1fba9d25d6899f/src/core/ext/filters/client_channel/lb_policy/round_robin/round_robin.c#L704)):*\n<details><summary><i>Previously, markdroth (Mark D. Roth) wrote…</i></summary><blockquote>\n\nSure, but if we get a second update when the previous one was still in flight (i.e., the previous one has not yet been swapped from `p->latest_pending_subchannel_list` to `p->subchannel_list`), don't we need to make sure that the previous update gets shut down?  Otherwise, won't we just leak the subchannel list for the previous update?\n</blockquote></details>\n\nI've made some changes. The main idea is a filter at the beginning of `rr_connectivity_changed_locked` that checks whether the `sd` belongs to an outdated subchannel list (one that's not the current nor the latest pending one). This works because, if the policy is picking, all `sd`s are watched and their callbacks eventually invoked: we don't need to unsubscribe these `sd`s from outdated subchannel lists, as they are guaranteed to transition from `INIT` to some other connectivity state eventually (likely READY if they can connect or TRANSIENT_FAILURE otherwise). \n If the policy isn't picking yet, I've also added some new self-explanatory code.\n\n---\n\n\n*Comments from [Reviewable](https://reviewable.io:443/reviews/grpc/grpc/11159)*\n<!-- Sent from Reviewable.io -->\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/306374134",
    "html_url": "https://github.com/grpc/grpc/pull/11159#issuecomment-306374134",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/11159",
    "id": 306374134,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjM3NDEzNA==",
    "user": {
      "login": "grpc-kokoro",
      "id": 12722905,
      "node_id": "MDQ6VXNlcjEyNzIyOTA1",
      "avatar_url": "https://avatars3.githubusercontent.com/u/12722905?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/grpc-kokoro",
      "html_url": "https://github.com/grpc-kokoro",
      "followers_url": "https://api.github.com/users/grpc-kokoro/followers",
      "following_url": "https://api.github.com/users/grpc-kokoro/following{/other_user}",
      "gists_url": "https://api.github.com/users/grpc-kokoro/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/grpc-kokoro/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/grpc-kokoro/subscriptions",
      "organizations_url": "https://api.github.com/users/grpc-kokoro/orgs",
      "repos_url": "https://api.github.com/users/grpc-kokoro/repos",
      "events_url": "https://api.github.com/users/grpc-kokoro/events{/privacy}",
      "received_events_url": "https://api.github.com/users/grpc-kokoro/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-06T03:49:10Z",
    "updated_at": "2017-06-06T03:49:10Z",
    "author_association": "NONE",
    "body": "```\nNo significant performance differences\n```"
  }
]
