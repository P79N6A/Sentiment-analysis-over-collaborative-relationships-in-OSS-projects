[
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/468039298",
    "html_url": "https://github.com/grpc/grpc/pull/18192#issuecomment-468039298",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/18192",
    "id": 468039298,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ2ODAzOTI5OA==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-02-27T21:32:41Z",
    "updated_at": "2019-02-27T21:32:41Z",
    "author_association": "MEMBER",
    "body": "I need more context here, since I haven't yet looked at #18021.  What do you mean by \"connectivity change notification is conditional in that PR\"?  And what does this representation have to do with that, given that there should be exactly the same functionality either way?\r\n\r\nThis approach seems strictly worse, since it isn't really any more efficient, but it does waste a few pointers' worth of memory for each channel.  (In fact, I could have sworn that this is the way this code used to be and that we changed it to avoid this recently, although I can't find the PR in which that happened, so I may be thinking of something else.)"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/468055435",
    "html_url": "https://github.com/grpc/grpc/pull/18192#issuecomment-468055435",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/18192",
    "id": 468055435,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ2ODA1NTQzNQ==",
    "user": {
      "login": "AspirinSJL",
      "id": 3314176,
      "node_id": "MDQ6VXNlcjMzMTQxNzY=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/3314176?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AspirinSJL",
      "html_url": "https://github.com/AspirinSJL",
      "followers_url": "https://api.github.com/users/AspirinSJL/followers",
      "following_url": "https://api.github.com/users/AspirinSJL/following{/other_user}",
      "gists_url": "https://api.github.com/users/AspirinSJL/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AspirinSJL/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AspirinSJL/subscriptions",
      "organizations_url": "https://api.github.com/users/AspirinSJL/orgs",
      "repos_url": "https://api.github.com/users/AspirinSJL/repos",
      "events_url": "https://api.github.com/users/AspirinSJL/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AspirinSJL/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-02-27T22:25:00Z",
    "updated_at": "2019-02-28T16:39:58Z",
    "author_association": "MEMBER",
    "body": "We need to keep watching the LB channel until it's shut down. Also, if the LB channel changes from READY state to READY state, we should catch this change and restart the LB call because the LB channel may have been updated with some new balancer addresses. \r\n\r\nIt's said in our code that the parent LB policy will catch such connectivity change https://github.com/grpc/grpc/blob/master/src/core/ext/filters/client_channel/lb_policy/xds/xds.cc#L1244. But in reality, I don't think this is happening, because the state tracker will do nothing if the new state is the same with the old one.\r\n\r\nI guess we have this false statement because our test is hiding this issue. In our test, the LB call is ended by the test before the LB channel update https://github.com/grpc/grpc/blob/master/test/cpp/end2end/grpclb_end2end_test.cc#L1070. Then, the parent policy tries to restart an LB call because the previous one fails. The LB channel state change (from READY to READY) doesn't trigger the connectivity closure to run. \r\n\r\nSo, we need a way to let the tracker to process identical state update if the watcher wants that. The update will trigger the closure to run only if the watcher wants that, so we will remove the watcher from the watcher list conditionally. With this PR, it's easier to do that. \r\n\r\nI think the code simplicity over weighs the additional memory in this case. But I don't have strong opinion about this. "
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/468364356",
    "html_url": "https://github.com/grpc/grpc/pull/18192#issuecomment-468364356",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/18192",
    "id": 468364356,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ2ODM2NDM1Ng==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-02-28T17:34:16Z",
    "updated_at": "2019-02-28T17:34:16Z",
    "author_association": "MEMBER",
    "body": "> It's said in our code that the parent LB policy will catch such connectivity change https://github.com/grpc/grpc/blob/master/src/core/ext/filters/client_channel/lb_policy/xds/xds.cc#L1244. But in reality, I don't think this is happening, because the state tracker will do nothing if the new state is the same with the old one.\r\n\r\nI think you're right about that problem.  I've noticed that comment a couple of times recently myself, and it does look wrong, but I hadn't had time to look into it.\r\n\r\nLooking into the history of this now, it looks like the code was originally added way back in #11159, when we first added support for LB policy updates (prior to that, we recreated the LB policy upon each new update from the resolver).  In that original code, we always started the connectivity watch from state INIT (which no longer exists; that state was originally added at my suggestion in #8666, but I subsequently realized that that was a bad idea, and it was removed in #12878), in which case the comment was true: we would always get a callback for state READY, because we always started the watch from a different state.  However, when the epoll1 polling engine was added, there wound up being a timing bug here, and we attempted to fix this in #11965 by starting the watch from the current state instead of from a fixed state.  Although we didn't notice at the time, I think this is when the code stopped providing the guarantee that it would always be invoked when the channel reached state READY.\r\n\r\nI suspect that we could get back to the original behavior simply by starting the watch from IDLE state, which would ensure that we do get a callback when it changes to state READY.  If this turns out to trigger the epoll1 timing bug again (although it may not, since a lot has changed since #11965), then we can find a better way to fix that.\r\n\r\nThat having been said, let me take a step back and ask a more basic question: Do we actually need this connectivity state watch in the first place?  It seems like the original intent of doing this was to ensure that we restart the balancer call if the balancer address that the call is currently talking to is removed by the resolver.  But it's not immediately obvious to me why we need to do that.  If the old balancer is still working, then we can stay connected to it, even if it's no longer in the server list.  If it stops working, then the current balancer call will fail, and we will start a new one automatically, even without this watch.  So one possibility here would be to simply remove this watch.  (Also, note that even if we fix this code, it causes some unnecessary churn, because it restarts the balancer call unconditionally, not just when the address that the current call is talking to is removed from the list.  So this may be another good reason to eliminate it.)  So we should probably talk to the load balancing team to see whether this is something we actually need to do.\r\n\r\n> So, we need a way to let the tracker to process identical state update if the watcher wants that. The update will trigger the closure to run only if the watcher wants that, so we will remove the watcher from the watcher list conditionally. With this PR, it's easier to do that.\r\n\r\nI don't think this is necessary; I think we can more easily solve this by simply starting the watch from a fixed state like IDLE.\r\n\r\nIt's not clear to me that this PR yields any benefits.  The new code doesn't seem any simpler to me, and there is some memory waste, so I think we should stick with the current implementation."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/468405080",
    "html_url": "https://github.com/grpc/grpc/pull/18192#issuecomment-468405080",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/18192",
    "id": 468405080,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ2ODQwNTA4MA==",
    "user": {
      "login": "AspirinSJL",
      "id": 3314176,
      "node_id": "MDQ6VXNlcjMzMTQxNzY=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/3314176?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AspirinSJL",
      "html_url": "https://github.com/AspirinSJL",
      "followers_url": "https://api.github.com/users/AspirinSJL/followers",
      "following_url": "https://api.github.com/users/AspirinSJL/following{/other_user}",
      "gists_url": "https://api.github.com/users/AspirinSJL/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AspirinSJL/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AspirinSJL/subscriptions",
      "organizations_url": "https://api.github.com/users/AspirinSJL/orgs",
      "repos_url": "https://api.github.com/users/AspirinSJL/repos",
      "events_url": "https://api.github.com/users/AspirinSJL/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AspirinSJL/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-02-28T19:29:06Z",
    "updated_at": "2019-02-28T19:29:06Z",
    "author_association": "MEMBER",
    "body": "\r\n> I suspect that we could get back to the original behavior simply by starting the watch from IDLE state, which would ensure that we do get a callback when it changes to state READY. If this turns out to trigger the epoll1 timing bug again (although it may not, since a lot has changed since #11965), then we can find a better way to fix that.\r\n\r\nI actually tried starting the watch from IDLE, but then the notification closure would be run immediately after we start watching if the current channel state is READY. Then it enters a dead loop. \r\n\r\n> That having been said, let me take a step back and ask a more basic question: Do we actually need this connectivity state watch in the first place?\r\n\r\nI think the intent of this watch is to restart the LB call if the LB channel is ready after being updated. That implies that we prefer contacting some new balancer even if the call to the old balancer is still working. \r\n\r\nThat's what the grpclb code has been doing (more accurately: has been thinking it's doing), although we need to talk with the grpclb folks to confirm that's still the expected behavior. Say if we do want that behavior, then we need to watch the LB channel with the ability to catch update from READY to READY. \r\n\r\n> It's not clear to me that this PR yields any benefits. The new code doesn't seem any simpler to me, and there is some memory waste, so I think we should stick with the current implementation.\r\n\r\nI think the code is simpler because we don't need to worry about whether we are deleting the first node in the list. That's basically the reason why we have such sentinel in any linked list. It trades memory for simplicity. "
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/468413266",
    "html_url": "https://github.com/grpc/grpc/pull/18192#issuecomment-468413266",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/18192",
    "id": 468413266,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ2ODQxMzI2Ng==",
    "user": {
      "login": "AspirinSJL",
      "id": 3314176,
      "node_id": "MDQ6VXNlcjMzMTQxNzY=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/3314176?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AspirinSJL",
      "html_url": "https://github.com/AspirinSJL",
      "followers_url": "https://api.github.com/users/AspirinSJL/followers",
      "following_url": "https://api.github.com/users/AspirinSJL/following{/other_user}",
      "gists_url": "https://api.github.com/users/AspirinSJL/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AspirinSJL/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AspirinSJL/subscriptions",
      "organizations_url": "https://api.github.com/users/AspirinSJL/orgs",
      "repos_url": "https://api.github.com/users/AspirinSJL/repos",
      "events_url": "https://api.github.com/users/AspirinSJL/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AspirinSJL/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-02-28T19:51:40Z",
    "updated_at": "2019-02-28T19:51:40Z",
    "author_association": "MEMBER",
    "body": "I just realized that if we want to contact some new balancer after update, it means the current balancer address is not in the update because pick_first will stick to the already selected subchannel. And if the current balancer address is not in the update, it probably means it's unhealthy and the current LB call will end soon.."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/468465300",
    "html_url": "https://github.com/grpc/grpc/pull/18192#issuecomment-468465300",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/18192",
    "id": 468465300,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ2ODQ2NTMwMA==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-02-28T22:30:41Z",
    "updated_at": "2019-02-28T22:30:41Z",
    "author_association": "MEMBER",
    "body": "I think we've agreed in our offline discussion that we no longer need to try to restart the balancer call in this case, so we can just get rid of the code that motivated this change.\r\n\r\n> I think the code is simpler because we don't need to worry about whether we are deleting the first node in the list. That's basically the reason why we have such sentinel in any linked list. It trades memory for simplicity.\r\n\r\nI think sentinels are messy.  Checking whether a value is null is not hard and is ultimately much more intuitive than a sentinel value, since the underlying representation is much more natural for C++.  I never write code that uses sentinels, and I've been known to rip them out whenever I'm touching existing code that uses them (one of my earliest PRs after joining grpc-team did this; see #7616)."
  }
]
