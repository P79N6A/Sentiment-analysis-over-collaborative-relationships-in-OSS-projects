[
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/326052129",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-326052129",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 326052129,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjA1MjEyOQ==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-08-30T16:52:29Z",
    "updated_at": "2017-08-30T16:52:29Z",
    "author_association": "MEMBER",
    "body": "For future reference, for items like this that are not bugs, instead of filing issues, please send email to the grpc.io mailing list.  That's a much better forum for this kind of discussion.\r\n\r\nI don't know anything about node or speak javascript, so @murgatroid99 will have to help you with that side of things.  But I can help answer your questions about how client-side load balancing is supposed to work.\r\n\r\nI think a similar question came up a while back; see discussion in #11406.  Glancing at your code, it looks like you're using the wrong name for the channel arg to select the LB policy.  Try changing `loadBalancingPolicy` to `grpc.lb_policy_name`.\r\n\r\nIf that doesn't fix the problem, please let us know what you're trying to do, what you expected to see, and what you're actually seeing."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/326156049",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-326156049",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 326156049,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjE1NjA0OQ==",
    "user": {
      "login": "carldanley",
      "id": 1470571,
      "node_id": "MDQ6VXNlcjE0NzA1NzE=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1470571?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/carldanley",
      "html_url": "https://github.com/carldanley",
      "followers_url": "https://api.github.com/users/carldanley/followers",
      "following_url": "https://api.github.com/users/carldanley/following{/other_user}",
      "gists_url": "https://api.github.com/users/carldanley/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/carldanley/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/carldanley/subscriptions",
      "organizations_url": "https://api.github.com/users/carldanley/orgs",
      "repos_url": "https://api.github.com/users/carldanley/repos",
      "events_url": "https://api.github.com/users/carldanley/events{/privacy}",
      "received_events_url": "https://api.github.com/users/carldanley/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-08-31T00:33:39Z",
    "updated_at": "2017-08-31T00:33:39Z",
    "author_association": "NONE",
    "body": "Hello @markdroth! First off, thank you!\r\n\r\nSecondly, I tried changing the load balancing policy to `grpc.lb_policy_name` but had no luck with client-side load balancing occurring. Let me explain what I'm using for a setup:\r\n\r\nI run kubernetes. I have a service for our gRPC server. I have a client that is told to use the internal DNS address for the service: `server.grpc.svc.cluster.local` which resolves to the 2 (for testing purposes) pods that run the gRPC server. When the client is instructed to connect to that service (via https://gist.github.com/carldanley/39d5a0d7f9b1ea865af94481da1e0cac#file-index-js-L50), it connects to the first one that the DNS resolves to and keeps creating gRPC requests to that single server (instead of round robin to the two servers). I'm not sure what I can do differently to make this work... I expect the code in that gist to have a single client connect to both hosts on start and then, per request, round-robin through each of the gRPC servers."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/326194421",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-326194421",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 326194421,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjE5NDQyMQ==",
    "user": {
      "login": "carldanley",
      "id": 1470571,
      "node_id": "MDQ6VXNlcjE0NzA1NzE=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1470571?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/carldanley",
      "html_url": "https://github.com/carldanley",
      "followers_url": "https://api.github.com/users/carldanley/followers",
      "following_url": "https://api.github.com/users/carldanley/following{/other_user}",
      "gists_url": "https://api.github.com/users/carldanley/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/carldanley/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/carldanley/subscriptions",
      "organizations_url": "https://api.github.com/users/carldanley/orgs",
      "repos_url": "https://api.github.com/users/carldanley/repos",
      "events_url": "https://api.github.com/users/carldanley/events{/privacy}",
      "received_events_url": "https://api.github.com/users/carldanley/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-08-31T05:34:53Z",
    "updated_at": "2017-08-31T05:34:53Z",
    "author_association": "NONE",
    "body": "Ok, so this was a kubernetes-related issue. I had to turn off kubernetes load balancing for this specific server's service entry in k8s. However, the next thing I tested was destroying a server randomly. It appears that the client does not periodically re-evaluate the entries for the DNS name provided; as such it continued to send traffic to the remaining instance it knew of but never detected the new instance (until I restarted the client). How can I have my clients automatically start using a new server instance that came online?"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/326319837",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-326319837",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 326319837,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjMxOTgzNw==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-08-31T14:50:13Z",
    "updated_at": "2017-08-31T14:50:13Z",
    "author_association": "MEMBER",
    "body": "Currently, we only re-resolve DNS names when all subchannels become disconnected.  So if you restart both of the servers, the client will re-resolve and then connect to the new addresses.  But if you move just one of the servers, we will just stop being able to use that one.\r\n\r\n@dgquintas and I have talked about possible ways to address this problem.  It's fairly tricky, because we can't really know when it's useful to re-resolve, and we need to avoid slamming the DNS server with constant queries.  For example, if a server is crashing every 10 seconds, we don't want every single client to try to re-resolve the name every 10 seconds.  And if this is an environment where servers have static addresses, then there's no point in re-resolving in the first place.\r\n\r\nOne possible solution would be to make the DNS resolver aware of DNS TTLs, so that we can automatically re-resolve after the previous results expire; this would essentially allow the DNS data to determine how often the clients re-resolve.  However, while we could probably do this in the C-core gRPC implementation, it's not clear that we have reasonable ways to access DNS TTL information in Java or Go, which would make our clients' behavior inconsistent.\r\n\r\nAnother possibility is to provide the ability to configure the threshold for what percentage of subchannels need to become disconnected before we re-resolve.  The default would be 100%, which would match the current behavior, but it would allow people to reduce the threshold to something more appropriate for their environment.  We might also want to provide a way to set the minimum interval between re-resolutions, just to provide some additional safety against slamming the DNS server.\r\n\r\nAnyway, we've had a lot of discussions about this but have not yet decided on any particular behavior or scheduled any work on this.  But if this is something you'd like, let us know, and we can start figuring out how to prioritize it."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/326383380",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-326383380",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 326383380,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjM4MzM4MA==",
    "user": {
      "login": "carldanley",
      "id": 1470571,
      "node_id": "MDQ6VXNlcjE0NzA1NzE=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1470571?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/carldanley",
      "html_url": "https://github.com/carldanley",
      "followers_url": "https://api.github.com/users/carldanley/followers",
      "following_url": "https://api.github.com/users/carldanley/following{/other_user}",
      "gists_url": "https://api.github.com/users/carldanley/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/carldanley/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/carldanley/subscriptions",
      "organizations_url": "https://api.github.com/users/carldanley/orgs",
      "repos_url": "https://api.github.com/users/carldanley/repos",
      "events_url": "https://api.github.com/users/carldanley/events{/privacy}",
      "received_events_url": "https://api.github.com/users/carldanley/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-08-31T18:31:45Z",
    "updated_at": "2017-08-31T18:31:45Z",
    "author_association": "NONE",
    "body": "Okay, I understand. I did some testing and splitting traffic reliably in a CI/CD environment is hit or miss at the moment. Consider the following:\r\n\r\nYou have 2 instances (iA and iB) of service 1 (s1) running. You have 2 instances (iC and iD) of service 2 (s2) running. If s1 has a rolling update and shortly after s2 has a rolling update: the events could pan out like this:\r\n\r\n1. `s1 rolling update starts`\r\n1. `iA goes down`\r\n1. `iA comes up`\r\n1. `s2 rolling update starts`\r\n1. `iB goes down`\r\n1. `iC goes down`\r\n1. `iB comes up`\r\n1. `iC comes up`\r\n1. `s1 rolling update stops`\r\n1. `iD goes down`\r\n1. `iD comes up`\r\n1. `s2 rolling update stops`\r\n\r\nIn the scenario above, s1 instances (iA and iB) will only see 1/2 of s2 instances (iC) given. This is bad and means that none of my traffic is round-robin'ed and that 1 instance is getting slammed.\r\n\r\nAfter reading the solutions you proposed, it kind of feels like all of those things should at least be available. Let the developer decided whether or not they want to take extra DNS traffic. Let the developer decided if they're using a client (in a language) which can access DNS TTL and so on.\r\n\r\n*Just thinking out loud here:*\r\n\r\nI think there are really 2 scenarios we care about:\r\n\r\n**1. Losing an established subchannel**\r\n\r\nWhen a subchannel disconnects (2/2 instances becomes 1/2 instances), we could attempt to re-resolve the DNS entry for some number (attempt-based or time-based) of times. This gives us a way to start listening until we're 2/2 again OR we were unsuccessful in getting back 2/2 (so we stay 1/2).\r\n\r\n**2. Discovery of new instances**\r\n\r\nConsider we had a healthy, happy service that was correctly load balancing 5/5 instances. What if we had autoscaling enabled and we had a sudden surge of traffic hit our servers. Now we're running 7 instances and because we never scaled the clients and only the servers, we have no way to serve the surge of traffic (because they'll never refresh the DNS pool) so we stay 5/7...\r\n\r\nOne possible solution is giving us a function to call that can refresh the pool of hosts via DNS resolution. This would give us a way to decide when we want to trigger it ourselves. For example, imagine that we performed a rolling update on a service that had 100 instances; we could publish an event onto our messaging queue (when 100 of 100 is up) that could tell clients to refresh their DNS hosts. Really, we could write whatever logic we want with this and perform the resolution whenever we saw fit; it gives us full control of something that works for us.\r\n\r\nAnyways... </2cents>"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/326412136",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-326412136",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 326412136,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjQxMjEzNg==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-08-31T20:30:39Z",
    "updated_at": "2017-08-31T20:30:39Z",
    "author_association": "MEMBER",
    "body": "I definitely agree that we should do better in the first scenario you mentioned, and some combination of the ideas we've been discussing could address that.\r\n\r\nWith regard to the second scenario you mention, I think it's worth noting that DNS is fundamentally unsuited to the kind of dynamic environment you're describing, because DNS is a polling-based mechanism, whereas what you really want is a push-based mechanism where the clients are proactively notified when addresses change.  While we might be able to find a way to work around this with DNS with the DNS TTL solution I mentioned above, I think it will never really scale the way it needs to, because it really wasn't designed for this kind of usage.  A better approach would be to write a new resolver mechanism that subscribes to notification from some centralized system as the servers move around.  For example, I'm not sure what mechanism kubernetes uses to update DNS, but you could presumably have it also notify some other name service that would allow clients to subscribe to particular names and would proactively send them updates when kubernetes notifies them of changes to those names.  Then your clients would be getting a constant stream of updates and would always have an up-to-date set of addresses.\r\n\r\nGiven that, I think that any changes we make here will likely be focused on the first scenario, not the second.  But we'll have to talk further to decide exactly how we're going to handle this."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/358453854",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-358453854",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 358453854,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODQ1Mzg1NA==",
    "user": {
      "login": "hollinwilkins",
      "id": 15824835,
      "node_id": "MDQ6VXNlcjE1ODI0ODM1",
      "avatar_url": "https://avatars0.githubusercontent.com/u/15824835?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/hollinwilkins",
      "html_url": "https://github.com/hollinwilkins",
      "followers_url": "https://api.github.com/users/hollinwilkins/followers",
      "following_url": "https://api.github.com/users/hollinwilkins/following{/other_user}",
      "gists_url": "https://api.github.com/users/hollinwilkins/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/hollinwilkins/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/hollinwilkins/subscriptions",
      "organizations_url": "https://api.github.com/users/hollinwilkins/orgs",
      "repos_url": "https://api.github.com/users/hollinwilkins/repos",
      "events_url": "https://api.github.com/users/hollinwilkins/events{/privacy}",
      "received_events_url": "https://api.github.com/users/hollinwilkins/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-01-17T21:33:06Z",
    "updated_at": "2018-01-17T21:33:06Z",
    "author_association": "NONE",
    "body": "@markdroth DNS may not be the perfect solution, but it is ubiquitous and easy to integrate with. I would prefer to setup a DNS poll every 10-20 seconds for my microservices to at least get going with load balancing my gRPC services. When that produces too much load on the DNS servers, then I will start looking at a lookaside balancer.\r\n\r\nRight now the cost to getting simple load balancing that we are used to with HTTP 1.1 is very high. The solutions are, as I see them:\r\n\r\n1. Create your own lookaside load balancer, modify ALL of your clients code to use the lookaside balancer, maintain the code and integrations with your service discovery platform of choice (K8, Consul, ZooKeeper, etc.)\r\n2. Create a service mesh using Istio or Linkerd, both of which have their own limitations, drawbacks, and advantages.\r\n3. Use builtin DNS resolver, which means you can't scale your servers up without first scaling all of your servers down.\r\n\r\nA DNS-based resolver with a refresh interval would be a very low-cost, low barrier-to-entry solution that lots of developers would be comfortable with and not require a huge investment in either infrastructure or coding."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/358483266",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-358483266",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 358483266,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODQ4MzI2Ng==",
    "user": {
      "login": "hollinwilkins",
      "id": 15824835,
      "node_id": "MDQ6VXNlcjE1ODI0ODM1",
      "avatar_url": "https://avatars0.githubusercontent.com/u/15824835?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/hollinwilkins",
      "html_url": "https://github.com/hollinwilkins",
      "followers_url": "https://api.github.com/users/hollinwilkins/followers",
      "following_url": "https://api.github.com/users/hollinwilkins/following{/other_user}",
      "gists_url": "https://api.github.com/users/hollinwilkins/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/hollinwilkins/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/hollinwilkins/subscriptions",
      "organizations_url": "https://api.github.com/users/hollinwilkins/orgs",
      "repos_url": "https://api.github.com/users/hollinwilkins/repos",
      "events_url": "https://api.github.com/users/hollinwilkins/events{/privacy}",
      "received_events_url": "https://api.github.com/users/hollinwilkins/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-01-17T23:22:09Z",
    "updated_at": "2018-01-17T23:22:09Z",
    "author_association": "NONE",
    "body": "For anyone encountering issues and looking for a simple solution: https://github.com/grpc/proposal/pull/23/files\r\n\r\nUsing server-side connection options can cause load to redistribute in a fairly easy manner! Wish I had seen this document 2 weeks ago."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/375098264",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-375098264",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 375098264,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTA5ODI2NA==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-21T21:15:26Z",
    "updated_at": "2018-03-21T21:15:26Z",
    "author_association": "MEMBER",
    "body": "We've recently done some work to make this somewhat better.  The round_robin code now re-resolves whenever any individual backend connection fails, and the DNS resolver enforces a minimum time between lookups to ensure that we don't hammer the DNS server when backend connections are flapping.\r\n\r\nThis doesn't address the discovery case, but it does improve the scenario where only a subset of backends fail."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/382697436",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-382697436",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 382697436,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM4MjY5NzQzNg==",
    "user": {
      "login": "jtattermusch",
      "id": 9939684,
      "node_id": "MDQ6VXNlcjk5Mzk2ODQ=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/9939684?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jtattermusch",
      "html_url": "https://github.com/jtattermusch",
      "followers_url": "https://api.github.com/users/jtattermusch/followers",
      "following_url": "https://api.github.com/users/jtattermusch/following{/other_user}",
      "gists_url": "https://api.github.com/users/jtattermusch/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jtattermusch/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jtattermusch/subscriptions",
      "organizations_url": "https://api.github.com/users/jtattermusch/orgs",
      "repos_url": "https://api.github.com/users/jtattermusch/repos",
      "events_url": "https://api.github.com/users/jtattermusch/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jtattermusch/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-04-19T11:13:07Z",
    "updated_at": "2018-04-19T11:13:07Z",
    "author_association": "CONTRIBUTOR",
    "body": "CC @jtattermusch"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/382700935",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-382700935",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 382700935,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM4MjcwMDkzNQ==",
    "user": {
      "login": "jtattermusch",
      "id": 9939684,
      "node_id": "MDQ6VXNlcjk5Mzk2ODQ=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/9939684?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jtattermusch",
      "html_url": "https://github.com/jtattermusch",
      "followers_url": "https://api.github.com/users/jtattermusch/followers",
      "following_url": "https://api.github.com/users/jtattermusch/following{/other_user}",
      "gists_url": "https://api.github.com/users/jtattermusch/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jtattermusch/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jtattermusch/subscriptions",
      "organizations_url": "https://api.github.com/users/jtattermusch/orgs",
      "repos_url": "https://api.github.com/users/jtattermusch/repos",
      "events_url": "https://api.github.com/users/jtattermusch/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jtattermusch/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-04-19T11:27:23Z",
    "updated_at": "2018-04-19T11:27:23Z",
    "author_association": "CONTRIBUTOR",
    "body": "@hollinwilkins  can you describe what changes to your setup you've made (in reference to https://github.com/grpc/grpc/issues/12295#issuecomment-358483266) and confirm that the \"discovering of new endpoints\" problem went away?   I am currently facing the exact issue you were facing (losing an established instance is handled correctly, but new instances are not being discovered) while trying to make a simple RoundRobin LB scenario work out of the box on kubernetes.\r\n\r\nAre there any other possible workarounds (like forcing re-resolution of backends)?"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/382794204",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-382794204",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 382794204,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM4Mjc5NDIwNA==",
    "user": {
      "login": "jtattermusch",
      "id": 9939684,
      "node_id": "MDQ6VXNlcjk5Mzk2ODQ=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/9939684?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jtattermusch",
      "html_url": "https://github.com/jtattermusch",
      "followers_url": "https://api.github.com/users/jtattermusch/followers",
      "following_url": "https://api.github.com/users/jtattermusch/following{/other_user}",
      "gists_url": "https://api.github.com/users/jtattermusch/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jtattermusch/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jtattermusch/subscriptions",
      "organizations_url": "https://api.github.com/users/jtattermusch/orgs",
      "repos_url": "https://api.github.com/users/jtattermusch/repos",
      "events_url": "https://api.github.com/users/jtattermusch/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jtattermusch/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-04-19T16:14:09Z",
    "updated_at": "2018-04-19T16:14:36Z",
    "author_association": "CONTRIBUTOR",
    "body": "Ad workaround based on https://github.com/grpc/proposal/blob/master/A9-server-side-conn-mgt.md:\r\n\r\nI tried setting `grpc.max_connection_age_ms` and `grpc.max_connection_age_grace_ms` channel arguments on the server that I'm trying to access with RoundRobin load balancing policy and it seems that it is helping: closing the connections occasionally leads to re-resolving the domain name and newly added service replicas are being picked up by the round-robin loadbalancer in a relatively short time."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/382816060",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-382816060",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 382816060,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM4MjgxNjA2MA==",
    "user": {
      "login": "hollinwilkins",
      "id": 15824835,
      "node_id": "MDQ6VXNlcjE1ODI0ODM1",
      "avatar_url": "https://avatars0.githubusercontent.com/u/15824835?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/hollinwilkins",
      "html_url": "https://github.com/hollinwilkins",
      "followers_url": "https://api.github.com/users/hollinwilkins/followers",
      "following_url": "https://api.github.com/users/hollinwilkins/following{/other_user}",
      "gists_url": "https://api.github.com/users/hollinwilkins/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/hollinwilkins/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/hollinwilkins/subscriptions",
      "organizations_url": "https://api.github.com/users/hollinwilkins/orgs",
      "repos_url": "https://api.github.com/users/hollinwilkins/repos",
      "events_url": "https://api.github.com/users/hollinwilkins/events{/privacy}",
      "received_events_url": "https://api.github.com/users/hollinwilkins/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-04-19T17:25:13Z",
    "updated_at": "2018-04-19T17:25:13Z",
    "author_association": "NONE",
    "body": "@jtattermusch This is the approach I took. Not ideal, but works for now."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/393675459",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-393675459",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 393675459,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzY3NTQ1OQ==",
    "user": {
      "login": "wjywbs",
      "id": 6233680,
      "node_id": "MDQ6VXNlcjYyMzM2ODA=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/6233680?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/wjywbs",
      "html_url": "https://github.com/wjywbs",
      "followers_url": "https://api.github.com/users/wjywbs/followers",
      "following_url": "https://api.github.com/users/wjywbs/following{/other_user}",
      "gists_url": "https://api.github.com/users/wjywbs/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/wjywbs/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wjywbs/subscriptions",
      "organizations_url": "https://api.github.com/users/wjywbs/orgs",
      "repos_url": "https://api.github.com/users/wjywbs/repos",
      "events_url": "https://api.github.com/users/wjywbs/events{/privacy}",
      "received_events_url": "https://api.github.com/users/wjywbs/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-05-31T20:50:39Z",
    "updated_at": "2018-05-31T20:50:39Z",
    "author_association": "NONE",
    "body": "grpc go client re-resolves DNS every 30 minutes. Could c++ client do the same, so we can configure the interval?\r\n\r\nhttps://github.com/grpc/grpc-go/blob/master/resolver/dns/dns_resolver.go#L46"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/431582244",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-431582244",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 431582244,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTU4MjI0NA==",
    "user": {
      "login": "hacst",
      "id": 237537,
      "node_id": "MDQ6VXNlcjIzNzUzNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/237537?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/hacst",
      "html_url": "https://github.com/hacst",
      "followers_url": "https://api.github.com/users/hacst/followers",
      "following_url": "https://api.github.com/users/hacst/following{/other_user}",
      "gists_url": "https://api.github.com/users/hacst/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/hacst/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/hacst/subscriptions",
      "organizations_url": "https://api.github.com/users/hacst/orgs",
      "repos_url": "https://api.github.com/users/hacst/repos",
      "events_url": "https://api.github.com/users/hacst/events{/privacy}",
      "received_events_url": "https://api.github.com/users/hacst/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-10-20T13:29:37Z",
    "updated_at": "2018-10-20T13:29:37Z",
    "author_association": "CONTRIBUTOR",
    "body": "After having read this thread and some of the linked issues I'm still not sure I understand why observing the DNS TTL for refresh would be a bad thing. From what I can tell it would just work. Be it scaling up or down, k8s or outside of it. I think its properties would fit the principle of least surprise. I cannot imagine many selecting a DNS based round-robin load balancing approach would be surprised by clients having to poll DNS in TTL interval and that producing load. However many will be surprised to learn it won't react to changes in DNS.\r\n\r\nLoad seems to be the most commonly stated reason why observing DNS TTL would be bad but I just don't see it. If my DNS service cannot handle the polling load I can easily trade-off with higher TTL, scale my DNS and ultimately once that no longer makes sense transition to another more scaleable LB approach. It is not like DNS RR LB in gRPC allows arbitrary scale to begin with so why pretend it has to? It is the simple solution for the simple cases. It should work as best as it can inside of those constraints.\r\n\r\nHaving to use MaxConnectionAge, which just happens to be coupled to re-resolution, to emulate a polling behaviour seems like a bad workaround to me. I don't see how making a DNS query to some DNS cache every X seconds would be seen as problematic but having to do a magnitudes more expensive re-connect plus (encryption-)handshake with each of the backends plus having to regularly refresh DNS anyway is an acceptable workaround to that.\r\n\r\nCurrently all the alternatives I can see are vastly more complex to run and expensive to implement. Why force users to use a service mesh or some custom look-aside load-balancing scheme when there's a way to make what is already supported just work for a lot of cases?"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/431858100",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-431858100",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 431858100,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTg1ODEwMA==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-10-22T14:50:51Z",
    "updated_at": "2018-10-22T14:50:51Z",
    "author_association": "MEMBER",
    "body": "Personally, I tend to agree that the max-connection-age approach is a fairly ugly solution to the problem of forcing re-resolution.  However, I'm not sure that everyone on the team agrees with that.\r\n\r\nI think the main argument against using TTLs is that we want consistent client behavior across languages, but while we would be able to access the TTL information in C-core, we have no reasonable mechanism for doing so in Java or Go.  So it's not really a portable solution.\r\n\r\nI do think we should consider providing a way for the client to be configured to periodically re-resolve at some fixed interval.\r\n\r\nI'd like to get feedback from @ejona86, @zhangkun83, and @dfawley on this."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/432016157",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-432016157",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 432016157,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQzMjAxNjE1Nw==",
    "user": {
      "login": "hacst",
      "id": 237537,
      "node_id": "MDQ6VXNlcjIzNzUzNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/237537?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/hacst",
      "html_url": "https://github.com/hacst",
      "followers_url": "https://api.github.com/users/hacst/followers",
      "following_url": "https://api.github.com/users/hacst/following{/other_user}",
      "gists_url": "https://api.github.com/users/hacst/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/hacst/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/hacst/subscriptions",
      "organizations_url": "https://api.github.com/users/hacst/orgs",
      "repos_url": "https://api.github.com/users/hacst/repos",
      "events_url": "https://api.github.com/users/hacst/events{/privacy}",
      "received_events_url": "https://api.github.com/users/hacst/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-10-22T22:39:48Z",
    "updated_at": "2018-10-22T22:39:48Z",
    "author_association": "CONTRIBUTOR",
    "body": "I see. Technically DNS TTL could be retrieved in any language by using a custom resolver (e.g. using something like miekg/dns in go or netty DNSResolver in Java. The latter would also get rid of the broken built-in DNS caching behaviour of the JVM...). That's basically what using c-ares in C-core amounts to. Whether that's a \"reasonable\" thing to do everywhere is of course debatable.\r\n\r\nIn any case I definitely would prefer a configurable polling interval for DNS to the current MaxConnectionAge approach. Maybe there could even be an opt-in flag that makes it use the DNS TTL when supported and fallback to the polling interval otherwise? I'm not sure whether such \"extensions\" is something that's done across the gRPC clients in different languages but I would be surprised if they are totally equal now. But as I said. Just having the configurable DNS polling interval would be a considerable improvement.\r\n\r\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/432833454",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-432833454",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 432833454,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQzMjgzMzQ1NA==",
    "user": {
      "login": "ejona86",
      "id": 2811396,
      "node_id": "MDQ6VXNlcjI4MTEzOTY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/2811396?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ejona86",
      "html_url": "https://github.com/ejona86",
      "followers_url": "https://api.github.com/users/ejona86/followers",
      "following_url": "https://api.github.com/users/ejona86/following{/other_user}",
      "gists_url": "https://api.github.com/users/ejona86/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ejona86/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ejona86/subscriptions",
      "organizations_url": "https://api.github.com/users/ejona86/orgs",
      "repos_url": "https://api.github.com/users/ejona86/repos",
      "events_url": "https://api.github.com/users/ejona86/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ejona86/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-10-24T21:24:03Z",
    "updated_at": "2018-10-24T21:24:03Z",
    "author_association": "MEMBER",
    "body": "> Load seems to be the most commonly stated reason why observing DNS TTL would be bad but I just don't see it.\r\n\r\nIt's not quite that simple. When caching DNS resolvers are in place a single response from the authoritative DNS server can be sent to 1000s of clients. All those clients will have the TTL expire at the same time (independent of when they originally queried) so they form a \"stampeding herd.\" Every time the TTL expires the entire \"herd\" will re-request DNS at the same time. Increasing the TTL would decrease average load but wouldn't reduce peak load.\r\n\r\nWith a limited number of clients, that can be fine. But the DNS resolver would do this in all cases, including in large-scale pick-first cases like googleapis.com. Using a consistent polling frequency doesn't cause herds, but configuration becomes a problem.\r\n\r\n> Having to use MaxConnectionAge, which just happens to be coupled to re-resolution, to emulate a polling behaviour seems like a bad workaround to me.\r\n\r\nI would call it a \"functional but non-optimal solution.\" We have to have MaxConnectionAge for other reasons, so the question is if the deficiencies are bad enough to warrant another solution for this specific case. Note that one _great_ property of the current solution is that the configuration is the service's control, and we'd want to avoid losing that property with a new solution.\r\n\r\nNote that I don't really consider the solution to be a \"workaround\" or \"hack,\" in that most of the web relies on the behavior of re-issuing DNS when reconnecting. The problem for round-robin is that it can refresh too frequently.\r\n\r\n> I don't see how making a DNS query to some DNS cache every X seconds would be seen as problematic but having to do a magnitudes more expensive re-connect plus (encryption-)handshake with each of the backends plus having to regularly refresh DNS anyway is an acceptable workaround to that.\r\n\r\nTLS Session Resumption should reduce the cost of the re-handshake to something fairly low. That said, I've not verified that our clients are using resumption and I _think_ I saw that Java is not. But that's a clearly-defined problem that could be resolved.\r\n\r\nYes, reconnecting is more expensive than a DNS query, but it is small when amortized over the lifetime of the connection. We're not trying to fully 100% optimize this one use-case at any expense, eking out every last CPU cycle in code that runs once every O(minutes). We support many use cases and we want them to work reasonably well at reasonable cost.\r\n\r\nSo to me, the discussion shouldn't be narrowly focusing on whether some alternative is more efficient than what we have now. Instead, it should focus on the problems caused by the existing approach.\r\n\r\nThis issue was started in the days that C core had very poor re-resolution behavior (which changed sometime around March, based on the markdroth's comment). The problem then was \"older clients virtually never connect to new servers,\" such that load was woefully underdistributed in normal use-cases. That has been resolved."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/432870733",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-432870733",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 432870733,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQzMjg3MDczMw==",
    "user": {
      "login": "hacst",
      "id": 237537,
      "node_id": "MDQ6VXNlcjIzNzUzNw==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/237537?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/hacst",
      "html_url": "https://github.com/hacst",
      "followers_url": "https://api.github.com/users/hacst/followers",
      "following_url": "https://api.github.com/users/hacst/following{/other_user}",
      "gists_url": "https://api.github.com/users/hacst/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/hacst/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/hacst/subscriptions",
      "organizations_url": "https://api.github.com/users/hacst/orgs",
      "repos_url": "https://api.github.com/users/hacst/repos",
      "events_url": "https://api.github.com/users/hacst/events{/privacy}",
      "received_events_url": "https://api.github.com/users/hacst/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-10-25T00:18:44Z",
    "updated_at": "2018-10-25T00:18:44Z",
    "author_association": "CONTRIBUTOR",
    "body": "@ejona86 I see your point about the stampeding herd. I hadn't considered that each cache will return the current remaining TTL of its cached value (which is kinda obvious in hindsight) so any client talking to that cache instead of the authoritative source will sync up when polling. That definitely isn't a great behaviour if you want to have thousands of clients.\r\n\r\nWhile it isn't as bad having these thousands of clients reconnecting to each single backend server with MaxConnectionAge period doesn't sound great to me either.\r\n\r\nBesides pure re-connection cost, tearing down perfectly fine connections that could otherwise stay long-running can also have other side-effects at a higher level in the stack. E.g. assume a service offers very long-running bidir streaming calls with an expensive to re-create context on the server related to the running call. In that case using MaxConnectionAge will force the client to regularly end the call and disconnect. The next call will hit some other random backend in which the context has to be re-created.\r\n\r\nCould you elaborate on the configuration issues you see with a configurable DNS polling interval disconnected from TTL? I would've thought it would just be a value like MaxConnectionAge that does nothing if not set."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/433097833",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-433097833",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 433097833,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzA5NzgzMw==",
    "user": {
      "login": "ejona86",
      "id": 2811396,
      "node_id": "MDQ6VXNlcjI4MTEzOTY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/2811396?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ejona86",
      "html_url": "https://github.com/ejona86",
      "followers_url": "https://api.github.com/users/ejona86/followers",
      "following_url": "https://api.github.com/users/ejona86/following{/other_user}",
      "gists_url": "https://api.github.com/users/ejona86/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ejona86/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ejona86/subscriptions",
      "organizations_url": "https://api.github.com/users/ejona86/orgs",
      "repos_url": "https://api.github.com/users/ejona86/repos",
      "events_url": "https://api.github.com/users/ejona86/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ejona86/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-10-25T15:31:23Z",
    "updated_at": "2018-10-25T15:31:23Z",
    "author_association": "MEMBER",
    "body": "> assume a service offers very long-running bidir streaming calls with an expensive to re-create context on the server related to the running call. In that case using MaxConnectionAge will force the client to regularly end the call and disconnect. The next call will hit some other random backend in which the context has to be re-created.\r\n\r\nSo two parts to this:\r\n1. MaxConnectionAge itself doesn't require the stream to be torn down. MaxConnectionAgeGrace will control how long old streams can live. Old streams can stay on old connection indefinitely, but will also keep the old connection alive as long as they do so. In the worst-case, over time each old stream will basically each get its own connection if streams have infinite lifetime. But we tend to expect few long-lived streams per backend.\r\n2. We've seen that service-owners frequently need to put a lifetime on streams, otherwise they aren't load balanced. If you bring up a new server, none of the existing clients will create the long-lived stream to the new server because they are happily connected to an old server.\r\n\r\nIt is possible to develop a client-side LB policy that uses affinity to consistently route long-lived streams back to the same warm backends, but because of (2) it puts you in a bit of a bind for distributing load when applied to this use-case. (This affinity-based system actually exists in gRPC Java today but the design went into a weird limbo state as we resolve some larger LB discussions. It was implemented in Go as well, but was reverted because the design went into limbo state. It is powered off service config, which isn't ready for prime-time, though.)\r\n\r\n> Could you elaborate on the configuration issues you see with a configurable DNS polling interval disconnected from TTL? I would've thought it would just be a value like MaxConnectionAge that does nothing if not set.\r\n\r\nMaxConnectionAge is configured on server-side. So if the service owner needs to change the value, they can change it fairly rapidly. Most obvious forms of configuring DNS polling interval would place it hard-coded on client-side, which means it can take O(years) for clients to pick up any change. Yes, some service owners control their clients and so it wouldn't be a problem, but many don't and so the solution would have more limited applicability.\r\n\r\nWhile it could be possible to provide live configuration to the client via the service config, that necessitates increasing the complexity of the solution."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/460806123",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-460806123",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 460806123,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ2MDgwNjEyMw==",
    "user": {
      "login": "srini100",
      "id": 35056280,
      "node_id": "MDQ6VXNlcjM1MDU2Mjgw",
      "avatar_url": "https://avatars2.githubusercontent.com/u/35056280?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/srini100",
      "html_url": "https://github.com/srini100",
      "followers_url": "https://api.github.com/users/srini100/followers",
      "following_url": "https://api.github.com/users/srini100/following{/other_user}",
      "gists_url": "https://api.github.com/users/srini100/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/srini100/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/srini100/subscriptions",
      "organizations_url": "https://api.github.com/users/srini100/orgs",
      "repos_url": "https://api.github.com/users/srini100/repos",
      "events_url": "https://api.github.com/users/srini100/events{/privacy}",
      "received_events_url": "https://api.github.com/users/srini100/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-02-05T21:14:24Z",
    "updated_at": "2019-02-05T21:14:24Z",
    "author_association": "CONTRIBUTOR",
    "body": "In theory, the timing of connection aging can line up in such a way that all backends drop connections almost at the same time causing increased latency spikes on clients even in a steady-state system. This would not be the case with periodic DNS refresh. \r\n\r\nBoth solutions (periodic DNS refresh and max connection age) have pros and cons. Are there are any users who have run into practical issues with max connection age solution? \r\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/473436615",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-473436615",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 473436615,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ3MzQzNjYxNQ==",
    "user": {
      "login": "schmohlio",
      "id": 3815022,
      "node_id": "MDQ6VXNlcjM4MTUwMjI=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/3815022?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/schmohlio",
      "html_url": "https://github.com/schmohlio",
      "followers_url": "https://api.github.com/users/schmohlio/followers",
      "following_url": "https://api.github.com/users/schmohlio/following{/other_user}",
      "gists_url": "https://api.github.com/users/schmohlio/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/schmohlio/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/schmohlio/subscriptions",
      "organizations_url": "https://api.github.com/users/schmohlio/orgs",
      "repos_url": "https://api.github.com/users/schmohlio/repos",
      "events_url": "https://api.github.com/users/schmohlio/events{/privacy}",
      "received_events_url": "https://api.github.com/users/schmohlio/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-03-15T20:50:08Z",
    "updated_at": "2019-03-15T20:50:08Z",
    "author_association": "NONE",
    "body": "bumping this - couldn't MaxConnectionAge approach also lead to a \"stampeding herd\"?  Also curious if anyone has experienced issues with the connection-based approach.\r\n\r\nWe would like to make use of out-of-the-box autoscaling features, whereby DNS records are added when new boxes come online. It's preferable that we don't need to bounce a service or set MaxConnectionAge (as this reduces the responsiveness of overall autoscaling approach)."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/479267554",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-479267554",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 479267554,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ3OTI2NzU1NA==",
    "user": {
      "login": "ejona86",
      "id": 2811396,
      "node_id": "MDQ6VXNlcjI4MTEzOTY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/2811396?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ejona86",
      "html_url": "https://github.com/ejona86",
      "followers_url": "https://api.github.com/users/ejona86/followers",
      "following_url": "https://api.github.com/users/ejona86/following{/other_user}",
      "gists_url": "https://api.github.com/users/ejona86/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ejona86/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ejona86/subscriptions",
      "organizations_url": "https://api.github.com/users/ejona86/orgs",
      "repos_url": "https://api.github.com/users/ejona86/repos",
      "events_url": "https://api.github.com/users/ejona86/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ejona86/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-04-03T00:17:29Z",
    "updated_at": "2019-04-03T00:17:29Z",
    "author_association": "MEMBER",
    "body": "> couldn't MaxConnectionAge approach also lead to a \"stampeding herd\"?\r\n\r\nNo, because it is based on the original connection creation. It can perpetuate a stampeding herd (by making it reoccur), but wouldn't be the cause. This is also why it uses a 10% randomization factor.\r\n\r\n> It's preferable that we don't need to bounce a service or set MaxConnectionAge (as this reduces the responsiveness of overall autoscaling approach).\r\n\r\nServices using DNS today create new connections _very_ frequently. If you want a MaxConnectionAge of 30 seconds, that's okay (although low ages may be more problematic for Go clients). The connection has been utilized _much_ better than it would have with HTTP/1 and the cost has likely been amortized over many RPCs. HTTP servers frequently shut down connections much younger.\r\n\r\n(Go clients currently reconnect eagerly. So with a MaxConnectionAge of 30 seconds the clients will reconnect every 30 seconds, even if they only do an RPC once an hour. This is a TODO for Go, and doesn't impact many users.)"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/479272001",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-479272001",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 479272001,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ3OTI3MjAwMQ==",
    "user": {
      "login": "wjywbs",
      "id": 6233680,
      "node_id": "MDQ6VXNlcjYyMzM2ODA=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/6233680?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/wjywbs",
      "html_url": "https://github.com/wjywbs",
      "followers_url": "https://api.github.com/users/wjywbs/followers",
      "following_url": "https://api.github.com/users/wjywbs/following{/other_user}",
      "gists_url": "https://api.github.com/users/wjywbs/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/wjywbs/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wjywbs/subscriptions",
      "organizations_url": "https://api.github.com/users/wjywbs/orgs",
      "repos_url": "https://api.github.com/users/wjywbs/repos",
      "events_url": "https://api.github.com/users/wjywbs/events{/privacy}",
      "received_events_url": "https://api.github.com/users/wjywbs/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-04-03T00:34:33Z",
    "updated_at": "2019-04-03T00:34:33Z",
    "author_association": "NONE",
    "body": "I tried to enable MaxConnectionAge on a server and call from a Go client. When the connection age was reached, the rpcs in the Go client failed with the unavailable error and the \"transport is closing\" message. Then I gave up on this MaxConnectionAge approach to avoid extra retry logics in the code."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/479272721",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-479272721",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 479272721,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ3OTI3MjcyMQ==",
    "user": {
      "login": "ejona86",
      "id": 2811396,
      "node_id": "MDQ6VXNlcjI4MTEzOTY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/2811396?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ejona86",
      "html_url": "https://github.com/ejona86",
      "followers_url": "https://api.github.com/users/ejona86/followers",
      "following_url": "https://api.github.com/users/ejona86/following{/other_user}",
      "gists_url": "https://api.github.com/users/ejona86/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ejona86/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ejona86/subscriptions",
      "organizations_url": "https://api.github.com/users/ejona86/orgs",
      "repos_url": "https://api.github.com/users/ejona86/repos",
      "events_url": "https://api.github.com/users/ejona86/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ejona86/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-04-03T00:37:17Z",
    "updated_at": "2019-04-03T00:37:17Z",
    "author_association": "MEMBER",
    "body": "@wjywbs, is there a bug open for that? That should not happen. @dfawley, do you know what could tickle the Go client to do that?"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/479276356",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-479276356",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 479276356,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ3OTI3NjM1Ng==",
    "user": {
      "login": "dfawley",
      "id": 26072277,
      "node_id": "MDQ6VXNlcjI2MDcyMjc3",
      "avatar_url": "https://avatars1.githubusercontent.com/u/26072277?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dfawley",
      "html_url": "https://github.com/dfawley",
      "followers_url": "https://api.github.com/users/dfawley/followers",
      "following_url": "https://api.github.com/users/dfawley/following{/other_user}",
      "gists_url": "https://api.github.com/users/dfawley/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dfawley/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dfawley/subscriptions",
      "organizations_url": "https://api.github.com/users/dfawley/orgs",
      "repos_url": "https://api.github.com/users/dfawley/repos",
      "events_url": "https://api.github.com/users/dfawley/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dfawley/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-04-03T00:50:44Z",
    "updated_at": "2019-04-03T00:50:44Z",
    "author_association": "CONTRIBUTOR",
    "body": "The only thing I can think is if you have long-running RPCs, max connection age will eventually time out (after `MaxConnectionAgeGrace`) and hard-close the connection.  Otherwise the RPCs are expected to complete successfully, and I'd be pretty surprised if we don't have tests for this.  If the above doesn't explain the errors you're getting, please file a bug in the grpc-go repo about that."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/479308577",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-479308577",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 479308577,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ3OTMwODU3Nw==",
    "user": {
      "login": "wjywbs",
      "id": 6233680,
      "node_id": "MDQ6VXNlcjYyMzM2ODA=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/6233680?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/wjywbs",
      "html_url": "https://github.com/wjywbs",
      "followers_url": "https://api.github.com/users/wjywbs/followers",
      "following_url": "https://api.github.com/users/wjywbs/following{/other_user}",
      "gists_url": "https://api.github.com/users/wjywbs/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/wjywbs/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wjywbs/subscriptions",
      "organizations_url": "https://api.github.com/users/wjywbs/orgs",
      "repos_url": "https://api.github.com/users/wjywbs/repos",
      "events_url": "https://api.github.com/users/wjywbs/events{/privacy}",
      "received_events_url": "https://api.github.com/users/wjywbs/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-04-03T02:42:05Z",
    "updated_at": "2019-04-03T02:42:05Z",
    "author_association": "NONE",
    "body": "Thanks for your help. I didn't set the grace time last time. However, when I tested again with both age and grace time set to one minute, the Go client reported lots of errors as well. Each rpc takes a few seconds to complete, within the one minute grace period.\r\n\r\n```\r\ngrpc-go v1.18.0/v1.19.1\r\nrpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: <nil>\r\nrpc error: code = Unavailable desc = the connection is draining\r\nrpc error: code = Unavailable desc = transport is closing\r\n```"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/480413884",
    "html_url": "https://github.com/grpc/grpc/issues/12295#issuecomment-480413884",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/12295",
    "id": 480413884,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ4MDQxMzg4NA==",
    "user": {
      "login": "ejona86",
      "id": 2811396,
      "node_id": "MDQ6VXNlcjI4MTEzOTY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/2811396?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ejona86",
      "html_url": "https://github.com/ejona86",
      "followers_url": "https://api.github.com/users/ejona86/followers",
      "following_url": "https://api.github.com/users/ejona86/following{/other_user}",
      "gists_url": "https://api.github.com/users/ejona86/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ejona86/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ejona86/subscriptions",
      "organizations_url": "https://api.github.com/users/ejona86/orgs",
      "repos_url": "https://api.github.com/users/ejona86/repos",
      "events_url": "https://api.github.com/users/ejona86/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ejona86/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-04-05T20:35:55Z",
    "updated_at": "2019-04-05T20:35:55Z",
    "author_association": "MEMBER",
    "body": "@wjywbs, please file that as an issue on grpc-go's repo. That does not appear to be expected behavior."
  }
]
