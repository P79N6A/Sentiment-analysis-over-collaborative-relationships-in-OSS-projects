[
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/450212451",
    "html_url": "https://github.com/grpc/grpc/issues/17569#issuecomment-450212451",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/17569",
    "id": 450212451,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ1MDIxMjQ1MQ==",
    "user": {
      "login": "ericgribkoff",
      "id": 5067076,
      "node_id": "MDQ6VXNlcjUwNjcwNzY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/5067076?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ericgribkoff",
      "html_url": "https://github.com/ericgribkoff",
      "followers_url": "https://api.github.com/users/ericgribkoff/followers",
      "following_url": "https://api.github.com/users/ericgribkoff/following{/other_user}",
      "gists_url": "https://api.github.com/users/ericgribkoff/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ericgribkoff/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ericgribkoff/subscriptions",
      "organizations_url": "https://api.github.com/users/ericgribkoff/orgs",
      "repos_url": "https://api.github.com/users/ericgribkoff/repos",
      "events_url": "https://api.github.com/users/ericgribkoff/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ericgribkoff/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-12-27T19:11:24Z",
    "updated_at": "2018-12-27T19:11:24Z",
    "author_association": "CONTRIBUTOR",
    "body": "Could you provide more details on what you are trying to accomplish?\r\n\r\nAs for the \"hard limit on concurrent RPCs\", [`grpc.create_server`](https://grpc.io/grpc/python/grpc.html#create-server) has a `maximum_concurrent_rpcs` parameter that defaults to `None`, indicating no limit on the number of concurrent RPCs. The thread pool provided to the server  limits the maximum number of RPCs that are processed simultaneously, as once all threads are in use future RPCs will be queued up, but they should not be failed.\r\n\r\nIt sounds like your goal is to set separate hard limits for different services via a per-service equivalent of `maximum_concurrent_rpcs`, but additional details about what specifically you need will help us determine if this actually requires multiple completion queues or an alternative approach may suffice. Thanks.\r\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/450443034",
    "html_url": "https://github.com/grpc/grpc/issues/17569#issuecomment-450443034",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/17569",
    "id": 450443034,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ1MDQ0MzAzNA==",
    "user": {
      "login": "therc",
      "id": 13481082,
      "node_id": "MDQ6VXNlcjEzNDgxMDgy",
      "avatar_url": "https://avatars3.githubusercontent.com/u/13481082?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/therc",
      "html_url": "https://github.com/therc",
      "followers_url": "https://api.github.com/users/therc/followers",
      "following_url": "https://api.github.com/users/therc/following{/other_user}",
      "gists_url": "https://api.github.com/users/therc/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/therc/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/therc/subscriptions",
      "organizations_url": "https://api.github.com/users/therc/orgs",
      "repos_url": "https://api.github.com/users/therc/repos",
      "events_url": "https://api.github.com/users/therc/events{/privacy}",
      "received_events_url": "https://api.github.com/users/therc/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-12-28T23:30:46Z",
    "updated_at": "2018-12-28T23:30:46Z",
    "author_association": "NONE",
    "body": "I'm trying to prevent overloaded servers from\r\n1. running out of memory\r\n1. getting killed while still processing requests\r\n\r\n`maximum_concurrent_rpcs` takes care of the first.  Under heavy load, queued up RPCs might actually fail, in a way, e.g. if the deadline is exceeded because of excessive backlog.\r\n\r\nWhen running the server under Kubernetes, you can use two different kinds of health checks:\r\n1. readiness: I'm ready to receive traffic from the load balancer\r\n1. liveness: I'm alive and doing work, please do not kill me\r\n\r\nThe problem under load is that both health checks (which have a short timeout) will fail. The readiness check is expected to fail and, if everything is set up correctly, will prevent the server instance from receiving further requests from the load balancer, until the backlog has cleared.\r\n\r\nIf the liveness check fails, though, Kubernetes will kill the server, which has two bad side effects:\r\n* you can lose requests that were in flight\r\n* you add gratuitous downtime, especially if the server does not restart instantly, e.g. because it needs to read configuration data from somewhere else or to precompute internal objects\r\n\r\nSo, for example, I want to run a server that\r\n\r\n- processes up to ten different `/RouteGuide/*`methods (from the gRPC demos) at the same time, queuing the rest\r\n- still answers one (maybe two, if really generous) `/Health/Check` request from the standard `grpc.health.v1` package, no matter how busy the other queue is.\r\n\r\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/450465181",
    "html_url": "https://github.com/grpc/grpc/issues/17569#issuecomment-450465181",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/17569",
    "id": 450465181,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ1MDQ2NTE4MQ==",
    "user": {
      "login": "ericgribkoff",
      "id": 5067076,
      "node_id": "MDQ6VXNlcjUwNjcwNzY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/5067076?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ericgribkoff",
      "html_url": "https://github.com/ericgribkoff",
      "followers_url": "https://api.github.com/users/ericgribkoff/followers",
      "following_url": "https://api.github.com/users/ericgribkoff/following{/other_user}",
      "gists_url": "https://api.github.com/users/ericgribkoff/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ericgribkoff/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ericgribkoff/subscriptions",
      "organizations_url": "https://api.github.com/users/ericgribkoff/orgs",
      "repos_url": "https://api.github.com/users/ericgribkoff/repos",
      "events_url": "https://api.github.com/users/ericgribkoff/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ericgribkoff/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-12-29T04:40:43Z",
    "updated_at": "2018-12-29T04:40:43Z",
    "author_association": "CONTRIBUTOR",
    "body": "Thanks for the detailed explanation. That seems like a very reasonable use case and one that we should support.\r\n\r\nFrom a look over our code, it appears that the necessary machinery to support multiple completion queues is already present in our Cython layer, wrapped around gRPC's core API. The Python layer (`_server.py`) will need to be modified to actually instantiate multiple queues, and we will also need to modify our current single threadpool usage to support priorities as well (the servicer invocation to handle in incoming rpc is executed via the threadpool). I would guess the API might be something like one threadpool per completion queue, but this will need some more thought.\r\n\r\nI don't have an ETA for this just yet but I'll post another update here sometime in the next week."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/451999516",
    "html_url": "https://github.com/grpc/grpc/issues/17569#issuecomment-451999516",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/17569",
    "id": 451999516,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ1MTk5OTUxNg==",
    "user": {
      "login": "ericgribkoff",
      "id": 5067076,
      "node_id": "MDQ6VXNlcjUwNjcwNzY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/5067076?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ericgribkoff",
      "html_url": "https://github.com/ericgribkoff",
      "followers_url": "https://api.github.com/users/ericgribkoff/followers",
      "following_url": "https://api.github.com/users/ericgribkoff/following{/other_user}",
      "gists_url": "https://api.github.com/users/ericgribkoff/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ericgribkoff/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ericgribkoff/subscriptions",
      "organizations_url": "https://api.github.com/users/ericgribkoff/orgs",
      "repos_url": "https://api.github.com/users/ericgribkoff/repos",
      "events_url": "https://api.github.com/users/ericgribkoff/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ericgribkoff/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-01-07T16:46:56Z",
    "updated_at": "2019-01-07T16:46:56Z",
    "author_association": "CONTRIBUTOR",
    "body": "Quick update: I've added implementing this feature to my todo list for this quarter (community contributions are always welcomed as well :))\r\n\r\nAs far as the API goes, @lidizheng suggested adding an optional \"annotation\"/member to the servicer that, when present, provides a service-specific `futures.ThreadPoolExecutor` to gRPC Python for handling that service's RPCs. Whether each such servicer gets its own completion queue or whether we ensure one completion queue per unique ThreadPoolExecutor (in case two+ services are annotated with the same executor instance) also needs to be resolved."
  }
]
