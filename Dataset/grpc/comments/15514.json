[
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/401177056",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-401177056",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 401177056,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwMTE3NzA1Ng==",
    "user": {
      "login": "oozie",
      "id": 1587140,
      "node_id": "MDQ6VXNlcjE1ODcxNDA=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/1587140?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/oozie",
      "html_url": "https://github.com/oozie",
      "followers_url": "https://api.github.com/users/oozie/followers",
      "following_url": "https://api.github.com/users/oozie/following{/other_user}",
      "gists_url": "https://api.github.com/users/oozie/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/oozie/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/oozie/subscriptions",
      "organizations_url": "https://api.github.com/users/oozie/orgs",
      "repos_url": "https://api.github.com/users/oozie/repos",
      "events_url": "https://api.github.com/users/oozie/events{/privacy}",
      "received_events_url": "https://api.github.com/users/oozie/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-06-28T21:20:47Z",
    "updated_at": "2018-06-28T21:20:47Z",
    "author_association": "NONE",
    "body": "We have worked around the issue in https://github.com/cloudfoundry-incubator/perm-rb/commit/0afccde31148878c4bf2215510cd9d71f0894f0e by explicitly creating a shared channel and passing it to each of the two client stubs. "
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/405484730",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-405484730",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 405484730,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTQ4NDczMA==",
    "user": {
      "login": "apolcyn",
      "id": 9566254,
      "node_id": "MDQ6VXNlcjk1NjYyNTQ=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/9566254?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/apolcyn",
      "html_url": "https://github.com/apolcyn",
      "followers_url": "https://api.github.com/users/apolcyn/followers",
      "following_url": "https://api.github.com/users/apolcyn/following{/other_user}",
      "gists_url": "https://api.github.com/users/apolcyn/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/apolcyn/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/apolcyn/subscriptions",
      "organizations_url": "https://api.github.com/users/apolcyn/orgs",
      "repos_url": "https://api.github.com/users/apolcyn/repos",
      "events_url": "https://api.github.com/users/apolcyn/events{/privacy}",
      "received_events_url": "https://api.github.com/users/apolcyn/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-17T07:19:51Z",
    "updated_at": "2018-07-17T07:33:40Z",
    "author_association": "CONTRIBUTOR",
    "body": "@iredelmeier great repro case, thanks!\r\n\r\nthis is very reproduceable, and I'm seeing it reproduce even with two different channels of the <i>same</i> service.\r\n\r\nI've trimmed this down further to something along the lines of:\r\n\r\n```\r\n# note stub_1 and stub_2 internally create new and separate channels, in ruby\r\nstub_1 = <service class>(\"127.0.0.1:1000\", :this_channel_is_insecure)\r\nstub_2 = <service class>(\"127.0.0.1:1000\", :this_channel_is_insecure)\r\nstub_1.do_call()\r\nstub_2.do_call()\r\nstub_2.do_call() # receives a GOAWAY\r\n```\r\n\r\n(using 127.0.0.1 directly is making the GOAWAY happen sooner because we're cycling through all possible addresses sooner, rather than when using localhost and needing to cycle through both ipv4 and ipv6 loopback addresses).\r\n\r\nNot at the bottom of this but it's looking like something along these lines:\r\n\r\n1) channel C1 creates subchannel S1, makes a call on it, and then receives a GOAWAY. Upon receiving a GOAWAY it unref's S1, destroys it and removes it from the index. C1 also re-resolves and creates a new subchannel S2\r\n2) channel C2 creates a call, matches S2 in the subchannel index, performs the call, and then receives a GOAWAY. It unref's S2 but doesn't destroy it because C1 is somehow still holding a ref on it\r\n3) channel C2 creates a second call, matches S2 in the subchannel index, attempts to perform a call on it, but then finds S2 in \"transient failure\" state because of the previous GOAWAY, and then fails.\r\n\r\nIt seems strange that that \"ref\" from C1 is causing C2 to keep matching and using S2, even when S2 is not useable....\r\n\r\ncc @markdroth  @dgquintas @AspirinSJL if there might be any known issues around this"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/405671603",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-405671603",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 405671603,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTY3MTYwMw==",
    "user": {
      "login": "AspirinSJL",
      "id": 3314176,
      "node_id": "MDQ6VXNlcjMzMTQxNzY=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/3314176?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AspirinSJL",
      "html_url": "https://github.com/AspirinSJL",
      "followers_url": "https://api.github.com/users/AspirinSJL/followers",
      "following_url": "https://api.github.com/users/AspirinSJL/following{/other_user}",
      "gists_url": "https://api.github.com/users/AspirinSJL/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AspirinSJL/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AspirinSJL/subscriptions",
      "organizations_url": "https://api.github.com/users/AspirinSJL/orgs",
      "repos_url": "https://api.github.com/users/AspirinSJL/repos",
      "events_url": "https://api.github.com/users/AspirinSJL/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AspirinSJL/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-17T17:58:41Z",
    "updated_at": "2018-07-17T17:58:41Z",
    "author_association": "MEMBER",
    "body": "@apolcyn C2 unref's S2 due to connectivity change to TRANSIENT_FAILURE right? I think C1 should also have received the connectivity change notification and unref'ed C2. Could you check why C1 didn't behave like that or paste the log with the relevant tracers on? Thanks!"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/405722050",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-405722050",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 405722050,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTcyMjA1MA==",
    "user": {
      "login": "apolcyn",
      "id": 9566254,
      "node_id": "MDQ6VXNlcjk1NjYyNTQ=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/9566254?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/apolcyn",
      "html_url": "https://github.com/apolcyn",
      "followers_url": "https://api.github.com/users/apolcyn/followers",
      "following_url": "https://api.github.com/users/apolcyn/following{/other_user}",
      "gists_url": "https://api.github.com/users/apolcyn/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/apolcyn/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/apolcyn/subscriptions",
      "organizations_url": "https://api.github.com/users/apolcyn/orgs",
      "repos_url": "https://api.github.com/users/apolcyn/repos",
      "events_url": "https://api.github.com/users/apolcyn/events{/privacy}",
      "received_events_url": "https://api.github.com/users/apolcyn/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-17T20:46:28Z",
    "updated_at": "2018-07-17T20:47:06Z",
    "author_association": "CONTRIBUTOR",
    "body": "here is my slightly trimmed down repro:\r\n\r\n```\r\n#!/usr/bin/env ruby\r\n\r\nthis_dir = File.expand_path(File.dirname(__FILE__))\r\nprotos_lib_dir = File.join(this_dir, 'lib')\r\ngrpc_lib_dir = File.join(File.dirname(File.dirname(File.dirname(this_dir))), 'grpc', 'src', 'ruby', 'lib')\r\n$LOAD_PATH.unshift(grpc_lib_dir) unless $LOAD_PATH.include?(grpc_lib_dir)\r\n$LOAD_PATH.unshift(protos_lib_dir) unless $LOAD_PATH.include?(protos_lib_dir)\r\n$LOAD_PATH.unshift(this_dir) unless $LOAD_PATH.include?(this_dir)\r\n\r\nrequire 'grpc'\r\nrequire 'decrementer_services_pb'\r\nrequire 'incrementer_services_pb'\r\n\r\ndef decrement(stub)\r\n  STDERR.puts 'Calling first service...'\r\n  stub.decrement(Goaway::DecrementRequest.new).count\r\n  STDERR.puts 'Called first service'\r\n  STDERR.puts 'Sleeping for 1 second...'\r\n  sleep 1.5\r\nend\r\n\r\ndef main\r\n  decrementer_stub1 = Goaway::Decrementer::Stub.new(\r\n    '127.0.0.1:50051',\r\n    :this_channel_is_insecure\r\n  )\r\n  decrementer_stub2 = Goaway::Decrementer::Stub.new(\r\n    '127.0.0.1:50051',\r\n    :this_channel_is_insecure\r\n  )\r\n\r\n  STDERR.puts 'Decrement on stub 1'\r\n  decrement(decrementer_stub1)\r\n  STDERR.puts 'Decrement on stub 2'\r\n  decrement(decrementer_stub2)\r\n  STDERR.puts 'Decrement on stub 2'\r\n  decrement(decrementer_stub2)\r\n  STDERR.puts 'Decrement on stub 2'\r\n  decrement(decrementer_stub2)\r\n  return\r\nend\r\n```\r\n\r\nhere is the log: https://gist.github.com/apolcyn/22ba8d5e796461ee21d8434a891f88d6\r\n\r\n.. when running under `client_channel` and `pick_first` tracers."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/405722420",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-405722420",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 405722420,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTcyMjQyMA==",
    "user": {
      "login": "apolcyn",
      "id": 9566254,
      "node_id": "MDQ6VXNlcjk1NjYyNTQ=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/9566254?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/apolcyn",
      "html_url": "https://github.com/apolcyn",
      "followers_url": "https://api.github.com/users/apolcyn/followers",
      "following_url": "https://api.github.com/users/apolcyn/following{/other_user}",
      "gists_url": "https://api.github.com/users/apolcyn/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/apolcyn/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/apolcyn/subscriptions",
      "organizations_url": "https://api.github.com/users/apolcyn/orgs",
      "repos_url": "https://api.github.com/users/apolcyn/repos",
      "events_url": "https://api.github.com/users/apolcyn/events{/privacy}",
      "received_events_url": "https://api.github.com/users/apolcyn/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-17T20:47:50Z",
    "updated_at": "2018-07-17T20:47:50Z",
    "author_association": "CONTRIBUTOR",
    "body": "> Could you check why C1 didn't behave like that or paste the log with the relevant tracers on? Thanks!\r\n\r\n@AspirinSJL I'm looking in to why C1 didn't pick up that notification and unref"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/405733781",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-405733781",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 405733781,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTczMzc4MQ==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-17T21:27:33Z",
    "updated_at": "2018-07-19T14:28:15Z",
    "author_association": "MEMBER",
    "body": "Note that the effect of #12767 (the PR containing the commit that was mentioned above to have triggered this problem) was to allow subchannel sharing for secure channels, which had been broken by a much earlier change.  That's the behavior we want, but it may be that it's triggering a bug that wouln't have been encountered if the various channels were not sharing subchannels.\r\n\r\nIf the original poster was using insecure channels, then that PR should not have had any effect."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/405765260",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-405765260",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 405765260,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTc2NTI2MA==",
    "user": {
      "login": "AspirinSJL",
      "id": 3314176,
      "node_id": "MDQ6VXNlcjMzMTQxNzY=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/3314176?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AspirinSJL",
      "html_url": "https://github.com/AspirinSJL",
      "followers_url": "https://api.github.com/users/AspirinSJL/followers",
      "following_url": "https://api.github.com/users/AspirinSJL/following{/other_user}",
      "gists_url": "https://api.github.com/users/AspirinSJL/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AspirinSJL/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AspirinSJL/subscriptions",
      "organizations_url": "https://api.github.com/users/AspirinSJL/orgs",
      "repos_url": "https://api.github.com/users/AspirinSJL/repos",
      "events_url": "https://api.github.com/users/AspirinSJL/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AspirinSJL/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-18T00:00:16Z",
    "updated_at": "2018-07-18T00:00:29Z",
    "author_association": "MEMBER",
    "body": "@markdroth Just want to make sure, the subchannels are shared for insecure channels, right?"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/405946031",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-405946031",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 405946031,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTk0NjAzMQ==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-18T14:14:58Z",
    "updated_at": "2018-07-18T14:14:58Z",
    "author_association": "MEMBER",
    "body": "Yes."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/406045242",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-406045242",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 406045242,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjA0NTI0Mg==",
    "user": {
      "login": "apolcyn",
      "id": 9566254,
      "node_id": "MDQ6VXNlcjk1NjYyNTQ=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/9566254?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/apolcyn",
      "html_url": "https://github.com/apolcyn",
      "followers_url": "https://api.github.com/users/apolcyn/followers",
      "following_url": "https://api.github.com/users/apolcyn/following{/other_user}",
      "gists_url": "https://api.github.com/users/apolcyn/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/apolcyn/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/apolcyn/subscriptions",
      "organizations_url": "https://api.github.com/users/apolcyn/orgs",
      "repos_url": "https://api.github.com/users/apolcyn/repos",
      "events_url": "https://api.github.com/users/apolcyn/events{/privacy}",
      "received_events_url": "https://api.github.com/users/apolcyn/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-18T19:22:04Z",
    "updated_at": "2018-07-18T19:22:04Z",
    "author_association": "CONTRIBUTOR",
    "body": "@AspirinSJL \r\n\r\n> @apolcyn C2 unref's S2 due to connectivity change to TRANSIENT_FAILURE right? I think C1 should also have received the connectivity change notification and unref'ed C2. Could you check why C1 didn't behave like that or paste the log with the relevant tracers on? Thanks!\r\n\r\nWith more looking, it actually seems that C1 is not picking up the connectivity change notification on S2 because it never starts watching it.\r\n\r\nThat is (from what I can tell), when C1 receives the GOAWAY on S1, it [it's pick first policy set's started_picking_=false](https://github.com/grpc/grpc/blob/master/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc#L514) and then it re-resolves. Then, upon re-resolving and creating S2, C1's pick fist policy [does not start a new connectivity watch](https://github.com/grpc/grpc/blob/master/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc#L406) because it's <i>started_picking_=false</i>, and C1 will never start a new watch on S2 because we never create a new call on C1 and so C1 never starts picking.\r\n\r\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/406123713",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-406123713",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 406123713,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjEyMzcxMw==",
    "user": {
      "login": "AspirinSJL",
      "id": 3314176,
      "node_id": "MDQ6VXNlcjMzMTQxNzY=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/3314176?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AspirinSJL",
      "html_url": "https://github.com/AspirinSJL",
      "followers_url": "https://api.github.com/users/AspirinSJL/followers",
      "following_url": "https://api.github.com/users/AspirinSJL/following{/other_user}",
      "gists_url": "https://api.github.com/users/AspirinSJL/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AspirinSJL/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AspirinSJL/subscriptions",
      "organizations_url": "https://api.github.com/users/AspirinSJL/orgs",
      "repos_url": "https://api.github.com/users/AspirinSJL/repos",
      "events_url": "https://api.github.com/users/AspirinSJL/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AspirinSJL/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-19T01:18:58Z",
    "updated_at": "2018-07-19T01:18:58Z",
    "author_association": "MEMBER",
    "body": "I see the problem.. Thanks for digging into this!\r\n\r\nSo this issue will happen as long as pick_first hasn't started picking, not just in case of re-resolution. The separation of subchannel reffing and state watching causes pick first to be unaware of that the reffed subchannel is already unusable. As a result, the subchannel is in TRANSIENT_FAILURE (even worse, with a GOAWAY error) when it's used next time. It will eventually be READY (if the server is up again), but before that the picks will fail if that's the only subchannel.\r\n\r\nOne possible fix is to force invalidating the subchannel when something really bad (like, GOAWAY received) happens. In subchannel_index, any invalid subchannel is invisible to future users. So the next LB policy will create a new subchannel from IDLE state. Before an LB policy starts watching a subchannel, it will check whether it's still valid or not.\r\n\r\n@markdroth Does that sound reasonable?\r\n\r\nBTW, I don't know why C2 still tries to reconnect to the subchannel after it sees the subchannel has a GOAWAY error in step 3 though. I thought GOAWAY means no retry. \r\n\r\n\r\n"
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/406318916",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-406318916",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 406318916,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjMxODkxNg==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-19T15:32:51Z",
    "updated_at": "2018-07-19T15:32:51Z",
    "author_association": "MEMBER",
    "body": "Ideally, PF should handle a GOAWAY by putting the channel into idle state, although the exact desired behavior is not well defined.  For example, it could be argued that we should really handle this by completely resetting the client_channel back to its initial state: unref the LB policy and stop the resolver from resolving.  Currently, we don't do any of that; instead, we re-resolve, create subchannels based on the resolver result, but then don't watch the connectivity state of the new subchannels, which seems pretty stupid -- we're doing some work we don't need to and leaving things in a bit of an inconsistent state (thus leading to problems like this bug).  @ejona86, @dfawley, and I were planning to write up an idleness design later this quarter to clearly define what the right behavior should be here.\r\n\r\nFor now, I agree that it seems wrong that we take a ref to a subchannel that we're not watching.  However, it's not clear to me that we can solve this problem by removing the subchannel from the index, because we would need some way to do that atomically with respect to the connectivity state notification.  Otherwise, there will always be a window during which another channel might still grab the subchannel from the index.\r\n\r\nAnother possible interim solution would be to change PF so that when it receives an update and `started_picking_` is false, it will save the addresses from the update but not actually create the subchannel list for those addresses until it does start picking.  However, if we do this, we should do it as non-invasively as possible, because if we later decide to handle this by resetting the client_channel back to its initial state, then we would want to remove this code.\r\n\r\nAlternatively, maybe the right thing to do here is to go ahead and do the work of resetting the client_channel to its initial state.  We could do that by having PF report IDLE, and then having client_channel see that state and unref the LB policy and stop resolving.  That might actually be both easier and cleaner, and it would also allow us to simplify the LB policy API slightly (internally, see http://go/grpc-c-core-client-channel-roadmap#heading=h.78p3kqsnue7z for details).\r\n\r\nLet's chat offline with Eric and Doug to decide how to proceed."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/406360535",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-406360535",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 406360535,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjM2MDUzNQ==",
    "user": {
      "login": "AspirinSJL",
      "id": 3314176,
      "node_id": "MDQ6VXNlcjMzMTQxNzY=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/3314176?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AspirinSJL",
      "html_url": "https://github.com/AspirinSJL",
      "followers_url": "https://api.github.com/users/AspirinSJL/followers",
      "following_url": "https://api.github.com/users/AspirinSJL/following{/other_user}",
      "gists_url": "https://api.github.com/users/AspirinSJL/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AspirinSJL/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AspirinSJL/subscriptions",
      "organizations_url": "https://api.github.com/users/AspirinSJL/orgs",
      "repos_url": "https://api.github.com/users/AspirinSJL/repos",
      "events_url": "https://api.github.com/users/AspirinSJL/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AspirinSJL/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-19T17:49:50Z",
    "updated_at": "2018-07-19T17:49:50Z",
    "author_association": "MEMBER",
    "body": "Yeah, this needs thorough thought.\r\n\r\nRegarding my proposed fix (which might be obsolete now), I think it's fine if some channel runs into the race. If a channel refs the subchannel just before it's labeled as invalid, there are two cases. (1) If the picking and watching also starts before the labeling, then that channel may have to accept that the server is not ready around that point and the next pick might fail. Even if the channel created a new subchannel, the server might still send it a GOAWAY. (2) If the picking and watching starts after the labeling, the invalidity of the subchannel will be detected if we add a check when the channel starts watching a subchannel."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/406363921",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-406363921",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 406363921,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjM2MzkyMQ==",
    "user": {
      "login": "AspirinSJL",
      "id": 3314176,
      "node_id": "MDQ6VXNlcjMzMTQxNzY=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/3314176?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AspirinSJL",
      "html_url": "https://github.com/AspirinSJL",
      "followers_url": "https://api.github.com/users/AspirinSJL/followers",
      "following_url": "https://api.github.com/users/AspirinSJL/following{/other_user}",
      "gists_url": "https://api.github.com/users/AspirinSJL/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AspirinSJL/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AspirinSJL/subscriptions",
      "organizations_url": "https://api.github.com/users/AspirinSJL/orgs",
      "repos_url": "https://api.github.com/users/AspirinSJL/repos",
      "events_url": "https://api.github.com/users/AspirinSJL/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AspirinSJL/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-19T18:01:25Z",
    "updated_at": "2018-07-19T18:01:25Z",
    "author_association": "MEMBER",
    "body": "Also, about why C2 still tries to re-connect to S2 when it sees a GOAWAY error. Is that because the discontinuation upon GOAWAY is only triggered by transport layer when the GOAWAY frame is initially received? "
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/406388460",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-406388460",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 406388460,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjM4ODQ2MA==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-19T19:30:02Z",
    "updated_at": "2018-07-19T19:30:02Z",
    "author_association": "MEMBER",
    "body": "I chatted with Eric this morning about this.  There are still some concerns that we need to think through as part of the idleness design that may push us in the direction of having LB policies be able to trigger idleness without shutting down name resolution, so let's not proceed with having the client_channel reset to its initial state at this point.\r\n\r\nEric did, however, suggest another option, which is to change the subchannel API such that (re)connection attempts are triggered separately from connectivity state watches (i.e., instead of a connectivity watch implying a connection attempt, there would be a separate method to tell it to try to connect).  That way, PF could always have a connectivity state watch, even if `started_picking_` is false.  The only thing that we'd trigger when we set `started_picking_` to true would be the connection attempts.  Thus, the LB policy would notice if the subchannel goes into `TRANSIENT_FAILURE` even if it's not trying to connect.  I'm not sure how invasive this change would be, but it's probably worth looking into.\r\n\r\nAnother interesting question (although I think not directly related to this bug) is why we're recreating the subchannel after a GOAWAY in the first place.  Ever since #13932, the subchannel throws away the connected subchannel when it goes into `TRANSIENT_FAILURE`, so if we just renew the watch on the subchannel, it should try to establish a new connection.  Maybe there's something we need to clean up here.\r\n\r\nWith regard to the question about why C2 still tries to use S2, I assume it's because after step 1, S2 is always held in the subchannel index due to the ref held by C1, so no matter how many times C2 re-resolves, it will always keep getting the same subchannel from the index for the same address."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/406941742",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-406941742",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 406941742,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjk0MTc0Mg==",
    "user": {
      "login": "AspirinSJL",
      "id": 3314176,
      "node_id": "MDQ6VXNlcjMzMTQxNzY=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/3314176?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AspirinSJL",
      "html_url": "https://github.com/AspirinSJL",
      "followers_url": "https://api.github.com/users/AspirinSJL/followers",
      "following_url": "https://api.github.com/users/AspirinSJL/following{/other_user}",
      "gists_url": "https://api.github.com/users/AspirinSJL/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AspirinSJL/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AspirinSJL/subscriptions",
      "organizations_url": "https://api.github.com/users/AspirinSJL/orgs",
      "repos_url": "https://api.github.com/users/AspirinSJL/repos",
      "events_url": "https://api.github.com/users/AspirinSJL/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AspirinSJL/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-23T05:16:19Z",
    "updated_at": "2018-07-23T05:16:19Z",
    "author_association": "MEMBER",
    "body": "I think separating the connectivity watching API from the connection API still can't solve the problem. First of all, I suppose we are still watching the subchannels one by one in PF. If the first subchannel is always in `IDLE` (which can happen if we re-resolved but haven't got a new pick), we won't watch the rest of the subchannel list. The problem remains. Actually, even if every subchannel in the subchannel list can can exit `IDLE` quickly, we still have a similar problem if the subchannel we are interested in is the 1000th one in the subchannel list and all the other 999 subchannels are connecting to unreachable backends. Before the 1000th subchannel is watched by PF 1, it may have already transitioned into `TRANSIENT_FAILURE` because it's used and discarded (due to a GOAWAY) quickly by another channel. Even when it's watched by PF 1, it won't be unreffed after we see it's in `TRANSIENT_FAILURE` state because we will keep the whole subchannel list and continue trying it (we will re-resolve and unref the subchannel list after #16076, but if the re-resolution result is the same as the current one, PF 1 still holds at least one ref to that subchannel). To sum up, we are not eliminating the gap between reffing and watching.\r\n\r\nOr, do we want to watch the subchannles in PF in parallel?\r\n\r\nBack to this issue, its direct problem is that when C2 is trying to connect to a recovered server, it's matching to a previously failed subchannel in subchannel index so the pick fails while it shouldn't. Instead of trying to notify the external users to unref the failed subchannel, I think it's more straightforward to hide the failed subchannel from C2 so that C2 can use a new subchannel from `IDLE` state. Or, reset the subchannel to `IDLE` after it's `TRANSIENT_FAILURE`, if we don't have restriction of such state transition.\r\n\r\nI can understand why C2 still uses S2, but I couldn't understand why it will transition from `IDLE` to `TRANSIENT_FAILURE` to `CONNECTING` to `READY`, as shown in the [log](https://gist.github.com/apolcyn/22ba8d5e796461ee21d8434a891f88d6). But now I guess that means the channel can keep trying connecting to a server even after GOAWAY is received. GOAWAY only applies to one connection; it doesn't affect re-connection.\r\n\r\nDo you mean the subchannel recreation in step 1 after C1 receives a GOAWAY? That's because C1's PF has selected S1. When a GOAWAY arrives, S1 transitions to `TRANSIENT_FAILURE`. If the selected subchannel goes to `TRANSIENT_FAILURE`, instead of renewing the watch, we will unref it, and maybe remove it from subchannel index."
  },
  {
    "url": "https://api.github.com/repos/grpc/grpc/issues/comments/407172835",
    "html_url": "https://github.com/grpc/grpc/issues/15514#issuecomment-407172835",
    "issue_url": "https://api.github.com/repos/grpc/grpc/issues/15514",
    "id": 407172835,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzE3MjgzNQ==",
    "user": {
      "login": "markdroth",
      "id": 18664614,
      "node_id": "MDQ6VXNlcjE4NjY0NjE0",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18664614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markdroth",
      "html_url": "https://github.com/markdroth",
      "followers_url": "https://api.github.com/users/markdroth/followers",
      "following_url": "https://api.github.com/users/markdroth/following{/other_user}",
      "gists_url": "https://api.github.com/users/markdroth/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markdroth/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markdroth/subscriptions",
      "organizations_url": "https://api.github.com/users/markdroth/orgs",
      "repos_url": "https://api.github.com/users/markdroth/repos",
      "events_url": "https://api.github.com/users/markdroth/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markdroth/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-23T19:26:17Z",
    "updated_at": "2018-07-23T19:26:17Z",
    "author_association": "MEMBER",
    "body": "> Or, do we want to watch the subchannles in PF in parallel?\r\n\r\nYes, if we split connectivity watching and triggering connection attempts into separate APIs, then we would want to always watch all subchannels' connectivity in parallel.  The only reason we do it sequentially today is that that's the connection attempt behavior we want for PF, and the two are tied together today.\r\n\r\n> Back to this issue, its direct problem is that when C2 is trying to connect to a recovered server, it's matching to a previously failed subchannel in subchannel index so the pick fails while it shouldn't. Instead of trying to notify the external users to unref the failed subchannel, I think it's more straightforward to hide the failed subchannel from C2 so that C2 can use a new subchannel from `IDLE` state. Or, reset the subchannel to `IDLE` after it's `TRANSIENT_FAILURE`, if we don't have restriction of such state transition.\r\n\r\nOur general approach here is that subchannels get removed from the index when they are unreffed, and we depend upon their state being monitored by external ref-holders to know when to give up those refs.  It seems very broken to me that we would be holding a ref and not monitoring the subchannel's state, and I think it makes more sense to fix that rather than introducing a new special case to work around it.\r\n\r\n> I can understand why C2 still uses S2, but I couldn't understand why it will transition from `IDLE` to `TRANSIENT_FAILURE` to `CONNECTING` to `READY`, as shown in the [log](https://gist.github.com/apolcyn/22ba8d5e796461ee21d8434a891f88d6). But now I guess that means the channel can keep trying connecting to a server even after GOAWAY is received. GOAWAY only applies to one connection; it doesn't affect re-connection.\r\n\r\nYes, exactly.  The channel would leave IDLE when trying to reconnect.\r\n\r\n> Do you mean the subchannel recreation in step 1 after C1 receives a GOAWAY? That's because C1's PF has selected S1. When a GOAWAY arrives, S1 transitions to `TRANSIENT_FAILURE`. If the selected subchannel goes to `TRANSIENT_FAILURE`, instead of renewing the watch, we will unref it, and maybe remove it from subchannel index.\r\n\r\nI'm not sure exactly what you're asking here.\r\n\r\nWhat you're describing is correct: in step 1, after receiving the GOAWAY, we do unref S1 and remove it from the index.  Then we get a re-resolution result that tells us to create a new subchannel for the same address, which is S2.  From this point forward, C1 is always holding a ref to S2, which forces it to remain in the index.  So C2 will always get S2 from the index, no matter how many times it unrefs, re-resolves, and tries again."
  }
]
