[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12955637",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12955637",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12955637,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTU1NjM3",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T17:58:15Z",
    "updated_at": "2013-01-31T17:58:15Z",
    "author_association": "CONTRIBUTOR",
    "body": "you are actually creating `Storer` objects here (e.g. this is NOT queryable), is this what you want?\n\nthere are some test cases that are skipped, look in `pandas\\io\\tests\\test_pytables.py`,\nnamed like `test_big_.....`;\n\nthese are pretty big tables, mainly that have lots of rows, but also are a few hundred columns wide.\nthey don't directly test what you are doing, as never tried to store a VERY wide table (as a put), in theory should work.\n\nare you going to retrieve in its entirety?\n\ncan you give a summary picture of your frame that you are storing?\nthe `nans` what type of columns are they in?\n\ndo you get the `performance warning` when trying to store this?\n\nand try to store these as tables `df.append('df',df)`, you can pass `chunksize=` to write it a chunk at a time\nhttp://pandas.pydata.org/pandas-docs/stable/io.html#performance\n\nyou might want to take a look at: http://pandas.pydata.org/pandas-docs/stable/io.html#multiple-table-queries\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12956800",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12956800",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12956800,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTU2ODAw",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T18:15:09Z",
    "updated_at": "2013-01-31T18:15:09Z",
    "author_association": "NONE",
    "body": "@jreback \n\nYes I am creating a store object and putting data in like\n\n`store['features'] = df`\n\nThanks for the pointers to the tests, I'll take a look and write some large ones.\n\nYes I want to retrieve it entirely, I have my 64GB machine to run ML algorithms on and most of those (though not SVM) want the data in a big chunk.\n\nI do not get a performance warning on this store b/c in actuality I try to guarantee it is all floats (again the ML libraries blow up on non-floats).  However I do get the performance warnings on other df's I write that are most definitely mixed.\n\nI'll try using append and a chunk size next, unfortunately it takes 8 hours to extract all these features, so playing with my real data is painful.  Hence why I want to write some tests...\n\nThanks again, and I know I still owe you some data on my other ticket.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12957503",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12957503",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12957503,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTU3NTAz",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T18:26:03Z",
    "updated_at": "2013-01-31T18:26:03Z",
    "author_association": "CONTRIBUTOR",
    "body": "ok..let me know.....but as a fyi....I would for sure split this into sub-tables (e.g. split by columns), then get them and concat - pytables seems to work better (for me) when you have more row-oriented tables (rather than lots of columns)\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12957750",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12957750",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12957750,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTU3NzUw",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T18:29:47Z",
    "updated_at": "2013-01-31T18:29:47Z",
    "author_association": "NONE",
    "body": "You are surely right, I am just being lazy.  I guess I'll write some wrappers to split and reform my tables for me so I can facade away the complexity.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12957851",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12957851",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12957851,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTU3ODUx",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T18:31:13Z",
    "updated_at": "2013-01-31T18:31:13Z",
    "author_association": "CONTRIBUTOR",
    "body": "another plug for Tables!, look at append_multiple_tables...this will split for you (you can specify a dict how to split)\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12957890",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12957890",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12957890,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTU3ODkw",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T18:31:49Z",
    "updated_at": "2013-01-31T18:31:49Z",
    "author_association": "CONTRIBUTOR",
    "body": "could add this same functionailty to storers as well.....\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12958510",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12958510",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12958510,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTU4NTEw",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T18:41:44Z",
    "updated_at": "2013-01-31T18:41:44Z",
    "author_association": "NONE",
    "body": "Awesome, I'll check that out!  Thanks again!\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12967694",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12967694",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12967694,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTY3Njk0",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T21:47:05Z",
    "updated_at": "2013-01-31T21:47:05Z",
    "author_association": "NONE",
    "body": "Can't test these yet b/c my big machine is running the random forest (wrote to csv as well as hdf5 after the last failure, so used the csv file to read back in), but here is my code for breaking up a big table:\n\n``` python\ndef write_dataframe(name, df, store):\n    ''' Write a set of keys to our store representing N columns each of a larger table '''\n    keys = {}\n    buffered = []\n    for i, col in enumerate(df.columns):\n        buffered.append(col)\n        if len(buffered == 100):\n            keys[\"{0}_{1}\".format(name, i)] = buffered\n            buffered = []\n    if len(buffered) > 0:\n        keys[\"{0}_{1}\".format(name, i)] = buffered\n    store.append_to_multiple(keys, df, keys.keys()[0])\n\n\ndef read_dataframe(name, store):\n    ''' Read a set of keys from our store representing N columns each of a larger table\n     and then join the pieces back into the full table. '''\n    keys = []\n    i = 0\n    while True:\n        if \"{0}_{1}\".format(name, i) in store.keys():\n            keys.append(\"{0}_{1}\".format(name, i))\n        else:\n            break\n        i += 1\n    return store.select_as_multiple(keys)\n```\n\nDoes this look reasonable?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12968281",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12968281",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12968281,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTY4Mjgx",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T21:59:16Z",
    "updated_at": "2013-01-31T21:59:16Z",
    "author_association": "CONTRIBUTOR",
    "body": "yes...since you are only storing 20000 rows, then each table will be very fast to store, i'd say you could\ndo 500 columns in each table (try out combinations), assuming this is all similar dtype data, e.g. float64/32?\n\nand btw...I am thinking about wrapping this type of code in a Splitter class, which will save alongside the tables, to make this easier\n\nhere is a similar idiom to batch\n\n```\ndef create_batches(l, maxn = 250):\n    \"\"\" create a list of batches, maxed at maxn \"\"\"\n    batches = []\n    while(True):\n        if len(l) <= maxn:\n            if len(l) > 0:\n                batches.append(l)\n            break\n        batches.append(l[0:maxn])\n        l = l[maxn:]\n    return batches\n```\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12968647",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12968647",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12968647,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTY4NjQ3",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T22:06:24Z",
    "updated_at": "2013-01-31T22:06:24Z",
    "author_association": "NONE",
    "body": "If indeed there is a problem with storing large tables, then encapsulation of this inside pandas might be a good idea.  My features data is all float64/32, but my other intermediate files are not (they are mixed types), so it might be slow till I get to the actual feature extraction.  I don't really care about speed as long as I can be sure that my work in progress is not going to be lost.  Without pickle working on large amounts of data I think pandas really needs a reliable alternative for saving work, hopefully this will be it.  \n\nI'll test as soon as my random forest finishes...\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12968897",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12968897",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12968897,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTY4ODk3",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T22:11:31Z",
    "updated_at": "2013-01-31T22:11:31Z",
    "author_association": "CONTRIBUTOR",
    "body": "this should work, and your dataset actually isn't that big (in row-space), you have large column-space;\npytables/hd5f is designed for many rows (like many millions)\nmixed types should work as well\n\nIt think this solution will work; you maybe also want to give `carray` a try (its actually by the same author as pytables), but no interface from pandas\n https://github.com/FrancescAlted/carray\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12969666",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12969666",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12969666,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTY5NjY2",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T22:26:33Z",
    "updated_at": "2013-01-31T22:26:33Z",
    "author_association": "NONE",
    "body": "Agreed, my data isn't really THAT big, but it is big the wrong way :)  If this works then I am a happy camper, I'd like to keep everything in pandas, it was invaluable in joining together all the data into this monstrosity I am serializing.  \n\nAt the very least this thread will hopefully point out a solution to anyone with the same problems!  Once I test I'll close this up.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/12970328",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-12970328",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 12970328,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyOTcwMzI4",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-31T22:40:00Z",
    "updated_at": "2013-01-31T22:40:00Z",
    "author_association": "CONTRIBUTOR",
    "body": "your use case is interesting (and different) from others I have been seeing later\nit's all about data orientation and use case for how HDFStore should be storing things\nwe have straight stores and tables now, each has different uses\nI think yours (and one which is essentially a really giant table on disk)\nneed to be handled (prob with some hints when u r storing)\n\nbtw I think u might be a me to pass index = False to append (as u really don't need indicies and they take some time to create)\n\nalso definitely try using a csv file - as I think u may need to experiment a bit to optimize your performance\n\nu can directly email if u would like\njeff@reback.net\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13018715",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13018715",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13018715,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzMDE4NzE1",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-01T22:59:14Z",
    "updated_at": "2013-02-01T22:59:14Z",
    "author_association": "NONE",
    "body": "I tried running with the code above and got the following:\n\n```\ntime.datetime' has no len()\nTraceback (most recent call last):\n  File \"XxXXX.py\", line 803, in get_joined_data\n    write_dataframe(\"{0}joined_{1}\".format(prefix, date_prefix), df, store)\n  File \"XXXX.py\", line 74, in write_dataframe\n    store.append_to_multiple(keys, df, keys.keys()[0])\n  File \"/Library/Python/2.7/site-packages/pandas/io/pytables.py\", line 591, in append_to_multiple\n    self.append(k, val, data_columns=dc, **kwargs)\n  File \"/Library/Python/2.7/site-packages/pandas/io/pytables.py\", line 532, in append\n    self._write_to_group(key, value, table=True, append=True, **kwargs)\n  File \"/Library/Python/2.7/site-packages/pandas/io/pytables.py\", line 788, in _write_to_group\n    s.write(obj = value, append=append, complib=complib, **kwargs)\n  File \"/Library/Python/2.7/site-packages/pandas/io/pytables.py\", line 2491, in write\n    min_itemsize=min_itemsize, **kwargs)\n  File \"/Library/Python/2.7/site-packages/pandas/io/pytables.py\", line 2254, in create_axes\n raise Exception(\"cannot find the correct atom type -> [dtype->%s,items->%s] %s\" % (b.dtype.name, b.items, str(detail)))\nException: cannot find the correct atom type -> [dtype->object,items->Index([....object of type 'datetime.datetime' has no len()\n```\n\nSame thing for datetime64's.  The columns have nan's expressed as np.nan.  Not quite sure what is going on.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13019177",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13019177",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13019177,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzMDE5MTc3",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-01T23:13:37Z",
    "updated_at": "2013-02-01T23:17:19Z",
    "author_association": "CONTRIBUTOR",
    "body": "assume you are running with 0.10.1\nhave a look at PR #2752 (first example)\n\nyou **MUST** have datetime64[ns] dtypes in the columns in order to store, you CANNOT have np.nan (this is a float type), and thus the column will be object, instead the nan MUST be NaT\n\nif you do not (and the exception tells you that), then cast them like this (this is a bug which is fixed in the above PR)\n\n`df['datetime'] = Series(df['datetime'].values,dtype='M8[ns]')`\n\nyou also cann't have datetimes, again cast like the above, they will be converted to Timestamps\n\nPyTables cannot store 'object' types efficiently, so we dont' allow it; HDFStore also cannot guess as 'object' generally represents string (but can also be unicode, or other types which cannot be serialized)\n\nif you can post a str(df) would be helpful\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13021221",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13021221",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13021221,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzMDIxMjIx",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-02T00:30:51Z",
    "updated_at": "2013-02-02T00:30:51Z",
    "author_association": "CONTRIBUTOR",
    "body": "I added some docs here: https://github.com/jreback/pandas/commit/673f91c3b15ddbaa143c4c62b11c9c93d2ccd01a\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13025635",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13025635",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13025635,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzMDI1NjM1",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-02T06:46:42Z",
    "updated_at": "2013-02-02T06:46:42Z",
    "author_association": "NONE",
    "body": "No luck for me, I tried making sure my parser used np.datetime64('nat') and np.datetime64(dateutil.parser.parse()) and doing the casting you suggested and I got nowhere.  At this point I am just going to go back to how I was doing it before.\n\nAt some point I will end up just writing a (or probably overriding the default) csv serializer that maintains column types, b/c it is getting a bit ridiculous how difficult it is to simply save out a dataframe and not lose all my type information.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13028557",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13028557",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13028557,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzMDI4NTU3",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-02T12:19:36Z",
    "updated_at": "2013-02-02T12:19:36Z",
    "author_association": "CONTRIBUTOR",
    "body": " can u post a small data sample, os, pandas, and numpy version\nand how you are reading and converting (the code)\n\nyou should not normally even use np.datetime64 directly.\nnp.datetime64('nat') doesn't do anything (and is not valid code)\n\nread_csv is a fantastic piece of code, no need to reinvent the wheel\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13211113",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13211113",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13211113,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzMjExMTEz",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-06T23:08:30Z",
    "updated_at": "2013-02-06T23:08:30Z",
    "author_association": "NONE",
    "body": "Sorry trying to get stuff done and ignore my serialization problems for now.  read_csv is good, except it does not keep dtypes on the columns, which makes it painful when I have a datetime column that is then an object...\n\nI'd suggest read_csv should optionally include column type information in the header, and maybe even do the proper casting, but I can write that myself.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13252241",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13252241",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13252241,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzMjUyMjQx",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-07T18:37:15Z",
    "updated_at": "2013-02-07T18:37:15Z",
    "author_association": "CONTRIBUTOR",
    "body": "have you seen: http://pandas.pydata.org/pandas-docs/stable/io.html#specifying-column-data-types\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13252369",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13252369",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13252369,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzMjUyMzY5",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-07T18:39:41Z",
    "updated_at": "2013-02-07T18:39:41Z",
    "author_association": "NONE",
    "body": "Sure, I just want that embedded in the header, so I don't have to re-specify it or serialize the dtypes separately and apply them separately after read_csv.  I think it would be a good idea to have a to_csv that when you read_csv it recreates the dataframe exactly as it was when you wrote it without further work.  Right now that is not the case (I think).\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13252657",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13252657",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13252657,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzMjUyNjU3",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-07T18:45:17Z",
    "updated_at": "2013-02-07T18:45:17Z",
    "author_association": "CONTRIBUTOR",
    "body": "that's being worked on, and datetimes are being improved. if you would post a sample of your data I can try to help you out.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13252756",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13252756",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13252756,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzMjUyNzU2",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-07T18:47:30Z",
    "updated_at": "2013-02-07T18:47:30Z",
    "author_association": "NONE",
    "body": "No doubt and I am going to try and help out, I am just on a deadline and just hacking away with whatever works.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13742653",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13742653",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13742653,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzNzQyNjUz",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-18T20:55:13Z",
    "updated_at": "2013-02-18T20:56:17Z",
    "author_association": "NONE",
    "body": "Alright I finally got around to attempting to patch to_csv and read_csv to preserve types.  I just monkey-patched over the existing methods to start b/c I am not very familiar with the inner-workings and didn't want to interfere with the things I did have working.\n\n```\nimport pandas as pd\nimport numpy as np\nfrom pandas.core import frame\nfrom pandas.core.index import MultiIndex\nfrom pandas.tseries.period import PeriodIndex\nimport pandas.lib as lib\nimport pandas.core.common as com\nimport csv\nimport dateutil\nimport pytz\nfrom pytz import timezone\n\n\n\ndef parse_date_time(val):\n    if val is not np.nan:\n        #'2012-11-12 17:30:00+00:00\n        try:\n            datetime_obj = dateutil.parser.parse(val)\n            datetime_obj = datetime_obj.replace(tzinfo=timezone('UTC'))\n            datetime_obj = datetime_obj.astimezone(timezone('UTC'))\n            return datetime_obj\n        except ValueError as e:\n#            print e\n            return np.nan\n    else:\n        return np.nan\n\n\ndef _my_helper_csv(self, writer, na_rep=None, cols=None,\n                header=True, index=True,\n                index_label=None, float_format=None, write_dtypes=None):\n    if cols is None:\n        cols = self.columns\n\n    series = {}\n    for k, v in self._series.iteritems():\n        series[k] = v.values\n\n    has_aliases = isinstance(header, (tuple, list, np.ndarray))\n    if has_aliases or header:\n        if index:\n            # should write something for index label\n            if index_label is not False:\n                if index_label is None:\n                    if isinstance(self.index, MultiIndex):\n                        index_label = []\n                        for i, name in enumerate(self.index.names):\n                            if name is None:\n                                name = ''\n                            index_label.append(name)\n                    else:\n                        index_label = self.index.name\n                        if index_label is None:\n                            index_label = ['']\n                        else:\n                            index_label = [index_label]\n                elif not isinstance(index_label, (list, tuple, np.ndarray)):\n                    # given a string for a DF with Index\n                    index_label = [index_label]\n\n                encoded_labels = list(index_label)\n            else:\n                encoded_labels = []\n\n            if has_aliases:\n                if len(header) != len(cols):\n                    raise ValueError(('Writing %d cols but got %d aliases'\n                                      % (len(cols), len(header))))\n                else:\n                    write_cols = header\n            else:\n                write_cols = cols\n            encoded_cols = list(write_cols)\n            if write_dtypes:\n                for j, col in enumerate(cols):\n                    encoded_cols[j] = \"{0}:{1}\".format(col, self._series[col].dtype)\n            writer.writerow(encoded_labels + encoded_cols)\n        else:\n            if write_dtypes:\n                for j, col in enumerate(cols):\n                    encoded_cols[j] = \"{0}:{1}\".format(col, self._series[col].dtype)\n            writer.writerow(encoded_cols)\n\n            encoded_cols = list(cols)\n            writer.writerow(encoded_cols)\n\n    data_index = self.index\n    if isinstance(self.index, PeriodIndex):\n        data_index = self.index.to_timestamp()\n\n    nlevels = getattr(data_index, 'nlevels', 1)\n    for j, idx in enumerate(data_index):\n        row_fields = []\n        if index:\n            if nlevels == 1:\n                row_fields = [idx]\n            else:  # handle MultiIndex\n                row_fields = list(idx)\n        for i, col in enumerate(cols):\n            val = series[col][j]\n            if lib.checknull(val):\n                val = na_rep\n\n            if float_format is not None and com.is_float(val):\n                val = float_format % val\n            elif isinstance(val, np.datetime64):\n                val = lib.Timestamp(val)._repr_base\n\n            row_fields.append(val)\n\n        writer.writerow(row_fields)\n\ndef my_to_csv(self, path_or_buf, sep=\",\", na_rep='', float_format=None,\n               cols=None, header=True, index=True, index_label=None,\n               mode='w', nanRep=None, encoding=None, quoting=None,\n               line_terminator='\\n', write_dtypes=None):\n        \"\"\"\n        Write DataFrame to a comma-separated values (csv) file\n\n        Parameters\n        ----------\n        path_or_buf : string or file handle / StringIO\n            File path\n        sep : character, default \",\"\n            Field delimiter for the output file.\n        na_rep : string, default ''\n            Missing data representation\n        float_format : string, default None\n            Format string for floating point numbers\n        cols : sequence, optional\n            Columns to write\n        header : boolean or list of string, default True\n            Write out column names. If a list of string is given it is\n            assumed to be aliases for the column names\n        index : boolean, default True\n            Write row names (index)\n        index_label : string or sequence, or False, default None\n            Column label for index column(s) if desired. If None is given, and\n            `header` and `index` are True, then the index names are used. A\n            sequence should be given if the DataFrame uses MultiIndex.  If\n            False do not print fields for index names. Use index_label=False\n            for easier importing in R\n        nanRep : deprecated, use na_rep\n        mode : Python write mode, default 'w'\n        encoding : string, optional\n            a string representing the encoding to use if the contents are\n            non-ascii, for python versions prior to 3\n        line_terminator: string, default '\\n'\n            The newline character or character sequence to use in the output\n            file\n        quoting : optional constant from csv module\n            defaults to csv.QUOTE_MINIMAL\n        \"\"\"\n        if nanRep is not None:  # pragma: no cover\n            import warnings\n            warnings.warn(\"nanRep is deprecated, use na_rep\",\n                          FutureWarning)\n            na_rep = nanRep\n\n        if hasattr(path_or_buf, 'read'):\n            f = path_or_buf\n            close = False\n        else:\n            f = com._get_handle(path_or_buf, mode, encoding=encoding)\n            close = True\n\n        if quoting is None:\n            quoting = csv.QUOTE_MINIMAL\n\n        try:\n            if encoding is not None:\n                csvout = com.UnicodeWriter(f, lineterminator=line_terminator,\n                                           delimiter=sep, encoding=encoding,\n                                           quoting=quoting)\n            else:\n                csvout = csv.writer(f, lineterminator=line_terminator,\n                                    delimiter=sep, quoting=quoting)\n            self._helper_csv(csvout, na_rep=na_rep,\n                             float_format=float_format, cols=cols,\n                             header=header, index=index,\n                             index_label=index_label, write_dtypes=write_dtypes)\n\n        finally:\n            if close:\n                f.close()\n\npd.core.frame.DataFrame.to_csv = my_to_csv\npd.core.frame.DataFrame._helper_csv = _my_helper_csv\n\nfrom pandas.io import parsers\n\ndef my_read_csv(filepath_or_buffer, sep=',', dialect=None, compression=None, doublequote=True, escapechar=None, quotechar='\"', quoting=0, skipinitialspace=False, lineterminator=None, header='infer', index_col=None, names=None, prefix=None, skiprows=None, skipfooter=None, skip_footer=0, na_values=None, true_values=None, false_values=None, delimiter=None, converters=None, dtype=None, usecols=None, engine='c', delim_whitespace=False, as_recarray=False, na_filter=True, compact_ints=False, use_unsigned=False, low_memory=True, buffer_lines=None, warn_bad_lines=True, error_bad_lines=True, keep_default_na=True, thousands=None, comment=None, decimal='.', parse_dates=False, keep_date_col=False, dayfirst=False, date_parser=None, memory_map=False, nrows=None, iterator=False, chunksize=None, verbose=False, encoding=None, squeeze=False, read_dtypes=None):\n    df = pd.read_csv(filepath_or_buffer, sep=sep, dialect=dialect, compression=compression, doublequote=doublequote, escapechar=escapechar, quotechar=quotechar, quoting=quoting, skipinitialspace=skipinitialspace, lineterminator=lineterminator, header=header, index_col=index_col, names=names, prefix=prefix, skiprows=skiprows, skipfooter=skip_footer, skip_footer=skip_footer, na_values=na_values, true_values=true_values, false_values=false_values, delimiter=delimiter, converters=converters, dtype=dtype, usecols=usecols, engine=engine, delim_whitespace=delim_whitespace, as_recarray=as_recarray, na_filter=na_filter, compact_ints=compact_ints, use_unsigned=use_unsigned, low_memory=low_memory, buffer_lines=buffer_lines, warn_bad_lines=warn_bad_lines, error_bad_lines=error_bad_lines, keep_default_na=keep_default_na, thousands=thousands, comment=comment, decimal=decimal, parse_dates=parse_dates, keep_date_col=keep_date_col, dayfirst=dayfirst, date_parser=date_parser, memory_map=memory_map, nrows=nrows, iterator=iterator, chunksize=chunksize, verbose=verbose, encoding=encoding, squeeze=squeeze)\n    if read_dtypes:\n        for col, series in df.iteritems():\n            splits= col.split(\":\")\n            read_dtype = splits[1]\n            if str(series.dtype) != read_dtype:\n                if read_dtype == \"datetime.datetime\":\n                    series = series.apply(lambda x: parse_date_time(x))\n                    series = pd.Series(series.values,dtype='M8[ns]')\n                elif \"datetime64\" in read_dtype:\n                    series = series.apply(lambda x: parse_date_time(x))\n                    series = pd.Series(df[col].values,dtype='M8[ns]')\n                elif read_dtype == \"float\" or read_dtype == \"float64\" or read_dtype == \"float32\": \n                    series.values = series.values.astype(np.float64)\n                elif read_dtype == \"int\" or read_dtype == \"int64\" or read_dtype == \"int32\":\n                    series.values = series.values.astype(np.int64)\n                elif read_dtype == \"bool\" or read_dtype == \"bool_\":\n                    series.values = series.values.astype(np.bool_)\n                elif read_dtype == \"complex_\":\n                    series.values = series.values.astype(np.complex_)\n                df[col] = series\n    return df\n\npd.my_read_csv = my_read_csv\n```\n\nQuick test case:\n\n```\n'''\nCreated on Feb 18, 2013\n\n@author: jostheim\n'''\nimport unittest\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport pandas_extensions\n\nclass Test(unittest.TestCase):\n\n\n    def test_reading_and_writing(self):\n        df = pd.DataFrame({'a':[1,2,4,7], 'b':[1.2, 2.3, 5.1, 6.3], \n                    'c':list('abcd'), \n                    'd':[datetime.datetime(2001,1,1),datetime.datetime(2001,1,2),np.nan, datetime.datetime(2012,11,2)] })\n        for col, series in df.iteritems():\n            df['d'] = pd.Series(df['d'].values,dtype='M8[ns]')\n        df.to_csv(\"/tmp/test.csv\", write_dtypes=True)\n        new_df = pd.my_read_csv(\"/tmp/test.csv\", index_col=0, read_dtypes=True)\n        for i, t in enumerate(df.dtypes):\n            print t, new_df.dtypes[i]\n            self.assertEqual(t, new_df.dtypes[i], \"dtypes match\")\n\n\nif __name__ == \"__main__\":\n    #import sys;sys.argv = ['', 'Test.test_reading_and_writing']\n    unittest.main()\n```\n\nThe general idea is just to write out (optionally) the dtype into the header and then when we read in the csv parse back out the dtype and do a proper conversion.  Obviously what I have here is very simple and doesn't take into account many things (like if a column name has a \":\" in it, since I use that as the dtype delimiter during serilization).  \n\nAnyway I wanted to post this b/c it does allow for the current version to be monkey-patched to handle dates a little bit better with read_csv.  I'd be happy to try and integrate this into the core code with the next release, but I probably need some coaching as the ideal way to handle this in a \"pandas\" way.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13746926",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13746926",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13746926,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzNzQ2OTI2",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-18T22:34:09Z",
    "updated_at": "2013-02-18T22:34:26Z",
    "author_association": "CONTRIBUTOR",
    "body": "this is an interesting idea.....more of @wesm perview...\n\non original issue however,\n\nif you update to master you can use\n\n`df.convert_objects(convert_dates='coerce')` to clean up your frames\n(will force conversion to datetime64[ns]) - invalid entries will be marked as NaT\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/13748250",
    "html_url": "https://github.com/pandas-dev/pandas/issues/2784#issuecomment-13748250",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/2784",
    "id": 13748250,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEzNzQ4MjUw",
    "user": {
      "login": "jostheim",
      "id": 132902,
      "node_id": "MDQ6VXNlcjEzMjkwMg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/132902?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jostheim",
      "html_url": "https://github.com/jostheim",
      "followers_url": "https://api.github.com/users/jostheim/followers",
      "following_url": "https://api.github.com/users/jostheim/following{/other_user}",
      "gists_url": "https://api.github.com/users/jostheim/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jostheim/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jostheim/subscriptions",
      "organizations_url": "https://api.github.com/users/jostheim/orgs",
      "repos_url": "https://api.github.com/users/jostheim/repos",
      "events_url": "https://api.github.com/users/jostheim/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jostheim/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-02-18T23:07:33Z",
    "updated_at": "2013-02-18T23:07:33Z",
    "author_association": "NONE",
    "body": "Good stuff, I need to give the update to master another shot.  I think even in master, it would be useful to have an option for to_csv and read_csv to explicitly keep dtype information through serialization/deserialization.  HDFStore is  really more than a lot of people need, especially when just saving working copies of things as backup.\n"
  }
]
