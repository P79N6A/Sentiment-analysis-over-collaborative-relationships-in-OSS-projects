[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/250127581",
    "html_url": "https://github.com/pandas-dev/pandas/issues/14310#issuecomment-250127581",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/14310",
    "id": 250127581,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI1MDEyNzU4MQ==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-09-28T10:14:55Z",
    "updated_at": "2016-09-28T10:14:55Z",
    "author_association": "CONTRIBUTOR",
    "body": "I guess. Why would this small difference actually matter in practice? If you are repeatedly using an indexer that is completely non-performant (in either case), and you should be indexing via a larger indexer.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/250127722",
    "html_url": "https://github.com/pandas-dev/pandas/issues/14310#issuecomment-250127722",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/14310",
    "id": 250127722,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI1MDEyNzcyMg==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-09-28T10:15:41Z",
    "updated_at": "2016-09-28T10:15:41Z",
    "author_association": "CONTRIBUTOR",
    "body": "If you would like to investigate and see where / propose fix, then that would be great.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/326261169",
    "html_url": "https://github.com/pandas-dev/pandas/issues/14310#issuecomment-326261169",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/14310",
    "id": 326261169,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjI2MTE2OQ==",
    "user": {
      "login": "nesdis",
      "id": 17020932,
      "node_id": "MDQ6VXNlcjE3MDIwOTMy",
      "avatar_url": "https://avatars3.githubusercontent.com/u/17020932?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/nesdis",
      "html_url": "https://github.com/nesdis",
      "followers_url": "https://api.github.com/users/nesdis/followers",
      "following_url": "https://api.github.com/users/nesdis/following{/other_user}",
      "gists_url": "https://api.github.com/users/nesdis/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/nesdis/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/nesdis/subscriptions",
      "organizations_url": "https://api.github.com/users/nesdis/orgs",
      "repos_url": "https://api.github.com/users/nesdis/repos",
      "events_url": "https://api.github.com/users/nesdis/events{/privacy}",
      "received_events_url": "https://api.github.com/users/nesdis/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-08-31T10:54:25Z",
    "updated_at": "2017-08-31T10:54:25Z",
    "author_association": "NONE",
    "body": "indexing of sparse dataframe looks to be slow because elements are being stored as multiple sparse series. It would be much faster if pandas stored the underlying elements as a coo_matrix instead, when all the columns are of the same datatype. @jreback Do you suppose sparse dataframe would benefit from this optimization?"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/326624485",
    "html_url": "https://github.com/pandas-dev/pandas/issues/14310#issuecomment-326624485",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/14310",
    "id": 326624485,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjYyNDQ4NQ==",
    "user": {
      "login": "nesdis",
      "id": 17020932,
      "node_id": "MDQ6VXNlcjE3MDIwOTMy",
      "avatar_url": "https://avatars3.githubusercontent.com/u/17020932?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/nesdis",
      "html_url": "https://github.com/nesdis",
      "followers_url": "https://api.github.com/users/nesdis/followers",
      "following_url": "https://api.github.com/users/nesdis/following{/other_user}",
      "gists_url": "https://api.github.com/users/nesdis/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/nesdis/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/nesdis/subscriptions",
      "organizations_url": "https://api.github.com/users/nesdis/orgs",
      "repos_url": "https://api.github.com/users/nesdis/repos",
      "events_url": "https://api.github.com/users/nesdis/events{/privacy}",
      "received_events_url": "https://api.github.com/users/nesdis/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-09-01T16:25:27Z",
    "updated_at": "2017-09-01T16:25:27Z",
    "author_association": "NONE",
    "body": "# python sparse dataframes are **ridiculously slow** when it comes to most operations vs sparse matrices:\r\n\r\n### Sparse matrix instantiation vs Sparse dataframe instantiation from numpy 2darray:\r\n\r\n```python\r\nd = numpy.zeros((10000,10000))\r\nd[1,2] = 3\r\n\r\ntimeit.timeit('m = coo_matrix(d)', globals=globals(), number=1)\r\n0.7182237296299819\r\n\r\ntimeit.timeit('df = pandas.DataFrame(d).to_sparse(0)', globals=globals(), number=1)\r\n206.5695096827077\r\n```\r\nSparse dataframe instantiation is about **280 times slower** vs sparse matrix\r\n\r\n### Sparse matrix slicing vs Sparse dataframe slicing\r\n\r\n```python\r\nr = m.tocsr()\r\ntimeit.timeit('r[:5,:].toarray()', globals=globals(), number=1)\r\n0.0005268476787705367\r\n\r\ntimeit.timeit('df.iloc[:5,:]', globals=globals(), number=1)\r\n\r\n'''\r\nMEMORY EXCEPTION!!\r\npython ended up consuming 6GB of my RAM\r\n'''\r\n```\r\n\r\nI dont understand why this bug is a duplicate of [Row slicing of a sparse dataframe is too slow #17408](https://github.com/pandas-dev/pandas/issues/17408)\r\n\r\n#14310 bug is about multi row indexing being slow. #17408 is about sparse dataframe being buggy and slow overall."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/326625130",
    "html_url": "https://github.com/pandas-dev/pandas/issues/14310#issuecomment-326625130",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/14310",
    "id": 326625130,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjYyNTEzMA==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-09-01T16:28:17Z",
    "updated_at": "2017-09-01T16:28:17Z",
    "author_association": "CONTRIBUTOR",
    "body": "@nesdis you are welcome to have a look. However, virtually all support for sparse would have to be community driven. This is not really well supported, though @kernc has done a very nice job fixing some things up.\r\n\r\nDataFrames are column based so changing to internal data storage to coo would likely be a herculean task. If you need a numpy coo, simply use one. The usecases are different."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/326628526",
    "html_url": "https://github.com/pandas-dev/pandas/issues/14310#issuecomment-326628526",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/14310",
    "id": 326628526,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjYyODUyNg==",
    "user": {
      "login": "nesdis",
      "id": 17020932,
      "node_id": "MDQ6VXNlcjE3MDIwOTMy",
      "avatar_url": "https://avatars3.githubusercontent.com/u/17020932?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/nesdis",
      "html_url": "https://github.com/nesdis",
      "followers_url": "https://api.github.com/users/nesdis/followers",
      "following_url": "https://api.github.com/users/nesdis/following{/other_user}",
      "gists_url": "https://api.github.com/users/nesdis/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/nesdis/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/nesdis/subscriptions",
      "organizations_url": "https://api.github.com/users/nesdis/orgs",
      "repos_url": "https://api.github.com/users/nesdis/repos",
      "events_url": "https://api.github.com/users/nesdis/events{/privacy}",
      "received_events_url": "https://api.github.com/users/nesdis/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-09-01T16:42:28Z",
    "updated_at": "2017-09-01T16:42:28Z",
    "author_association": "NONE",
    "body": "@jreback can't the internal structure be changed only for the case of sparse-dataframe. The main advantage of a dataframe is the automatic reindexing and alignment based on labelled indexes for arithmetic operations. using coo means i am having to re-index and align manually for my arithmetics."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/326630420",
    "html_url": "https://github.com/pandas-dev/pandas/issues/14310#issuecomment-326630420",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/14310",
    "id": 326630420,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjYzMDQyMA==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-09-01T16:50:16Z",
    "updated_at": "2017-09-01T16:50:16Z",
    "author_association": "CONTRIBUTOR",
    "body": "@nesdis the current design of pandas would make this quite difficult. In any event, I would suspect your issues are related to how you are indexing. Simply indexing by a list like a small number of times makes a slightly slower access pattern acceptable. Repeated indexing is an anti-pattern (IOW in a loop)."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/326635327",
    "html_url": "https://github.com/pandas-dev/pandas/issues/14310#issuecomment-326635327",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/14310",
    "id": 326635327,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjYzNTMyNw==",
    "user": {
      "login": "nesdis",
      "id": 17020932,
      "node_id": "MDQ6VXNlcjE3MDIwOTMy",
      "avatar_url": "https://avatars3.githubusercontent.com/u/17020932?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/nesdis",
      "html_url": "https://github.com/nesdis",
      "followers_url": "https://api.github.com/users/nesdis/followers",
      "following_url": "https://api.github.com/users/nesdis/following{/other_user}",
      "gists_url": "https://api.github.com/users/nesdis/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/nesdis/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/nesdis/subscriptions",
      "organizations_url": "https://api.github.com/users/nesdis/orgs",
      "repos_url": "https://api.github.com/users/nesdis/repos",
      "events_url": "https://api.github.com/users/nesdis/events{/privacy}",
      "received_events_url": "https://api.github.com/users/nesdis/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-09-01T17:12:07Z",
    "updated_at": "2017-09-01T17:12:07Z",
    "author_association": "NONE",
    "body": "@jreback I am not sure, but does pandas convert 2d numpy array (which is used to instantiate a dataframe) into multiple 1d arrays or refer to the array as a single 2d block? If  it is the latter then treating the input data as a sparse 2d array block should be possible too? (instead of multiple 1d series/arrays).\r\n\r\nYes in my case my slicing is happening inside a for loop to get a subset of values, the sliced matrix/df is being multiplied with other matrices to generate probabilities for a subset of elements. At the moment I have not gotten rid of the loop."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/326802053",
    "html_url": "https://github.com/pandas-dev/pandas/issues/14310#issuecomment-326802053",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/14310",
    "id": 326802053,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjgwMjA1Mw==",
    "user": {
      "login": "kernc",
      "id": 684364,
      "node_id": "MDQ6VXNlcjY4NDM2NA==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/684364?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/kernc",
      "html_url": "https://github.com/kernc",
      "followers_url": "https://api.github.com/users/kernc/followers",
      "following_url": "https://api.github.com/users/kernc/following{/other_user}",
      "gists_url": "https://api.github.com/users/kernc/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/kernc/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/kernc/subscriptions",
      "organizations_url": "https://api.github.com/users/kernc/orgs",
      "repos_url": "https://api.github.com/users/kernc/repos",
      "events_url": "https://api.github.com/users/kernc/events{/privacy}",
      "received_events_url": "https://api.github.com/users/kernc/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-09-03T12:31:45Z",
    "updated_at": "2017-09-03T12:31:45Z",
    "author_association": "CONTRIBUTOR",
    "body": "> It would be much faster if pandas stored the underlying elements as a coo_matrix instead, when all the columns are of the same datatype.\r\n\r\nImho not general enough. The case when all columns are of the same dtype is quite uncommon in the space of all possible columns/dtypes. What would probably work, however, is `SparseBlock` collecting all columns of the same dtype and no longer sharing the limitations of `NonConsolidatableMixIn`. One'd have to investigate. PRs are welcome.\r\n\r\n> Sparse dataframe instantiation is about **280 times slower** vs sparse matrix\r\n\r\nA better instantiation of SparseDataFrame would be via coo_matrix; just twice as slow:\r\n```py\r\n>>> %timeit coo_matrix(d)\r\n858 ms ± 2.67 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\r\n\r\n>>> %timeit pd.SparseDataFrame(coo_matrix(d))\r\n1.83 s ± 16.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\n> does pandas convert 2d numpy array (which is used to instantiate a dataframe) into multiple 1d arrays\r\n\r\nYes, pandas is a column-oriented storage. Some columns are consolidated into a single 2d block of the same dtype, but not `SparseBlock`. https://github.com/pandas-dev/pandas/blob/870b6a6d6415c76d051b287adcb180ac3020b6e8/pandas/core/internals.py#L2606\r\n\r\nOptimize tight loops by converting first to a format with less overhead:\r\n```py\r\ncoo = sdf.to_coo()\r\n```"
  }
]
