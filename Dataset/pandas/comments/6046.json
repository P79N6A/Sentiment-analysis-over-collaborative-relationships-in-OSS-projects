[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33137176",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33137176",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33137176,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTM3MTc2",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T16:00:32Z",
    "updated_at": "2014-01-23T16:00:32Z",
    "author_association": "CONTRIBUTOR",
    "body": "doing this is in a loop shows no problem\n\n```\n('Before', usage(total=33802862592L, used=30500061184L, free=3302801408L, percent=28.7))\n('After', usage(total=33802862592L, used=30663581696L, free=3139280896L, percent=29.2))\n('After deleting', usage(total=33802862592L, used=30631075840L, free=3171786752L, percent=29.1))\n('Before', usage(total=33802862592L, used=30631075840L, free=3171786752L, percent=29.1))\n('After', usage(total=33802862592L, used=30695452672L, free=3107409920L, percent=29.2))\n('After deleting', usage(total=33802862592L, used=30631075840L, free=3171786752L, percent=29.1))\n('Before', usage(total=33802862592L, used=30631075840L, free=3171786752L, percent=29.1))\n('After', usage(total=33802862592L, used=30695198720L, free=3107663872L, percent=29.2))\n('After deleting', usage(total=33802862592L, used=30631424000L, free=3171438592L, percent=29.1))\n('Before', usage(total=33802862592L, used=30631424000L, free=3171438592L, percent=29.1))\n('After', usage(total=33802862592L, used=30695419904L, free=3107442688L, percent=29.2))\n('After deleting', usage(total=33802862592L, used=30631424000L, free=3171438592L, percent=29.1))\n('Before', usage(total=33802862592L, used=30631424000L, free=3171438592L, percent=29.1))\n('After', usage(total=33802862592L, used=30695641088L, free=3107221504L, percent=29.2))\n('After deleting', usage(total=33802862592L, used=30631645184L, free=3171217408L, percent=29.1))\n('Before', usage(total=33802862592L, used=30631645184L, free=3171217408L, percent=29.1))\n('After', usage(total=33802862592L, used=30695641088L, free=3107221504L, percent=29.2))\n('After deleting', usage(total=33802862592L, used=30631645184L, free=3171217408L, percent=29.1))\n('Before', usage(total=33802862592L, used=30631645184L, free=3171217408L, percent=29.1))\n('After', usage(total=33802862592L, used=30695768064L, free=3107094528L, percent=29.2))\n('After deleting', usage(total=33802862592L, used=30631391232L, free=3171471360L, percent=29.1))\n('Before', usage(total=33802862592L, used=30631391232L, free=3171471360L, percent=29.1))\n('After', usage(total=33802862592L, used=30695768064L, free=3107094528L, percent=29.2))\n('After deleting', usage(total=33802862592L, used=30631518208L, free=3171344384L, percent=29.1))\n('Before', usage(total=33802862592L, used=30631518208L, free=3171344384L, percent=29.1))\n('After', usage(total=33802862592L, used=30695641088L, free=3107221504L, percent=29.2))\n('After deleting', usage(total=33802862592L, used=30631645184L, free=3171217408L, percent=29.1))\n('Before', usage(total=33802862592L, used=30631645184L, free=3171217408L, percent=29.1))\n('After', usage(total=33802862592L, used=30695768064L, free=3107094528L, percent=29.2))\n('After deleting', usage(total=33802862592L, used=30631772160L, free=3171090432L, percent=29.1))\n```\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33137429",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33137429",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33137429,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTM3NDI5",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T16:02:50Z",
    "updated_at": "2014-01-23T16:02:50Z",
    "author_association": "CONTRIBUTOR",
    "body": "python 'holds' onto memory even after allocation; it reuses if for the next allocation. \n\nA leak would show this steadily increasing.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33141177",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33141177",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33141177,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTQxMTc3",
    "user": {
      "login": "Marigold",
      "id": 1550888,
      "node_id": "MDQ6VXNlcjE1NTA4ODg=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1550888?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Marigold",
      "html_url": "https://github.com/Marigold",
      "followers_url": "https://api.github.com/users/Marigold/followers",
      "following_url": "https://api.github.com/users/Marigold/following{/other_user}",
      "gists_url": "https://api.github.com/users/Marigold/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Marigold/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Marigold/subscriptions",
      "organizations_url": "https://api.github.com/users/Marigold/orgs",
      "repos_url": "https://api.github.com/users/Marigold/repos",
      "events_url": "https://api.github.com/users/Marigold/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Marigold/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T16:35:35Z",
    "updated_at": "2014-01-23T16:35:35Z",
    "author_association": "NONE",
    "body": "Cool. Is it possible to release the memory somehow? I have a huge memory problems when reading HDFs. When I am done reading the file, the final dataframe takes just about 20% of memory used for reading.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33142166",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33142166",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33142166,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTQyMTY2",
    "user": {
      "login": "ghost",
      "id": 10137,
      "node_id": "MDQ6VXNlcjEwMTM3",
      "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ghost",
      "html_url": "https://github.com/ghost",
      "followers_url": "https://api.github.com/users/ghost/followers",
      "following_url": "https://api.github.com/users/ghost/following{/other_user}",
      "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ghost/subscriptions",
      "organizations_url": "https://api.github.com/users/ghost/orgs",
      "repos_url": "https://api.github.com/users/ghost/repos",
      "events_url": "https://api.github.com/users/ghost/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ghost/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T16:44:09Z",
    "updated_at": "2014-01-23T16:44:09Z",
    "author_association": "NONE",
    "body": "possibly https://github.com/pydata/pandas/issues/2659 has something for you.\nor the python  `gc` module.\n\nClosing as not a bug.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33142198",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33142198",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33142198,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTQyMTk4",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T16:44:26Z",
    "updated_at": "2014-01-23T16:44:26Z",
    "author_association": "CONTRIBUTOR",
    "body": "back to the os; I don't think their is any way to do this (except by exiting the process). If you are processing HDF; use the chunk iterator if possible, that way it won't increase too much. I process hdf in this way, that is I run a process to do a computation (and create an output / new hdf file). Then exit the process. (I a actually multi-process this as the computations and output files are independent).\n\nsee this recent question (the bottom of my answer) for a nice pattern: http://stackoverflow.com/questions/21295329/fastest-way-to-copy-columns-from-one-dataframe-to-another-using-pandas/21296133?noredirect=1#comment32114620_21296133\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33142228",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33142228",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33142228,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTQyMjI4",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T16:44:43Z",
    "updated_at": "2014-01-23T16:44:43Z",
    "author_association": "CONTRIBUTOR",
    "body": "@Marigold if you post your code for what you are doing I can take a look\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33151241",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33151241",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33151241,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTUxMjQx",
    "user": {
      "login": "Marigold",
      "id": 1550888,
      "node_id": "MDQ6VXNlcjE1NTA4ODg=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1550888?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Marigold",
      "html_url": "https://github.com/Marigold",
      "followers_url": "https://api.github.com/users/Marigold/followers",
      "following_url": "https://api.github.com/users/Marigold/following{/other_user}",
      "gists_url": "https://api.github.com/users/Marigold/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Marigold/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Marigold/subscriptions",
      "organizations_url": "https://api.github.com/users/Marigold/orgs",
      "repos_url": "https://api.github.com/users/Marigold/repos",
      "events_url": "https://api.github.com/users/Marigold/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Marigold/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T18:10:22Z",
    "updated_at": "2014-01-23T18:10:22Z",
    "author_association": "NONE",
    "body": "I am trying to merge HDF files on disk (very painful experience). I ended up using smaller chunks too, for now it seems quite ok.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33151725",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33151725",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33151725,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTUxNzI1",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T18:14:51Z",
    "updated_at": "2014-01-23T18:14:51Z",
    "author_association": "CONTRIBUTOR",
    "body": "@Marigold ok...lmk...as I said I do this a lot; it shouldn't be painful :)\n\nYou are using `HDFStroe` yes?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33153330",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33153330",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33153330,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTUzMzMw",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T18:30:36Z",
    "updated_at": "2014-01-23T18:30:36Z",
    "author_association": "CONTRIBUTOR",
    "body": "you may find these useful: http://pandas.pydata.org/pandas-docs/dev/cookbook.html#hdfstore\n\nspecifically this:\nhttp://stackoverflow.com/questions/14614512/merging-two-tables-with-millions-of-rows-in-python/14617925#14617925\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33164340",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33164340",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33164340,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTY0MzQw",
    "user": {
      "login": "Marigold",
      "id": 1550888,
      "node_id": "MDQ6VXNlcjE1NTA4ODg=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1550888?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Marigold",
      "html_url": "https://github.com/Marigold",
      "followers_url": "https://api.github.com/users/Marigold/followers",
      "following_url": "https://api.github.com/users/Marigold/following{/other_user}",
      "gists_url": "https://api.github.com/users/Marigold/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Marigold/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Marigold/subscriptions",
      "organizations_url": "https://api.github.com/users/Marigold/orgs",
      "repos_url": "https://api.github.com/users/Marigold/repos",
      "events_url": "https://api.github.com/users/Marigold/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Marigold/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T20:21:53Z",
    "updated_at": "2014-01-23T20:21:53Z",
    "author_association": "NONE",
    "body": "I saw your post on SO, very nice solution. However, I need to do an outer join. Dont know if it can be done in a similar way. At the moment I have like three types of joins - one sorts the data first (sorting takes ages) and then iterate over both of them, second merges indices first and select data with `where=Index` (very slow too) and the third one find min and max values of index and then select it with `where=[index]` (which is unfortunately unable to do outer join).\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33164520",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33164520",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33164520,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTY0NTIw",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T20:23:54Z",
    "updated_at": "2014-01-23T20:23:54Z",
    "author_association": "CONTRIBUTOR",
    "body": "sounds like an interesting problem....can you put up in memory what you are doing (e.g. an example which is all memory based).....? I can think about it\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33168404",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33168404",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33168404,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTY4NDA0",
    "user": {
      "login": "Marigold",
      "id": 1550888,
      "node_id": "MDQ6VXNlcjE1NTA4ODg=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1550888?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Marigold",
      "html_url": "https://github.com/Marigold",
      "followers_url": "https://api.github.com/users/Marigold/followers",
      "following_url": "https://api.github.com/users/Marigold/following{/other_user}",
      "gists_url": "https://api.github.com/users/Marigold/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Marigold/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Marigold/subscriptions",
      "organizations_url": "https://api.github.com/users/Marigold/orgs",
      "repos_url": "https://api.github.com/users/Marigold/repos",
      "events_url": "https://api.github.com/users/Marigold/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Marigold/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T21:05:50Z",
    "updated_at": "2014-01-23T21:05:50Z",
    "author_association": "NONE",
    "body": "Here is a little more complex example\n\n``` python\nimport pandas as pd\n\nleft = pd.DataFrame({'a': [0] * 5, 'b': [1] * 5})\nright = pd.DataFrame({'a': range(5), 'c': ['a' * i for i in range(5)]})\n\nleft.merge(right, on='a', how='outer')\n```\n\nIt shows all the possible problems - outer join, NaN values for int columns (was int, but now it has to be float because of NaN) and min_itemsize for string (although this can be found easily in metadata).\n\nI was thinking how to do the outer join with the example you provided and it seems pretty intuitive. When you have inner join, just go over both dataframes, look for index values not in inner join and append it to inner join afterwards. Unfortunately, it takes three nested iterations (as in your example) over dataframes.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33168845",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33168845",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33168845,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTY4ODQ1",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T21:10:42Z",
    "updated_at": "2014-01-23T21:10:42Z",
    "author_association": "CONTRIBUTOR",
    "body": "you might be able to do this by just selecting the index values of the table (use `store.select_column`, see here: http://pandas.pydata.org/pandas-docs/dev/io.html#advanced-queries)\n\nwhich gives you basically a frame which the index values AND an integer index, which is in fact the coordinates of those index values) - call these coordinates.\n\nThen you can do your joins in memory (keeping around those coordinates), then select the final result using those coordinates. That way you don't actually bring in any data until you need it.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33171797",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33171797",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33171797,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTcxNzk3",
    "user": {
      "login": "Marigold",
      "id": 1550888,
      "node_id": "MDQ6VXNlcjE1NTA4ODg=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1550888?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Marigold",
      "html_url": "https://github.com/Marigold",
      "followers_url": "https://api.github.com/users/Marigold/followers",
      "following_url": "https://api.github.com/users/Marigold/following{/other_user}",
      "gists_url": "https://api.github.com/users/Marigold/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Marigold/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Marigold/subscriptions",
      "organizations_url": "https://api.github.com/users/Marigold/orgs",
      "repos_url": "https://api.github.com/users/Marigold/repos",
      "events_url": "https://api.github.com/users/Marigold/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Marigold/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-23T21:39:15Z",
    "updated_at": "2014-01-23T21:39:15Z",
    "author_association": "NONE",
    "body": "That's what I do now. It is the fastest method so far (still the selection by index is very slow), slightly faster than looping over two dataframes in your example. I don't think it can get any better, so I would close this discussion for now until something \"new\" appears. Thanks a lot for help!\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33213111",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33213111",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33213111,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMjEzMTEx",
    "user": {
      "login": "Marigold",
      "id": 1550888,
      "node_id": "MDQ6VXNlcjE1NTA4ODg=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1550888?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Marigold",
      "html_url": "https://github.com/Marigold",
      "followers_url": "https://api.github.com/users/Marigold/followers",
      "following_url": "https://api.github.com/users/Marigold/following{/other_user}",
      "gists_url": "https://api.github.com/users/Marigold/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Marigold/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Marigold/subscriptions",
      "organizations_url": "https://api.github.com/users/Marigold/orgs",
      "repos_url": "https://api.github.com/users/Marigold/repos",
      "events_url": "https://api.github.com/users/Marigold/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Marigold/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-24T10:44:15Z",
    "updated_at": "2014-01-24T10:45:17Z",
    "author_association": "NONE",
    "body": "Back to the original question - now with relation to hdf itself. I have a following script\n\n``` python\nimport pandas as pd\nimport psutil\nimport gc\n\ndef create_hdf():\n    d = pd.DataFrame({'a': range(int(2e7)), 'b': range(int(2e7))})\n    d.to_hdf('test.h5', 'df', format='t')\n\ndef load_hdf(columns):\n    before = psutil.phymem_usage()\n    print('Before', before)\n    d = pd.read_hdf('test.h5', 'df', columns=columns)\n    gc.collect()\n    print('MB used by dataframe itself: {:.2f}'.format(float(d.values.nbytes) / 2**20))\n    after = psutil.phymem_usage()\n    print('After', after)\n    print('Memory change in MB {:.2f}'.format((after.used - before.used) / float(2**20)))\n```\n\nAnd here are the results for different `columns`\n\n### columns = None\n\n```\n('Before', usage(total=6132502528L, used=3364777984L, free=2767724544L, percent=22.6))\nMB used by dataframe itself: 305.18\n('After', usage(total=6132502528L, used=4186906624L, free=1945595904L, percent=36.0))\nMemory change in MB 784.04\n```\n\n### columns = []\n\n```\n('Before', usage(total=6132502528L, used=3391778816L, free=2740723712L, percent=22.8))\nMB used by dataframe itself: 0.00\n('After', usage(total=6132502528L, used=3893465088L, free=2239037440L, percent=31.0))\nMemory change in MB 478.45\n```\n\n### columns = ['a']\n\n```\n('Before', usage(total=6132502528L, used=3393855488L, free=2738647040L, percent=22.8))\nMB used by dataframe itself: 152.59\n('After', usage(total=6132502528L, used=4056539136L, free=2075963392L, percent=33.6))\nMemory change in MB 631.98\n```\n\nI can limit this \"memory leak\" to some extent by using `chunksize` and then concatenate the results. Anyway, should this be default behavior?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33216404",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33216404",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33216404,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMjE2NDA0",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-24T11:42:19Z",
    "updated_at": "2014-01-24T11:42:19Z",
    "author_association": "CONTRIBUTOR",
    "body": "setting columns only causes a reindex\n\nhdf is row oriented so it will bring in ALL the columns no matter what u ask and just reindex to give you back what u want\n\nif u want to limit peak memory definite use an iterator and concatenate\n\ngenerally I try to work on smaller parts of my stores at once\n\nif I need the entire thing u can chunk by iterator or by looping over another axis of the data and selecting (eg you can say select the unique values for a particular field then loop over those )\n\nif u do heavily column oriented stuff you really need a column store\n\nsee this/ #4454 want to contribute on this?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/33217809",
    "html_url": "https://github.com/pandas-dev/pandas/issues/6046#issuecomment-33217809",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/6046",
    "id": 33217809,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMjE3ODA5",
    "user": {
      "login": "Marigold",
      "id": 1550888,
      "node_id": "MDQ6VXNlcjE1NTA4ODg=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1550888?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Marigold",
      "html_url": "https://github.com/Marigold",
      "followers_url": "https://api.github.com/users/Marigold/followers",
      "following_url": "https://api.github.com/users/Marigold/following{/other_user}",
      "gists_url": "https://api.github.com/users/Marigold/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Marigold/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Marigold/subscriptions",
      "organizations_url": "https://api.github.com/users/Marigold/orgs",
      "repos_url": "https://api.github.com/users/Marigold/repos",
      "events_url": "https://api.github.com/users/Marigold/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Marigold/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-24T12:08:13Z",
    "updated_at": "2014-01-24T12:08:13Z",
    "author_association": "NONE",
    "body": "Thanks for explanation. I'll definitely look at it and see if I can contribute with something. \n"
  }
]
