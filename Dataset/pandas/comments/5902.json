[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/32050791",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-32050791",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 32050791,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDUwNzkx",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-10T18:10:30Z",
    "updated_at": "2014-01-10T18:10:30Z",
    "author_association": "CONTRIBUTOR",
    "body": "can you give an actual use case of this?\n\nare you trying to create really large frames?\n\nand if you are, then why would you not write a disk-based storage, say HDF and then read chunks as needed?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/32053602",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-32053602",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 32053602,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDUzNjAy",
    "user": {
      "login": "tinproject",
      "id": 3742174,
      "node_id": "MDQ6VXNlcjM3NDIxNzQ=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/3742174?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/tinproject",
      "html_url": "https://github.com/tinproject",
      "followers_url": "https://api.github.com/users/tinproject/followers",
      "following_url": "https://api.github.com/users/tinproject/following{/other_user}",
      "gists_url": "https://api.github.com/users/tinproject/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/tinproject/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/tinproject/subscriptions",
      "organizations_url": "https://api.github.com/users/tinproject/orgs",
      "repos_url": "https://api.github.com/users/tinproject/repos",
      "events_url": "https://api.github.com/users/tinproject/events{/privacy}",
      "received_events_url": "https://api.github.com/users/tinproject/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-10T18:45:53Z",
    "updated_at": "2014-01-10T18:45:53Z",
    "author_association": "CONTRIBUTOR",
    "body": "Load data efficiently from a remote endpoint, a database query for example, you have a iterator (cursor, pointer to data) and how many values the query have. The same with an HDF file. Iterator to elemets and count of elements (cursor tpye) could be consider as an universal endpoint to any kind of external library\n\nThe problem originally arise when I try to load some data on my 4GB machine. I read a big file with a generator and try to load it directly on pandas, I can't. \nI solve this creating an empty numpy.ndarray and loading the data of my generator directly on the array. Then pass the numpy.ndarray to pandas.\n\nCurrently we have better machines but it'a a nosense not to use it efficiently.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/32055734",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-32055734",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 32055734,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDU1NzM0",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-10T19:11:28Z",
    "updated_at": "2014-01-10T19:11:28Z",
    "author_association": "CONTRIBUTOR",
    "body": "ok, so you want to read say a 1GB of data (say that's what the frame ends up being). But right now you say it say takes 2GB to create. ok prob true. What happens if you magically create it for only 1GB then say perform an operation with it (say`df+1`), which creates another object of 1GB? You have exactly the same problem.\n\nThe way to do this is to operate out-of-core. I didn't mean iterator over HDF per se, but rather\n\nIn chunks, create an intermediate store from your generator (doesn't have to be HDF but generally the most efficient). Then operate on those chunks.\n\nDoesn't matter how much memory you have; when you have a dataset that is a large fraction of your memory and you want to do actual operations your are forced to do it this way.\n\nI think you can attack the iterator problem in a very similar way.\n\nProcess a chunk of data, form to a frame, repeat, then concat these at the end. I think a way to do this in pandas would be nice; I also think that doing this in the constructor is not intuitive (well you could do it internally I suppose, but you maybe need a more sophisticated way of doing this, including additional arguments, not only the count of the generator, but a place for intermediate storage, etc.)\n\nso you _could_ do something like this:\n\n`df = DataFrame(data=GeneratorObject(src, addtitional arg))...`\n\nwhere the `GeneratorObject` is a pandas object that could handle the construction via chunks\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/32055798",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-32055798",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 32055798,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDU1Nzk4",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-10T19:12:09Z",
    "updated_at": "2014-01-10T19:12:09Z",
    "author_association": "CONTRIBUTOR",
    "body": "see #3202 which is a template for doing this (a `GeneratorObject` could be a form of this actually)\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/32068859",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-32068859",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 32068859,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDY4ODU5",
    "user": {
      "login": "tinproject",
      "id": 3742174,
      "node_id": "MDQ6VXNlcjM3NDIxNzQ=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/3742174?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/tinproject",
      "html_url": "https://github.com/tinproject",
      "followers_url": "https://api.github.com/users/tinproject/followers",
      "following_url": "https://api.github.com/users/tinproject/following{/other_user}",
      "gists_url": "https://api.github.com/users/tinproject/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/tinproject/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/tinproject/subscriptions",
      "organizations_url": "https://api.github.com/users/tinproject/orgs",
      "repos_url": "https://api.github.com/users/tinproject/repos",
      "events_url": "https://api.github.com/users/tinproject/events{/privacy}",
      "received_events_url": "https://api.github.com/users/tinproject/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-10T21:38:47Z",
    "updated_at": "2014-01-10T21:38:47Z",
    "author_association": "CONTRIBUTOR",
    "body": "I'm not thinking on any particular data, the goal it's to solve the bug and improve pandas.\n\nYou are rigth, it's always needed space for operations but I'm only talking about loading data. When load chunked data that it's not in memory you always need twice the memory, half for the chunks dataframes, half for the final dataframe. \nCurrently pandas only load data from memory to memory (previously allocated, externally or internally). Which I try to express it's load data from x directly to memory, x being disk, network, memory or whatever. \n\nThis approach may help to load the data from the pipeline without the need to store the whole tank first.\n\nThe way of loading data in pandas internally it's column base, if wants to load data from an element/row base (iterator) efficiently it might change the way of doing it.\n\nI'll start with the simplest case, a Series without index, it loads 0-dimensional data (elements) from a 1-dimensional collection. \nElements can come in one type of data and pandas store in another type, that could be explicitly expressed (dtype) or infered.\n\nThe data collection could be of one type:\n- Iterable of known length:\n  - Sequence (list, tuple, str..), numeric index access\n  - Sets, unordered access (but one arbitrary order for the iterator)\n  - Mappings (dicts..), key access\n- Iterable of unknown length (must set explicitely):\n  - Generator, Iterator..\n- Non iterable \n  - It's there any collection object in python?\n\nTo load the data could do the following:\n1.  Take an iterator to the data\n2.  How many elements are?\n   \n   When read data from a unknown length collection it must be explicitely given. If not expressed takes it with len(), if collection is sized. Otherwise there it's no other solution than read the whole data to an intermediary list.\n3.  Which type the elements are? It will be stored in some other type?\n   \n   This could be explictely indicated with dtype property. If not it have to infer it. It could be done by reading the first element of the collection. As it trying to do this in one pass, it assume that the type of the first element is going to be compatible with the rest elements.\n4.  Allocate the memory needed for the array and store the first element.\n   \n   This array it's at first empty (np.empty for example), or na filled.\n5.  Store the rest of colection in data structure.\n   \n   Element type could be tracked without store the whole collection in memory, if someone its from a different type it could raise a exception, treat it like a na value, or reallocate the data in a new array with the new type before move to the next element.\n6.  Fit data structure.\n   \n   If the iterator exhausted before reach the number of elements of count, resize the array to the correct size\n\nThis structure can be applied to whatever kind of data collection. The question it's to do the checks and data loading in a flip way, by elements (rows) rather than columns.\n\nThis example can be consider the easiest case, the harder case it's when a DataFrame reads from a mapping iterator.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/32074062",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-32074062",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 32074062,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDc0MDYy",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-10T22:40:41Z",
    "updated_at": "2014-01-10T22:40:41Z",
    "author_association": "CONTRIBUTOR",
    "body": "you are welcome to try to implement a solution\n\nyou can't change the API and must pass all current tests\n\nlmk if u have questions \n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/32097782",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-32097782",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 32097782,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDk3Nzgy",
    "user": {
      "login": "tinproject",
      "id": 3742174,
      "node_id": "MDQ6VXNlcjM3NDIxNzQ=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/3742174?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/tinproject",
      "html_url": "https://github.com/tinproject",
      "followers_url": "https://api.github.com/users/tinproject/followers",
      "following_url": "https://api.github.com/users/tinproject/following{/other_user}",
      "gists_url": "https://api.github.com/users/tinproject/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/tinproject/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/tinproject/subscriptions",
      "organizations_url": "https://api.github.com/users/tinproject/orgs",
      "repos_url": "https://api.github.com/users/tinproject/repos",
      "events_url": "https://api.github.com/users/tinproject/events{/privacy}",
      "received_events_url": "https://api.github.com/users/tinproject/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-11T14:46:43Z",
    "updated_at": "2014-01-11T14:46:43Z",
    "author_association": "CONTRIBUTOR",
    "body": "@jreback Could you point me to any kind of doc/explanations of how internals/Block API works?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/32098357",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-32098357",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 32098357,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDk4MzU3",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-01-11T15:10:00Z",
    "updated_at": "2014-01-11T15:10:00Z",
    "author_association": "CONTRIBUTOR",
    "body": "No docs. You just have to step thru creation code. Here's a mini-version.\n\nBlockManagers hold blocks that are divided by dtype. A block is a container that holds items which label the 0'th dimension of a block; ref_items are reference (e.g. by take) back to where these livie in the BlockManager. A block generally has 2 or more dimensions (even in the case of a Series); a SparseBlock is the exception. The 0th dimension in block is the info_axis (e.g. columns for a Frame, they are 'backwards'). Panels have 3-d blocks. Again the 0th is are the items (the info_axis).\n\nBlock are created then merged/consolidated. Very rarely are they actually modified in-place. When you see an in-place operation it atually is creating a new block manager (possibly with the same blocks) and replace in the top-level object. (thats the `._data` attribute).\n\nTheir are some non-trivial things going on, esp when you have duplicate items which are the whole `_ref_locs` bizness.\n\nYou just have to spend some time with it tracing flow.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/223794705",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-223794705",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 223794705,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIyMzc5NDcwNQ==",
    "user": {
      "login": "leifwalsh",
      "id": 46406,
      "node_id": "MDQ6VXNlcjQ2NDA2",
      "avatar_url": "https://avatars2.githubusercontent.com/u/46406?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/leifwalsh",
      "html_url": "https://github.com/leifwalsh",
      "followers_url": "https://api.github.com/users/leifwalsh/followers",
      "following_url": "https://api.github.com/users/leifwalsh/following{/other_user}",
      "gists_url": "https://api.github.com/users/leifwalsh/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/leifwalsh/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/leifwalsh/subscriptions",
      "organizations_url": "https://api.github.com/users/leifwalsh/orgs",
      "repos_url": "https://api.github.com/users/leifwalsh/repos",
      "events_url": "https://api.github.com/users/leifwalsh/events{/privacy}",
      "received_events_url": "https://api.github.com/users/leifwalsh/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-06-05T05:49:16Z",
    "updated_at": "2016-06-05T05:49:16Z",
    "author_association": "CONTRIBUTOR",
    "body": "I am also interested in creating a DataFrame in place from a streaming source (generators producing tuples or dicts).  Happy to write some C code to get this done, but I don't know where to start.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/223815247",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-223815247",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 223815247,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIyMzgxNTI0Nw==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-06-05T14:12:36Z",
    "updated_at": "2016-06-05T14:12:36Z",
    "author_association": "CONTRIBUTOR",
    "body": "@leifwalsh not even really sure what this issue was/is about. I think a concrete proposal of 'have pandas do this' in pseudo code would be the best.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/224071092",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-224071092",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 224071092,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDA3MTA5Mg==",
    "user": {
      "login": "tinproject",
      "id": 3742174,
      "node_id": "MDQ6VXNlcjM3NDIxNzQ=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/3742174?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/tinproject",
      "html_url": "https://github.com/tinproject",
      "followers_url": "https://api.github.com/users/tinproject/followers",
      "following_url": "https://api.github.com/users/tinproject/following{/other_user}",
      "gists_url": "https://api.github.com/users/tinproject/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/tinproject/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/tinproject/subscriptions",
      "organizations_url": "https://api.github.com/users/tinproject/orgs",
      "repos_url": "https://api.github.com/users/tinproject/repos",
      "events_url": "https://api.github.com/users/tinproject/events{/privacy}",
      "received_events_url": "https://api.github.com/users/tinproject/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-06-06T20:02:17Z",
    "updated_at": "2016-06-06T20:02:17Z",
    "author_association": "CONTRIBUTOR",
    "body": "This issue was about creating a DataFrame from an iterator-like object avoiding copying and allocate more memory that it's needed, ideally only the resulting DataFrame space.\n\nNote: I'm not aware of the current state of pandas, and perhaps I'm completely mistaken, but as the OP I'll try to explain what this issue was about (at least two years ago)\n\nSuppose that we got a _table_ compound of _records_(rows) that have a number of _fields_. This table was accesible by an iterator-like object but their data it's not on python memory space,the data could be originally a remote connection to an sql cursor, a csv file, or whatever...\n\nBecause we only have the access to the table from a interator it means that we couldn't access the whole table at one time, we only could read a record from the table at once. Python iterators don't have a notion of index, the only operation available it's 'give me the next value', and then the reference to that value(record) it's lost if not stored.\n\nPandas need to access the whole table to infer what are the data types of the record's fields, so the _current_ approach it's to read the whole iterator in a list and then the data from the table it's on the python memory space and pandas can read the whole table and infer the dtypes before creating the final ndarrays for the DataFrame. \n\nThe proposal it's a constructor for a DataFrame that pre-allocates the internals ndarray before read the data. As we read from an iterator it need to define a **count** (number of records readed from the iterator) and the **data types** for the fields of the record to be able to allocate the correct size ndarrays Then read the data into it.\n\nThis is the more simple case, a defined count of records and the correct data types, a stricter approach could be feasible to implement this and saves memory.\n\nA more generalized version is far more trickier:\n- Iterator could be exhausted before get to the count of records, whats the desired behavior then?\n- What if the count it's not known? A chunked approach could be used.\n- Record could be tuples, list, sequences with a positional index, what if they are not homogeneous?\n- Records could be also dicts(mappings), what if there are missing fields?\n- Data types of fields could not match with defined ones.\n\n---\n\nI remember time ago when I was looking the pandas code to try to solve this that the load of a DataFrame it's done in a columnar (Series) fashion, this is the main problem to load the data in a row(record) oriented way, from an iterator, without putting all the data from the iterator in memory.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/224267166",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-224267166",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 224267166,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDI2NzE2Ng==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-06-07T12:37:09Z",
    "updated_at": "2016-06-07T12:37:09Z",
    "author_association": "CONTRIBUTOR",
    "body": "@tinproject this is not the usecase of a pandas dataframe, dealing with streaming data is a non-trivial problem. In some ways it _could_ be accomodated by pandas, but fundamentally the storage layer needs data materialized in memory. \n\n[dask](http://dask.readthedocs.io/en/latest/) and [distributed](https://distributed.readthedocs.io/en/latest/) can accomplish what I _think_ you are looking. (and in fact use pandas under the hood for computation).\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/225031465",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-225031465",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 225031465,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTAzMTQ2NQ==",
    "user": {
      "login": "tinproject",
      "id": 3742174,
      "node_id": "MDQ6VXNlcjM3NDIxNzQ=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/3742174?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/tinproject",
      "html_url": "https://github.com/tinproject",
      "followers_url": "https://api.github.com/users/tinproject/followers",
      "following_url": "https://api.github.com/users/tinproject/following{/other_user}",
      "gists_url": "https://api.github.com/users/tinproject/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/tinproject/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/tinproject/subscriptions",
      "organizations_url": "https://api.github.com/users/tinproject/orgs",
      "repos_url": "https://api.github.com/users/tinproject/repos",
      "events_url": "https://api.github.com/users/tinproject/events{/privacy}",
      "received_events_url": "https://api.github.com/users/tinproject/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-06-09T21:24:49Z",
    "updated_at": "2016-06-09T21:24:49Z",
    "author_association": "CONTRIBUTOR",
    "body": "@jreback I'm not talking about streaming data, I'm talking about load data in a row fashion.\nWhen I'm looking at the code two years ago I realize that it's imposible to load data in pandas in a row way without making a complete refactor that would not be easily aproved as a PR. \n\nApart of the previous process of infer data types, that is done field by filed traveling the whole field column, DataFrames are formed by a group of Series, in other words, columnar data types, this means in the end that to load a table from a file you need twice the memory of the file size.\n\nThe alternative to save memory, as its pointed on other related issues, is load the data as numpy ndarrays and then pass them directly to pandas to form the DataFrame.\n\nI understand that it's hard and a involves a big change. Thanks for all the great work in pandas!!!\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/227920209",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-227920209",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 227920209,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIyNzkyMDIwOQ==",
    "user": {
      "login": "leifwalsh",
      "id": 46406,
      "node_id": "MDQ6VXNlcjQ2NDA2",
      "avatar_url": "https://avatars2.githubusercontent.com/u/46406?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/leifwalsh",
      "html_url": "https://github.com/leifwalsh",
      "followers_url": "https://api.github.com/users/leifwalsh/followers",
      "following_url": "https://api.github.com/users/leifwalsh/following{/other_user}",
      "gists_url": "https://api.github.com/users/leifwalsh/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/leifwalsh/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/leifwalsh/subscriptions",
      "organizations_url": "https://api.github.com/users/leifwalsh/orgs",
      "repos_url": "https://api.github.com/users/leifwalsh/repos",
      "events_url": "https://api.github.com/users/leifwalsh/events{/privacy}",
      "received_events_url": "https://api.github.com/users/leifwalsh/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-06-23T00:51:58Z",
    "updated_at": "2016-06-23T00:54:04Z",
    "author_association": "CONTRIBUTOR",
    "body": "Suppose you have a streaming data source like\n\n```\ndef streaming_source():\n    for something in something_else:\n        yield {'key1': val1, 'key2': val2}\n```\n\nYou can currently have pandas create a dataframe with columns `key1` and `key2` with something like\n\n```\nrows = list(streaming_source())\ndf = pd.DataFrame(rows)\n```\n\nor if you like, a bit better for not repeating keys,\n\n```\ncols = {}\nfor row in streaming_source():\n    for key, val in row.items():\n        cols.setdefault(key, []).append(val)\ndf = pd.DataFrame(cols)\n```\n\nHowever, both of these approaches require building a python list of python objects in memory, inside the interpreter, until you have all the data, then asking pandas to reify the dataframe structure.  These objects will be spread all over the heap and be larger than the actual data you want, and while you can do it from the C API you're still calling into libpython rather than just building arrays of doubles.\n\nThe idea I have in my mind is something that can take a generator or provide an \"append\" method which builds up numpy ndarrays that grow in place (with doubling arrays or a list of slabs or something [smarter](https://github.com/facebook/folly/blob/master/folly/docs/FBVector.md)), but do so in the packed, column-oriented format we eventually want them to be in.  Then, once built, we would hand them to pandas and pandas wouldn't need to rearrange anything.\n\nI did a little bit of digging around numpy, couldn't easily find documentation that describes the memory layout well, and didn't want to spend too much time flopping around aimlessly on my own.  If someone can point me to the right documentation I could probably whip up a prototype.\n\nPerhaps this is a job for Arrow? cc @wesm \n\nEDIT: to be clear, I don't think this is actually a pandas feature, I think it's a numpy feature.  I came here hoping to find someone with more numpy experience.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/234850315",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-234850315",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 234850315,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIzNDg1MDMxNQ==",
    "user": {
      "login": "wesm",
      "id": 329591,
      "node_id": "MDQ6VXNlcjMyOTU5MQ==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/329591?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/wesm",
      "html_url": "https://github.com/wesm",
      "followers_url": "https://api.github.com/users/wesm/followers",
      "following_url": "https://api.github.com/users/wesm/following{/other_user}",
      "gists_url": "https://api.github.com/users/wesm/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/wesm/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wesm/subscriptions",
      "organizations_url": "https://api.github.com/users/wesm/orgs",
      "repos_url": "https://api.github.com/users/wesm/repos",
      "events_url": "https://api.github.com/users/wesm/events{/privacy}",
      "received_events_url": "https://api.github.com/users/wesm/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-07-25T06:26:01Z",
    "updated_at": "2016-07-25T06:26:01Z",
    "author_association": "MEMBER",
    "body": "@leifwalsh sorry it's taken me a month to reply on this. I think having fast streaming data accumulators that yield DataFrames at the end is a good idea -- we have a lot of code floating around for coercing incoming data (with fixed number of rows) into a DataFrame. There's also the internals of pandas.read_csv which deal with type inference and other matters.\n\nAs one example of code that I recently worked, on, the converter functions for converting HiveServer2 columns into a pandas-compatible representation. This includes deduplicating string values (since you don't want to create tons of copies of the same string):\n\nhttps://github.com/cloudera/hs2client/blob/master/python/hs2client/converters.h\n\nAs we've discussed at various times on pandas-dev and elsewhere, NumPy hasn't supported pandas particularly well on these types of matters, particularly for non-numeric data. But in the meantime, it's just about creating the \"right\" NumPy arrays internally (for example: if you have boolean data with nulls, you will have to return a dtype=object ndarray)\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/235488211",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-235488211",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 235488211,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTQ4ODIxMQ==",
    "user": {
      "login": "leifwalsh",
      "id": 46406,
      "node_id": "MDQ6VXNlcjQ2NDA2",
      "avatar_url": "https://avatars2.githubusercontent.com/u/46406?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/leifwalsh",
      "html_url": "https://github.com/leifwalsh",
      "followers_url": "https://api.github.com/users/leifwalsh/followers",
      "following_url": "https://api.github.com/users/leifwalsh/following{/other_user}",
      "gists_url": "https://api.github.com/users/leifwalsh/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/leifwalsh/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/leifwalsh/subscriptions",
      "organizations_url": "https://api.github.com/users/leifwalsh/orgs",
      "repos_url": "https://api.github.com/users/leifwalsh/repos",
      "events_url": "https://api.github.com/users/leifwalsh/events{/privacy}",
      "received_events_url": "https://api.github.com/users/leifwalsh/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-07-27T05:12:06Z",
    "updated_at": "2016-07-27T05:12:06Z",
    "author_association": "CONTRIBUTOR",
    "body": "@wesm thanks. I think we're aligned on the problem: do the work to construct numpy arrays from a streaming source without involving a Python list as an intermediary. I don't know how to do this yet but could learn. Need to prioritize and schedule the work. I think it's fine if numpy doesn't support us in this and we just figure out the numpy format and construct it in c++ as long as we can hand the result to numpy. Then we can play with dynamic allocation strategies to our hearts' content. \n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/236321755",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-236321755",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 236321755,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIzNjMyMTc1NQ==",
    "user": {
      "login": "wesm",
      "id": 329591,
      "node_id": "MDQ6VXNlcjMyOTU5MQ==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/329591?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/wesm",
      "html_url": "https://github.com/wesm",
      "followers_url": "https://api.github.com/users/wesm/followers",
      "following_url": "https://api.github.com/users/wesm/following{/other_user}",
      "gists_url": "https://api.github.com/users/wesm/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/wesm/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wesm/subscriptions",
      "organizations_url": "https://api.github.com/users/wesm/orgs",
      "repos_url": "https://api.github.com/users/wesm/repos",
      "events_url": "https://api.github.com/users/wesm/events{/privacy}",
      "received_events_url": "https://api.github.com/users/wesm/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-07-29T23:54:44Z",
    "updated_at": "2016-07-29T23:54:44Z",
    "author_association": "MEMBER",
    "body": "We would probably want to do something semantically similar to the builder classes in the Apache Arrow codebase:\n\nhttps://github.com/apache/arrow/blob/master/cpp/src/arrow/types/string-test.cc#L141\n\nCreate some `std::vector`-like object that accumulates data in an internal buffer (that grows by factor of 1.5x or 2x when full / reallocation required) and then giving ownership of the final buffer to a NumPy array. Doesn't need to be in C++ (actually, probably better right now for it not to be until we start digging in on pandas 2.x -- see the pandas-dev@python.org mailing list) \n\nOne important thing to keep in mind: you will want to deduplicate string data as it's coming in (each call to `PyString_FromStringAndSize` allocates memory, Python has no interning mechanism / global hash table by default). It might also make sense to have an option to convert all strings to categorical.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/237151117",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-237151117",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 237151117,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIzNzE1MTExNw==",
    "user": {
      "login": "leifwalsh",
      "id": 46406,
      "node_id": "MDQ6VXNlcjQ2NDA2",
      "avatar_url": "https://avatars2.githubusercontent.com/u/46406?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/leifwalsh",
      "html_url": "https://github.com/leifwalsh",
      "followers_url": "https://api.github.com/users/leifwalsh/followers",
      "following_url": "https://api.github.com/users/leifwalsh/following{/other_user}",
      "gists_url": "https://api.github.com/users/leifwalsh/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/leifwalsh/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/leifwalsh/subscriptions",
      "organizations_url": "https://api.github.com/users/leifwalsh/orgs",
      "repos_url": "https://api.github.com/users/leifwalsh/repos",
      "events_url": "https://api.github.com/users/leifwalsh/events{/privacy}",
      "received_events_url": "https://api.github.com/users/leifwalsh/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-08-03T06:11:13Z",
    "updated_at": "2016-08-03T06:11:13Z",
    "author_association": "CONTRIBUTOR",
    "body": "Yup. Sounds like we're on the same page. I just don't know enough about numpy to know exactly how to structure this data in memory as I receive it. We can dig in soon. \n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/237224356",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-237224356",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 237224356,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIzNzIyNDM1Ng==",
    "user": {
      "login": "tinproject",
      "id": 3742174,
      "node_id": "MDQ6VXNlcjM3NDIxNzQ=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/3742174?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/tinproject",
      "html_url": "https://github.com/tinproject",
      "followers_url": "https://api.github.com/users/tinproject/followers",
      "following_url": "https://api.github.com/users/tinproject/following{/other_user}",
      "gists_url": "https://api.github.com/users/tinproject/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/tinproject/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/tinproject/subscriptions",
      "organizations_url": "https://api.github.com/users/tinproject/orgs",
      "repos_url": "https://api.github.com/users/tinproject/repos",
      "events_url": "https://api.github.com/users/tinproject/events{/privacy}",
      "received_events_url": "https://api.github.com/users/tinproject/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-08-03T12:33:16Z",
    "updated_at": "2016-08-03T12:33:16Z",
    "author_association": "CONTRIBUTOR",
    "body": "> do the work to construct numpy arrays from a streaming source without involving a Python list as an intermediary.\n\n@leifwalsh you summarize far, far better than me what this is all about: avoid the intermediary list, just change 'numpy arrays' with 'pandas DataFrame' and 'streaming source' with 'iterator' and it was my original wish.\n\nNumpy arrays are fixed size and fixed data type, memory is allocated on it's creation, if you want to increase the size of the numpy array the only way is to copy it to a new memory location, dynamic memory allocation is far from the numpy scope. I believe it could be possible to downsize an numpy array in place, depending on the platform.\n\nBecause we want to avoid memory copying the right numpy arrays should be created once, this implies that we must know some _metadata_ of the streaming source/iterator that we need to create the arrays: the number of records (size), and the data types of the fields of the record. \nWith that data you could create the necessary numpy arrays to put your data in. To avoid creating an intermediary list and having that we only have access to the current record, all the data from the current record must be processed in one time, so all the numpy arrays must be created before read the data from the streaming source. \nWhen you have the data in the np arrays you could create a DataFrame telling pandas that use your np array and not create a new ones.\n\nIf you want to work on this I believe the easier is to restrict moving parts: constrain the streaming source metadata (size and types) to some known values, and later try to deal with unknown size or types sources.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/237756935",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-237756935",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 237756935,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIzNzc1NjkzNQ==",
    "user": {
      "login": "shoyer",
      "id": 1217238,
      "node_id": "MDQ6VXNlcjEyMTcyMzg=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1217238?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/shoyer",
      "html_url": "https://github.com/shoyer",
      "followers_url": "https://api.github.com/users/shoyer/followers",
      "following_url": "https://api.github.com/users/shoyer/following{/other_user}",
      "gists_url": "https://api.github.com/users/shoyer/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/shoyer/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/shoyer/subscriptions",
      "organizations_url": "https://api.github.com/users/shoyer/orgs",
      "repos_url": "https://api.github.com/users/shoyer/repos",
      "events_url": "https://api.github.com/users/shoyer/events{/privacy}",
      "received_events_url": "https://api.github.com/users/shoyer/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-08-05T05:13:25Z",
    "updated_at": "2016-08-05T05:13:25Z",
    "author_association": "MEMBER",
    "body": "One unfortunate complication of pandas's current memory model (with the block manager) is that it is only _sometimes_ column oriented -- if adjacent columns all have different dtypes. If you pass in a dict of numpy.ndarrays giving DataFrame columns of the same dtype, these will be \"consolidated\" into a single 2D numpy.ndarray in C-contiguous order, which means row oriented! This makes it very difficult to construct DataFrames, even from things memory-mapping to numpy arrays, without the risk of unnecessary copies. For what it's worth, this is something that we definitely hope to fix in pandas 2.0.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/238699165",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5902#issuecomment-238699165",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5902",
    "id": 238699165,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIzODY5OTE2NQ==",
    "user": {
      "login": "wesm",
      "id": 329591,
      "node_id": "MDQ6VXNlcjMyOTU5MQ==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/329591?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/wesm",
      "html_url": "https://github.com/wesm",
      "followers_url": "https://api.github.com/users/wesm/followers",
      "following_url": "https://api.github.com/users/wesm/following{/other_user}",
      "gists_url": "https://api.github.com/users/wesm/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/wesm/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wesm/subscriptions",
      "organizations_url": "https://api.github.com/users/wesm/orgs",
      "repos_url": "https://api.github.com/users/wesm/repos",
      "events_url": "https://api.github.com/users/wesm/events{/privacy}",
      "received_events_url": "https://api.github.com/users/wesm/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-08-09T21:34:37Z",
    "updated_at": "2016-08-09T21:34:37Z",
    "author_association": "MEMBER",
    "body": "Invested parties should get involved in the discussion on #13944 -- the hope here is that streaming data collectors could efficiently construct the internals of Series / DataFrame and guaranteeing contiguousness and zero-copy on construction afterwards (there are plenty of cases where you can have \"unavoidable consolidation\" inside the guts of DataFrame right now). \n"
  }
]
