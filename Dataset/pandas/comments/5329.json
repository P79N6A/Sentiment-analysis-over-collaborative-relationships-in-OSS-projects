[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27120177",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27120177",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27120177,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTIwMTc3",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-25T19:44:07Z",
    "updated_at": "2013-10-25T19:44:07Z",
    "author_association": "CONTRIBUTOR",
    "body": "I would do a garbage collect inside the loop (every once in a while), e.g.\n\n```\nimport gc\n\nfor i, frame in enumerate(frames):\n\n     ......\n\n     if i % 5 == 0:\n          gc.collect()\n          print gc.collect()\n```\n\nYou can get a memory error because the memory is too fragmented in order to allocate. Which is exactly what you are going, e.g. creating and destroying lots of objects. (if the 2nd gc shows non-zero then there are leaks, e.g. these are unreachable by the collector).\n\nlmk\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27120313",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27120313",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27120313,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTIwMzEz",
    "user": {
      "login": "michaelaye",
      "id": 69774,
      "node_id": "MDQ6VXNlcjY5Nzc0",
      "avatar_url": "https://avatars1.githubusercontent.com/u/69774?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/michaelaye",
      "html_url": "https://github.com/michaelaye",
      "followers_url": "https://api.github.com/users/michaelaye/followers",
      "following_url": "https://api.github.com/users/michaelaye/following{/other_user}",
      "gists_url": "https://api.github.com/users/michaelaye/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/michaelaye/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/michaelaye/subscriptions",
      "organizations_url": "https://api.github.com/users/michaelaye/orgs",
      "repos_url": "https://api.github.com/users/michaelaye/repos",
      "events_url": "https://api.github.com/users/michaelaye/events{/privacy}",
      "received_events_url": "https://api.github.com/users/michaelaye/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-25T19:46:11Z",
    "updated_at": "2013-10-25T19:46:11Z",
    "author_association": "CONTRIBUTOR",
    "body": "ok, will do, for now I got to let this run through started from the point of crash, to have some data for a meeting in 15 mins. ;) Will try the gc in the afternoon, thks!\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27121065",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27121065",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27121065,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTIxMDY1",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-25T19:57:33Z",
    "updated_at": "2013-10-25T19:57:33Z",
    "author_association": "CONTRIBUTOR",
    "body": "gr8!\n\nI also always run this type of processing separate linux processes; I find in general python memory management just keeps eating memory. since you are doing a parallelizable problem anyhow.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27132421",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27132421",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27132421,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTMyNDIx",
    "user": {
      "login": "michaelaye",
      "id": 69774,
      "node_id": "MDQ6VXNlcjY5Nzc0",
      "avatar_url": "https://avatars1.githubusercontent.com/u/69774?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/michaelaye",
      "html_url": "https://github.com/michaelaye",
      "followers_url": "https://api.github.com/users/michaelaye/followers",
      "following_url": "https://api.github.com/users/michaelaye/following{/other_user}",
      "gists_url": "https://api.github.com/users/michaelaye/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/michaelaye/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/michaelaye/subscriptions",
      "organizations_url": "https://api.github.com/users/michaelaye/orgs",
      "repos_url": "https://api.github.com/users/michaelaye/repos",
      "events_url": "https://api.github.com/users/michaelaye/events{/privacy}",
      "received_events_url": "https://api.github.com/users/michaelaye/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-25T23:07:06Z",
    "updated_at": "2013-10-25T23:07:52Z",
    "author_association": "CONTRIBUTOR",
    "body": "Ok, so I run it like you wrote above, it starts with a count of 259 before doing anything (is that normal or does that mean that my iPython instance is already messed up?) and it is indeed higher everytime I looked (every 4th run), then stabilizes at around 308, and this times it ran through without a crash, maybe because of the forced garbarge collection?\n\n259\n200907.h5\n200908.h5\n200909.h5\n200910.h5\n287\n200911.h5\n200912.h5\n201001.h5\n201002.h5\n301\n201003.h5\n201004.h5\n201005.h5\n201006.h5\n308\n201007.h5\n201008.h5\n201009.h5\n201010.h5\n308\n201011.h5\n201012.h5\n201101.h5\n201102.h5\n308\n201103.h5\n201104.h5\n201105.h5\n201106.h5\n294\n201107.h5\n201108.h5\n201109.h5\n201110.h5\n301\n201111.h5\n201112.h5\n201201.h5\n201202.h5\n301\n201203.h5\n201204.h5\n201205.h5\n201207.h5\n301\n201208.h5\n201209.h5\n201210.h5\n201211.h5\n294\n201212.h5\n201301.h5\n201302.h5\n201303.h5\n308\n201304.h5\n201305.h5\n201306.h5\n201307.h5\n308\n201308.h5\n\nHow can I prevent the leak? I tried a run with using `del df` at the end of the loop, but that seems to make it slightly worse? the first counts returned with even higher differences, but it then again stabilised at a count of 308. Weird all this memory stuff. I will now try a parallel approach, but because there's so much data flowing I am pretty sure that the task will be disk I/O dominated very soon, maybe already at 3 parallel processes.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27135522",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27135522",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27135522,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTM1NTIy",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-26T00:38:01Z",
    "updated_at": "2013-10-26T00:38:01Z",
    "author_association": "CONTRIBUTOR",
    "body": "the returning number is the number of unreachablees....so if you gc before your loop you prob have a certain number (e.g. ipython globals etc)....\n\nnow....sometimes ipython itself shows a leak...this is because it caches results in things like '_', '__', '___' and so on (so you can use them interactively). the only reliable way I have found is to run this under a regular session (e.g. not ipython/notebook) (though not sure if a remote server does this sort of caching). and basically print your memory usage each iteration.\n\nI don't believe there is an actual leak, its possible, but unlikely unless you are holding a reference in top-level code (e.g. even if there are cycles python WILL reclaim them, it may just take a couple of cycles).\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27135955",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27135955",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27135955,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTM1OTU1",
    "user": {
      "login": "michaelaye",
      "id": 69774,
      "node_id": "MDQ6VXNlcjY5Nzc0",
      "avatar_url": "https://avatars1.githubusercontent.com/u/69774?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/michaelaye",
      "html_url": "https://github.com/michaelaye",
      "followers_url": "https://api.github.com/users/michaelaye/followers",
      "following_url": "https://api.github.com/users/michaelaye/following{/other_user}",
      "gists_url": "https://api.github.com/users/michaelaye/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/michaelaye/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/michaelaye/subscriptions",
      "organizations_url": "https://api.github.com/users/michaelaye/orgs",
      "repos_url": "https://api.github.com/users/michaelaye/repos",
      "events_url": "https://api.github.com/users/michaelaye/events{/privacy}",
      "received_events_url": "https://api.github.com/users/michaelaye/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-26T00:57:13Z",
    "updated_at": "2013-10-26T00:57:13Z",
    "author_association": "CONTRIBUTOR",
    "body": "I got this in the console (so far, need to leave):\n\n``` bash\n0\n/raid1/maye/rdr_out/metadata/200907.h5\n/raid1/maye/rdr_out/metadata/200908.h5\n/raid1/maye/rdr_out/metadata/200909.h5\n/raid1/maye/rdr_out/metadata/200910.h5\n35\n/raid1/maye/rdr_out/metadata/200911.h5\n/raid1/maye/rdr_out/metadata/200912.h5\n/raid1/maye/rdr_out/metadata/201001.h5\n/raid1/maye/rdr_out/metadata/201002.h5\n42\n/raid1/maye/rdr_out/metadata/201003.h5\n/raid1/maye/rdr_out/metadata/201004.h5\n/raid1/maye/rdr_out/metadata/201005.h5\n/raid1/maye/rdr_out/metadata/201006.h5\n49\n/raid1/maye/rdr_out/metadata/201007.h5\n```\n\nLooks like it's leaking there as well, no? Though I have the feeling that in general the memory consumption looks lower than it was via the ipython notebook.\n\nBelow is all the code that I invoked via a direct `python script.py` call.\n\n``` python\n# -*- coding: utf-8 -*-\nimport pandas as pd\nfrom diviner import data_prep\nfrom diviner import file_utils\nfrom scipy.ndimage import label\nimport glob\nimport gc\n\nroot = '/raid1/maye/rdr_out/metadata'\nfnames = glob.glob(root+\"/20????.h5\")\nfnames.sort()\ncolname = 'last_el_cmd'\n\nfor i,fname in enumerate(fnames):\n    if i % 4 ==0:\n        gc.collect()\n        print gc.collect()\n    print fname\n    df = pd.read_hdf(fname, 'df', columns=[colname])\n    df['mybool'] = df[colname] == 90\n    df['label'] = label(df.mybool)[0]\n    df['time'] = df.index\n    g = df.groupby('label')['time']\n    pd.DataFrame({'start':g.first(),\n                  'duration':g.last() - g.first()})[1:].to_hdf('./test_scans.h5',\n                                                           'df',\n                                                           mode='a',\n                                                           format='table',\n                                                           append=True)\n```\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27135990",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27135990",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27135990,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTM1OTkw",
    "user": {
      "login": "michaelaye",
      "id": 69774,
      "node_id": "MDQ6VXNlcjY5Nzc0",
      "avatar_url": "https://avatars1.githubusercontent.com/u/69774?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/michaelaye",
      "html_url": "https://github.com/michaelaye",
      "followers_url": "https://api.github.com/users/michaelaye/followers",
      "following_url": "https://api.github.com/users/michaelaye/following{/other_user}",
      "gists_url": "https://api.github.com/users/michaelaye/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/michaelaye/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/michaelaye/subscriptions",
      "organizations_url": "https://api.github.com/users/michaelaye/orgs",
      "repos_url": "https://api.github.com/users/michaelaye/repos",
      "events_url": "https://api.github.com/users/michaelaye/events{/privacy}",
      "received_events_url": "https://api.github.com/users/michaelaye/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-26T00:58:57Z",
    "updated_at": "2013-10-26T00:58:57Z",
    "author_association": "CONTRIBUTOR",
    "body": "Got another 49 in the next row. That's consistent with the previous runs in that the checks number 4 and 5 always end up at the same number?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27136100",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27136100",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27136100,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTM2MTAw",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-26T01:04:09Z",
    "updated_at": "2013-10-26T01:04:09Z",
    "author_association": "CONTRIBUTOR",
    "body": "if u read the same file over and over it should be constant (what comes out of the gc)\nit could change if the data in your file is different \n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27136139",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27136139",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27136139,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTM2MTM5",
    "user": {
      "login": "michaelaye",
      "id": 69774,
      "node_id": "MDQ6VXNlcjY5Nzc0",
      "avatar_url": "https://avatars1.githubusercontent.com/u/69774?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/michaelaye",
      "html_url": "https://github.com/michaelaye",
      "followers_url": "https://api.github.com/users/michaelaye/followers",
      "following_url": "https://api.github.com/users/michaelaye/following{/other_user}",
      "gists_url": "https://api.github.com/users/michaelaye/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/michaelaye/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/michaelaye/subscriptions",
      "organizations_url": "https://api.github.com/users/michaelaye/orgs",
      "repos_url": "https://api.github.com/users/michaelaye/repos",
      "events_url": "https://api.github.com/users/michaelaye/events{/privacy}",
      "received_events_url": "https://api.github.com/users/michaelaye/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-26T01:06:06Z",
    "updated_at": "2013-10-26T01:06:06Z",
    "author_association": "CONTRIBUTOR",
    "body": "Now you mean why the the gc.collect() output is the same at checks 4 and 5, right? It is true that I am using the exact files to test this. But that doesn't explain the increase in the gc.collect() return number, correct?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27136179",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27136179",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27136179,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTM2MTc5",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-26T01:08:02Z",
    "updated_at": "2013-10-26T01:08:02Z",
    "author_association": "CONTRIBUTOR",
    "body": "no it could increase even doing the same exact thing because of memory fragmentation (but shouldn't that much) and these r tiny anyhow - this is the number of in reachable objects \n\nbest bet is to print the memory used by the process each loop\n\nsee if it increases non trivially\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27136220",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27136220",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27136220,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTM2MjIw",
    "user": {
      "login": "michaelaye",
      "id": 69774,
      "node_id": "MDQ6VXNlcjY5Nzc0",
      "avatar_url": "https://avatars1.githubusercontent.com/u/69774?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/michaelaye",
      "html_url": "https://github.com/michaelaye",
      "followers_url": "https://api.github.com/users/michaelaye/followers",
      "following_url": "https://api.github.com/users/michaelaye/following{/other_user}",
      "gists_url": "https://api.github.com/users/michaelaye/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/michaelaye/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/michaelaye/subscriptions",
      "organizations_url": "https://api.github.com/users/michaelaye/orgs",
      "repos_url": "https://api.github.com/users/michaelaye/repos",
      "events_url": "https://api.github.com/users/michaelaye/events{/privacy}",
      "received_events_url": "https://api.github.com/users/michaelaye/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-26T01:09:37Z",
    "updated_at": "2013-10-26T01:09:37Z",
    "author_association": "CONTRIBUTOR",
    "body": "After being 3 times at count 49 it now went down to 35, it really seems data-dependent on what the gc can do? \nI would not have expected that though because all of the data is a pd.DataFrame, just with different amounts inside. But I admit I have abs. no clue how this garbage collection and memory management works. I was thinking of collecting memory read outs per process-id, maybe also to confirm that memory usage is higher in the notebook. Need to find the library that can provide me with that, maybe sys?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27136443",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27136443",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27136443,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTM2NDQz",
    "user": {
      "login": "michaelaye",
      "id": 69774,
      "node_id": "MDQ6VXNlcjY5Nzc0",
      "avatar_url": "https://avatars1.githubusercontent.com/u/69774?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/michaelaye",
      "html_url": "https://github.com/michaelaye",
      "followers_url": "https://api.github.com/users/michaelaye/followers",
      "following_url": "https://api.github.com/users/michaelaye/following{/other_user}",
      "gists_url": "https://api.github.com/users/michaelaye/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/michaelaye/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/michaelaye/subscriptions",
      "organizations_url": "https://api.github.com/users/michaelaye/orgs",
      "repos_url": "https://api.github.com/users/michaelaye/repos",
      "events_url": "https://api.github.com/users/michaelaye/events{/privacy}",
      "received_events_url": "https://api.github.com/users/michaelaye/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-26T01:21:06Z",
    "updated_at": "2013-10-26T01:21:06Z",
    "author_association": "CONTRIBUTOR",
    "body": "One more thing: You said you are doing this stuff routinely, are you using chunksize to keep memory usage down? Because I can not run more than 3 parallel tasks of this just because each of them uses between 20-30% of my 96 GB RAM.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/27136474",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-27136474",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 27136474,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTM2NDc0",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-26T01:23:30Z",
    "updated_at": "2013-10-26T01:23:30Z",
    "author_association": "CONTRIBUTOR",
    "body": "I use psutil\n\nI spread what I do across multiple machines\nI do some hdf processing parallel where I write files then merge them using a single process\n\nmine r not nearly as big as your maybe 10gb or so and I only read in chunks at a time \n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/39966003",
    "html_url": "https://github.com/pandas-dev/pandas/issues/5329#issuecomment-39966003",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/5329",
    "id": 39966003,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM5OTY2MDAz",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-04-09T13:53:48Z",
    "updated_at": "2014-04-09T13:53:48Z",
    "author_association": "CONTRIBUTOR",
    "body": "@michaelaye closing this as I cannot reproduce\n\nSee the self-contained script below\n\npls reopen / new issue if you find differently\n\n```\nimport gc\nimport os\nimport psutil\nimport pandas as pd\nimport numpy as np\n\ndef print_memory(m=None):\n    p = psutil.Process(os.getpid())\n    (rss, vms) = p.get_memory_info()\n    mp = p.get_memory_percent()\n    print(\"%-10.10s cur_mem->%.2f (MB),per_mem->%.2f\" % (m, rss / 1000000.0, mp))\n\nprint gc.collect()\nprint_memory('start')\n\ndf = pd.DataFrame(np.random.randn(10000,10),index=pd.date_range('20130101',periods=10000,freq='s'))\ndf['string'] = 'foo'\ndf.to_hdf('test.h5','df',mode='w',format='table')\n\nprint_memory('create')\n\nif os.path.exists('out.h5'):\n    os.remove('out.h5')\nfor i in range(10):\n    t = pd.read_hdf('test.h5','df',columns=[0,1])\n    t['mybool'] = t[0] > 0\n    t['label'] = 'foo2'\n    t['time'] = t.index\n    g = t.groupby('label')['time']\n    pd.DataFrame({'start':g.first(),\n                  'duration':g.last() - g.first()})[1:].to_hdf('./out.h5',\n                                                               'df',\n                                                               mode='a',\n                                                               format='table',\n                                                               append=True)\n    print_memory(\"%d read \" % i)\n    print gc.collect()\n    print_memory(\"%d gc   \" % i)\n\ndel df\nprint_memory('done')\nprint gc.collect()\nprint_memory('post')\nprint gc.garbage\n```\n\n```\ntest.py~  test.py   \n[goat-jreback-~/pandas] python test.py\n0\nstart      cur_mem->51.09 (MB),per_mem->0.15\ncreate     cur_mem->62.51 (MB),per_mem->0.18\n0 read     cur_mem->66.15 (MB),per_mem->0.20\n186\n0 gc       cur_mem->66.15 (MB),per_mem->0.20\n1 read     cur_mem->67.06 (MB),per_mem->0.20\n258\n1 gc       cur_mem->64.68 (MB),per_mem->0.19\n2 read     cur_mem->67.59 (MB),per_mem->0.20\n258\n2 gc       cur_mem->64.68 (MB),per_mem->0.19\n3 read     cur_mem->67.06 (MB),per_mem->0.20\n258\n3 gc       cur_mem->64.68 (MB),per_mem->0.19\n4 read     cur_mem->67.59 (MB),per_mem->0.20\n258\n4 gc       cur_mem->64.68 (MB),per_mem->0.19\n5 read     cur_mem->67.59 (MB),per_mem->0.20\n258\n5 gc       cur_mem->64.68 (MB),per_mem->0.19\n6 read     cur_mem->67.59 (MB),per_mem->0.20\n258\n6 gc       cur_mem->64.68 (MB),per_mem->0.19\n7 read     cur_mem->67.59 (MB),per_mem->0.20\n258\n7 gc       cur_mem->64.68 (MB),per_mem->0.19\n8 read     cur_mem->67.59 (MB),per_mem->0.20\n258\n8 gc       cur_mem->64.68 (MB),per_mem->0.19\n9 read     cur_mem->67.59 (MB),per_mem->0.20\n258\n9 gc       cur_mem->64.68 (MB),per_mem->0.19\ndone       cur_mem->63.88 (MB),per_mem->0.19\n21\npost       cur_mem->63.88 (MB),per_mem->0.19\n[]\n[goat-jreback-~/pandas] \n```\n"
  }
]
