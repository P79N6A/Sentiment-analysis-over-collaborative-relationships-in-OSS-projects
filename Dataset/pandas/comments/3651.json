[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/18128321",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3651#issuecomment-18128321",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3651",
    "id": 18128321,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE4MTI4MzIx",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-20T01:06:43Z",
    "updated_at": "2013-05-20T01:38:15Z",
    "author_association": "CONTRIBUTOR",
    "body": "Your sample is really too small. HDF5 has a fair amount of overhead with really small sizes (even 300k entries is on the smaller side). The following is with no compression on either side. Floats are really more efficiently represented in binary (that as a text representation).\n\n In addition, HDF5 is row based. You get MUCH efficiency by having tables that are not too wide but are fairly long. (Hence your example is not very efficient in HDF5 at all, store it transposed in this case)\n\nI routinely have tables that are 10M+ rows and query times can be in the ms. Even the below example is small. Having 10+GB files is quite common (not to mention the astronomy guys who 10GB+ is a few seconds!)\n\n```\n-rw-rw-r--  1 jreback users 203200986 May 19 20:58 test.csv\n-rw-rw-r--  1 jreback users  88007312 May 19 20:59 test.h5\n```\n\n```\nIn [1]: df = DataFrame(randn(1000000,10))\n\nIn [9]: df\nOut[9]: \n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1000000 entries, 0 to 999999\nData columns (total 10 columns):\n0    1000000  non-null values\n1    1000000  non-null values\n2    1000000  non-null values\n3    1000000  non-null values\n4    1000000  non-null values\n5    1000000  non-null values\n6    1000000  non-null values\n7    1000000  non-null values\n8    1000000  non-null values\n9    1000000  non-null values\ndtypes: float64(10)\n\nIn [5]: %timeit df.to_csv('test.csv',mode='w')\n1 loops, best of 3: 12.7 s per loop\n\nIn [6]: %timeit df.to_hdf('test.h5','df',mode='w')\n1 loops, best of 3: 825 ms per loop\n\nIn [7]: %timeit pd.read_csv('test.csv',index_col=0)\n1 loops, best of 3: 2.35 s per loop\n\nIn [8]: %timeit pd.read_hdf('test.h5','df')\n10 loops, best of 3: 38 ms per loop\n```\n\nI really wouldn't worry about the size (I suspect you are not, but are merely interested, which is fine). The point of HDF5 is that disk is cheap, cpu is cheap, but you can't have everything in memory at once so we optimize by using chunking.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/18204678",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3651#issuecomment-18204678",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3651",
    "id": 18204678,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE4MjA0Njc4",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-21T12:11:06Z",
    "updated_at": "2013-05-21T12:11:06Z",
    "author_association": "CONTRIBUTOR",
    "body": "closing. if you have a further question, pls open a new issue (or reopen this one).\n\nthanks\n"
  }
]
