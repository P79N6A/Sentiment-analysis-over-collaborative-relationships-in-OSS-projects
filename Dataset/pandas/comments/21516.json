[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/398229638",
    "html_url": "https://github.com/pandas-dev/pandas/issues/21516#issuecomment-398229638",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/21516",
    "id": 398229638,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODIyOTYzOA==",
    "user": {
      "login": "gfyoung",
      "id": 9273653,
      "node_id": "MDQ6VXNlcjkyNzM2NTM=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/9273653?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gfyoung",
      "html_url": "https://github.com/gfyoung",
      "followers_url": "https://api.github.com/users/gfyoung/followers",
      "following_url": "https://api.github.com/users/gfyoung/following{/other_user}",
      "gists_url": "https://api.github.com/users/gfyoung/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gfyoung/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gfyoung/subscriptions",
      "organizations_url": "https://api.github.com/users/gfyoung/orgs",
      "repos_url": "https://api.github.com/users/gfyoung/repos",
      "events_url": "https://api.github.com/users/gfyoung/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gfyoung/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-06-18T23:45:58Z",
    "updated_at": "2018-06-18T23:45:58Z",
    "author_association": "MEMBER",
    "body": "* This might be related to #21353 \r\n* When you say you tried `low_memory=True`, and it's not working, what do you mean?\r\n* You might need to check your concatenation when using `engine='python'` and `memory_map=...`"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/398306856",
    "html_url": "https://github.com/pandas-dev/pandas/issues/21516#issuecomment-398306856",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/21516",
    "id": 398306856,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODMwNjg1Ng==",
    "user": {
      "login": "Shirui816",
      "id": 2361394,
      "node_id": "MDQ6VXNlcjIzNjEzOTQ=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/2361394?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Shirui816",
      "html_url": "https://github.com/Shirui816",
      "followers_url": "https://api.github.com/users/Shirui816/followers",
      "following_url": "https://api.github.com/users/Shirui816/following{/other_user}",
      "gists_url": "https://api.github.com/users/Shirui816/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Shirui816/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Shirui816/subscriptions",
      "organizations_url": "https://api.github.com/users/Shirui816/orgs",
      "repos_url": "https://api.github.com/users/Shirui816/repos",
      "events_url": "https://api.github.com/users/Shirui816/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Shirui816/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-06-19T07:46:35Z",
    "updated_at": "2018-06-19T08:15:33Z",
    "author_association": "NONE",
    "body": "Thanks for replying :) @gfyoung \r\n\r\nI mean that adding `low_memory=True` option to\r\n\r\n```python\r\ntrajectory = [pd.read_csv(f, delim_whitespace=True, header=None, chunksize=10000, low_memory=True) for f in filelist]\r\n```\r\n\r\nthe memory usage is not change in contrast to the case without this option."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/398313361",
    "html_url": "https://github.com/pandas-dev/pandas/issues/21516#issuecomment-398313361",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/21516",
    "id": 398313361,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODMxMzM2MQ==",
    "user": {
      "login": "Shirui816",
      "id": 2361394,
      "node_id": "MDQ6VXNlcjIzNjEzOTQ=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/2361394?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Shirui816",
      "html_url": "https://github.com/Shirui816",
      "followers_url": "https://api.github.com/users/Shirui816/followers",
      "following_url": "https://api.github.com/users/Shirui816/following{/other_user}",
      "gists_url": "https://api.github.com/users/Shirui816/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Shirui816/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Shirui816/subscriptions",
      "organizations_url": "https://api.github.com/users/Shirui816/orgs",
      "repos_url": "https://api.github.com/users/Shirui816/repos",
      "events_url": "https://api.github.com/users/Shirui816/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Shirui816/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-06-19T08:10:42Z",
    "updated_at": "2018-06-19T08:14:43Z",
    "author_association": "NONE",
    "body": "The environment is:\r\nCentOS Linux release 7.4.1708 (Core)\r\nPython 3.6.5 :: Anaconda custom (64-bit)\r\nwith pandas version 0.23.1\r\n\r\nFrom #21353 , I tracked the memory usage:\r\n\r\n```python\r\nimport psutil\r\nimport pandas as pd\r\n\r\ntraj = []\r\ni = 0\r\nfor f in argv[1:]:\r\n    a = pd.read_csv(f, squeeze=0, header=None, delim_whitespace=1, chunksize=10000, comment='#')\r\n    traj.append(a)\r\n    if not i % 100:\r\n        print('%s th file, memory: ' % (i),psutil.Process().memory_info().rss / 1024**2)\r\n    i += 1\r\n```\r\n\r\nand the output:\r\n\r\n```python\r\n0 th file, memory:  61.96484375\r\n100 th file, memory:  214.66015625\r\n200 th file, memory:  367.32421875\r\n300 th file, memory:  520.046875\r\n400 th file, memory:  674.76953125\r\n500 th file, memory:  829.5\r\n600 th file, memory:  982.22265625\r\n700 th file, memory:  1134.9453125\r\n800 th file, memory:  1287.66796875\r\n900 th file, memory:  1442.3828125\r\n1000 th file, memory:  1597.109375\r\n1100 th file, memory:  1749.84765625\r\n1200 th file, memory:  1932.57421875\r\n1300 th file, memory:  2122.796875\r\n1400 th file, memory:  2313.01953125\r\n1500 th file, memory:  2503.2421875\r\n...\r\n4600 th file, memory:  8414.0234375\r\n4700 th file, memory:  8604.24609375\r\n4800 th file, memory:  8794.4765625\r\n4900 th file, memory:  8984.6953125\r\n5000 th file, memory:  9174.921875\r\n5100 th file, memory:  9367.14453125\r\n5200 th file, memory:  9557.37109375\r\n5300 th file, memory:  9747.59375\r\n5400 th file, memory:  9937.81640625\r\n5500 th file, memory:  10128.04296875\r\n5600 th file, memory:  10320.26953125\r\n```\r\n\r\nIt turns out that the memory increases ~1.9 mB per file. The files using in this test is about 800 kB for each.\r\n\r\nAlso tried `malloc_trim(0)` from #2659 : \r\n\r\n```python\r\nimport psutil\r\nimport pandas as pd\r\nfrom ctypes import cdll, CDLL\r\ncdll.LoadLibrary(\"libc.so.6\")\r\nlibc = CDLL(\"libc.so.6\")\r\n\r\n\r\ntraj = []\r\ni = 0\r\nfor f in argv[1:]:\r\n    libc.malloc_trim(0)\r\n    a = pd.read_csv(f, squeeze=0, header=None, delim_whitespace=1, chunksize=10000, comment='#')\r\n    traj.append(a)\r\n    if not i % 100:\r\n        print('%s th file, memory: ' % (i),psutil.Process().memory_info().rss / 1024**2)\r\n    i += 1\r\n```\r\n\r\nThe results are same with above, the memory usage still increases quickly."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/398316060",
    "html_url": "https://github.com/pandas-dev/pandas/issues/21516#issuecomment-398316060",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/21516",
    "id": 398316060,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODMxNjA2MA==",
    "user": {
      "login": "gfyoung",
      "id": 9273653,
      "node_id": "MDQ6VXNlcjkyNzM2NTM=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/9273653?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gfyoung",
      "html_url": "https://github.com/gfyoung",
      "followers_url": "https://api.github.com/users/gfyoung/followers",
      "following_url": "https://api.github.com/users/gfyoung/following{/other_user}",
      "gists_url": "https://api.github.com/users/gfyoung/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gfyoung/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gfyoung/subscriptions",
      "organizations_url": "https://api.github.com/users/gfyoung/orgs",
      "repos_url": "https://api.github.com/users/gfyoung/repos",
      "events_url": "https://api.github.com/users/gfyoung/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gfyoung/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-06-19T08:20:40Z",
    "updated_at": "2018-06-19T08:20:40Z",
    "author_association": "MEMBER",
    "body": "Hmm...admittedly, this is the first time I've been seeing so many of these issues regarding memory leakage in `read_csv`, and I'm still uncertain as to whether it has to deal with `DataFrame` or `read_csv`.\r\n\r\ncc @jreback "
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/402302026",
    "html_url": "https://github.com/pandas-dev/pandas/issues/21516#issuecomment-402302026",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/21516",
    "id": 402302026,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwMjMwMjAyNg==",
    "user": {
      "login": "Liam3851",
      "id": 546210,
      "node_id": "MDQ6VXNlcjU0NjIxMA==",
      "avatar_url": "https://avatars0.githubusercontent.com/u/546210?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Liam3851",
      "html_url": "https://github.com/Liam3851",
      "followers_url": "https://api.github.com/users/Liam3851/followers",
      "following_url": "https://api.github.com/users/Liam3851/following{/other_user}",
      "gists_url": "https://api.github.com/users/Liam3851/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Liam3851/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Liam3851/subscriptions",
      "organizations_url": "https://api.github.com/users/Liam3851/orgs",
      "repos_url": "https://api.github.com/users/Liam3851/repos",
      "events_url": "https://api.github.com/users/Liam3851/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Liam3851/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-03T21:49:33Z",
    "updated_at": "2018-07-03T21:49:33Z",
    "author_association": "CONTRIBUTOR",
    "body": "@Shirui816 You're appending the result of pd.read_csv to a list:\r\n```\r\ntraj = []\r\nfor f in argv[1:]:\r\n    a = pd.read_csv(f, squeeze=0, header=None, delim_whitespace=1, chunksize=10000, comment='#')\r\n    traj.append(a)\r\n```\r\n\r\nAdding objects to a list means they can't be garbage collected. Thus you're keeping keeping thousands of file handles and the related iterator objects open-- so we would expect memory use to grow. I've confirmed that that memory does not grow if you remove the `traj.append` call.\r\n\r\nIf the issue is that the memory use is growing faster than you expect based on the filesizes (based on your comment (\"It turns out that the memory increases ~1.9 mB per file. The files using in this test is about 800 kB for each.\"), note that you're not actually reading the file in all the way in the above call, you're creating a persistent iterator and file handle on the file, because you're using the chunksize parameter. If you only want the first 10000 lines of the file, use\r\n\r\n```\r\na = next(pd.read_csv(f, squeeze=0, header=None, delim_whitespace=1, chunksize=10000, comment='#'))\r\n```\r\n\r\nThis will throw away the handle and the rest of the iterator object and contain just your data. "
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/402348815",
    "html_url": "https://github.com/pandas-dev/pandas/issues/21516#issuecomment-402348815",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/21516",
    "id": 402348815,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwMjM0ODgxNQ==",
    "user": {
      "login": "Shirui816",
      "id": 2361394,
      "node_id": "MDQ6VXNlcjIzNjEzOTQ=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/2361394?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Shirui816",
      "html_url": "https://github.com/Shirui816",
      "followers_url": "https://api.github.com/users/Shirui816/followers",
      "following_url": "https://api.github.com/users/Shirui816/following{/other_user}",
      "gists_url": "https://api.github.com/users/Shirui816/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Shirui816/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Shirui816/subscriptions",
      "organizations_url": "https://api.github.com/users/Shirui816/orgs",
      "repos_url": "https://api.github.com/users/Shirui816/repos",
      "events_url": "https://api.github.com/users/Shirui816/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Shirui816/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-04T02:57:14Z",
    "updated_at": "2018-07-04T02:58:46Z",
    "author_association": "NONE",
    "body": "@Liam3851 Thank you very much for the explanation. I increased file size and re-ran the test, the memory gain per file was still about 1.9mB. This handler is much larger than the `open` function....emmmm.... Is option `engine='python'` means the iterator and file handler thing held by python like `open` function? I am wondering why after adding this option (or/and adding `memory_map=...` option) the parallel acceleration of MKL doesnt work any more. I totally have no clue about this problem. Are there any suggested tests to find the reason? The codes are in my first post, after creating a list of iterators in `trajectory`, take a chunk from each handler then perform an FFT.\r\n\r\nThe environment is:\r\nCentOS Linux release 7.4.1708 (Core)\r\nPython 3.6.5 :: Anaconda custom (64-bit)\r\nwith pandas version 0.23.1"
  }
]
