[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/376152324",
    "html_url": "https://github.com/pandas-dev/pandas/issues/20488#issuecomment-376152324",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/20488",
    "id": 376152324,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjE1MjMyNA==",
    "user": {
      "login": "TomAugspurger",
      "id": 1312546,
      "node_id": "MDQ6VXNlcjEzMTI1NDY=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1312546?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/TomAugspurger",
      "html_url": "https://github.com/TomAugspurger",
      "followers_url": "https://api.github.com/users/TomAugspurger/followers",
      "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
      "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
      "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
      "repos_url": "https://api.github.com/users/TomAugspurger/repos",
      "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
      "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-26T12:43:12Z",
    "updated_at": "2018-03-26T12:43:12Z",
    "author_association": "CONTRIBUTOR",
    "body": "Have you profiled `from_dict` to see where the time is spent?"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/376152803",
    "html_url": "https://github.com/pandas-dev/pandas/issues/20488#issuecomment-376152803",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/20488",
    "id": 376152803,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjE1MjgwMw==",
    "user": {
      "login": "TomAugspurger",
      "id": 1312546,
      "node_id": "MDQ6VXNlcjEzMTI1NDY=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1312546?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/TomAugspurger",
      "html_url": "https://github.com/TomAugspurger",
      "followers_url": "https://api.github.com/users/TomAugspurger/followers",
      "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
      "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
      "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
      "repos_url": "https://api.github.com/users/TomAugspurger/repos",
      "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
      "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-26T12:45:03Z",
    "updated_at": "2018-03-26T12:45:03Z",
    "author_association": "CONTRIBUTOR",
    "body": "Other than that, it seems like your problem has a lot of sparsity. I wonder if you could exploit that to get things into a dataframe more quickly? Something like a pass over the data to scan the union of the inner keys, into a scipy sparse matrix, fill with 0, and then a regular DataFrame?"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/376299789",
    "html_url": "https://github.com/pandas-dev/pandas/issues/20488#issuecomment-376299789",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/20488",
    "id": 376299789,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjI5OTc4OQ==",
    "user": {
      "login": "Melnorme1984",
      "id": 18717778,
      "node_id": "MDQ6VXNlcjE4NzE3Nzc4",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18717778?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Melnorme1984",
      "html_url": "https://github.com/Melnorme1984",
      "followers_url": "https://api.github.com/users/Melnorme1984/followers",
      "following_url": "https://api.github.com/users/Melnorme1984/following{/other_user}",
      "gists_url": "https://api.github.com/users/Melnorme1984/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Melnorme1984/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Melnorme1984/subscriptions",
      "organizations_url": "https://api.github.com/users/Melnorme1984/orgs",
      "repos_url": "https://api.github.com/users/Melnorme1984/repos",
      "events_url": "https://api.github.com/users/Melnorme1984/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Melnorme1984/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-26T20:24:51Z",
    "updated_at": "2018-03-26T20:26:43Z",
    "author_association": "NONE",
    "body": "See here: https://stackoverflow.com/questions/49491511/pandas-dataframe-from-dict-poor-performance-when-generating-from-a-lengthy-dic\r\n\r\nYou'll note that I didn't actually need to use a sparse scipy matrix and SparseDataFrame to get the speedup. Those seemed to cause more problems than they solved. With dtype=numpy.uint8, the final DataFrame is 1.1GB."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/376468110",
    "html_url": "https://github.com/pandas-dev/pandas/issues/20488#issuecomment-376468110",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/20488",
    "id": 376468110,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjQ2ODExMA==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-27T10:00:44Z",
    "updated_at": "2018-03-27T10:00:44Z",
    "author_association": "CONTRIBUTOR",
    "body": "your example doesn't provide enough detail to see if this actually scales poorly. can you provide a function to construct a dummy of your data?"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/376468416",
    "html_url": "https://github.com/pandas-dev/pandas/issues/20488#issuecomment-376468416",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/20488",
    "id": 376468416,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjQ2ODQxNg==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-27T10:01:42Z",
    "updated_at": "2018-03-27T10:01:42Z",
    "author_association": "CONTRIBUTOR",
    "body": "note generally the times you gave in the SO article don't seem unreasonable. you are filling 1.2B entries here. This would likely be memory bound (and you will need a lot)."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/376476096",
    "html_url": "https://github.com/pandas-dev/pandas/issues/20488#issuecomment-376476096",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/20488",
    "id": 376476096,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjQ3NjA5Ng==",
    "user": {
      "login": "Melnorme1984",
      "id": 18717778,
      "node_id": "MDQ6VXNlcjE4NzE3Nzc4",
      "avatar_url": "https://avatars2.githubusercontent.com/u/18717778?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Melnorme1984",
      "html_url": "https://github.com/Melnorme1984",
      "followers_url": "https://api.github.com/users/Melnorme1984/followers",
      "following_url": "https://api.github.com/users/Melnorme1984/following{/other_user}",
      "gists_url": "https://api.github.com/users/Melnorme1984/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Melnorme1984/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Melnorme1984/subscriptions",
      "organizations_url": "https://api.github.com/users/Melnorme1984/orgs",
      "repos_url": "https://api.github.com/users/Melnorme1984/repos",
      "events_url": "https://api.github.com/users/Melnorme1984/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Melnorme1984/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-27T10:28:32Z",
    "updated_at": "2018-03-28T16:13:14Z",
    "author_association": "NONE",
    "body": "Yeah, I'm running on a machine with over 256GB of RAM so that shouldn't be the issue."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/400707285",
    "html_url": "https://github.com/pandas-dev/pandas/issues/20488#issuecomment-400707285",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/20488",
    "id": 400707285,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwMDcwNzI4NQ==",
    "user": {
      "login": "TomAugspurger",
      "id": 1312546,
      "node_id": "MDQ6VXNlcjEzMTI1NDY=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1312546?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/TomAugspurger",
      "html_url": "https://github.com/TomAugspurger",
      "followers_url": "https://api.github.com/users/TomAugspurger/followers",
      "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}",
      "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions",
      "organizations_url": "https://api.github.com/users/TomAugspurger/orgs",
      "repos_url": "https://api.github.com/users/TomAugspurger/repos",
      "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}",
      "received_events_url": "https://api.github.com/users/TomAugspurger/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-06-27T15:06:19Z",
    "updated_at": "2018-06-27T15:06:19Z",
    "author_association": "CONTRIBUTOR",
    "body": "Let us know if you can profile things to narrow down where the bottleneck is."
  }
]
