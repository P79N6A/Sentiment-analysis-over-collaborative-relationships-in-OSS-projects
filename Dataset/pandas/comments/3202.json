[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/15642124",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-15642124",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 15642124,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE1NjQyMTI0",
    "user": {
      "login": "Zelazny7",
      "id": 1675613,
      "node_id": "MDQ6VXNlcjE2NzU2MTM=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1675613?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Zelazny7",
      "html_url": "https://github.com/Zelazny7",
      "followers_url": "https://api.github.com/users/Zelazny7/followers",
      "following_url": "https://api.github.com/users/Zelazny7/following{/other_user}",
      "gists_url": "https://api.github.com/users/Zelazny7/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Zelazny7/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Zelazny7/subscriptions",
      "organizations_url": "https://api.github.com/users/Zelazny7/orgs",
      "repos_url": "https://api.github.com/users/Zelazny7/repos",
      "events_url": "https://api.github.com/users/Zelazny7/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Zelazny7/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-03-29T14:05:46Z",
    "updated_at": "2013-03-29T14:05:46Z",
    "author_association": "NONE",
    "body": "This is the functionality that pandas currently lacks that is preventing me from ditching SAS.  This is a monumental task but so worth the effort!  I'm very happy to see it being discussed.  Is there any possibility of collaborating with this project? http://continuum.io/blog/blaze\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/15647480",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-15647480",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 15647480,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE1NjQ3NDgw",
    "user": {
      "login": "wesm",
      "id": 329591,
      "node_id": "MDQ6VXNlcjMyOTU5MQ==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/329591?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/wesm",
      "html_url": "https://github.com/wesm",
      "followers_url": "https://api.github.com/users/wesm/followers",
      "following_url": "https://api.github.com/users/wesm/following{/other_user}",
      "gists_url": "https://api.github.com/users/wesm/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/wesm/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wesm/subscriptions",
      "organizations_url": "https://api.github.com/users/wesm/orgs",
      "repos_url": "https://api.github.com/users/wesm/repos",
      "events_url": "https://api.github.com/users/wesm/events{/privacy}",
      "received_events_url": "https://api.github.com/users/wesm/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-03-29T16:01:39Z",
    "updated_at": "2013-03-29T16:01:39Z",
    "author_association": "MEMBER",
    "body": "The timeline for when something production-ready is going to ship there is unclear but worth keeping an eye on for collaboration possibilities. \n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/15652447",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-15652447",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 15652447,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE1NjUyNDQ3",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-03-29T17:56:17Z",
    "updated_at": "2013-03-29T17:56:17Z",
    "author_association": "CONTRIBUTOR",
    "body": "@Zelazny7 could u put up some example calculations?\nobviously doesn't have to be very big and can use random data\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/15657748",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-15657748",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 15657748,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE1NjU3NzQ4",
    "user": {
      "login": "Zelazny7",
      "id": 1675613,
      "node_id": "MDQ6VXNlcjE2NzU2MTM=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1675613?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Zelazny7",
      "html_url": "https://github.com/Zelazny7",
      "followers_url": "https://api.github.com/users/Zelazny7/followers",
      "following_url": "https://api.github.com/users/Zelazny7/following{/other_user}",
      "gists_url": "https://api.github.com/users/Zelazny7/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Zelazny7/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Zelazny7/subscriptions",
      "organizations_url": "https://api.github.com/users/Zelazny7/orgs",
      "repos_url": "https://api.github.com/users/Zelazny7/repos",
      "events_url": "https://api.github.com/users/Zelazny7/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Zelazny7/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-03-29T19:54:21Z",
    "updated_at": "2013-03-29T20:22:17Z",
    "author_association": "NONE",
    "body": "For example, finding the within group sum for a dataframe could be broken up iteratively like so.  This is all done in memory, but the same concept could be applied to a file stored on disk and read-in piece-meal.  The purpose of this enhancement, I hope, is to make the management of the IO and chunking transparent to the user:\n\n``` python\ndef gen(df, chunksize=50000):\n    nrows = len(df)\n    current = 0\n    while current < nrows:\n        yield df[current:current+chunksize]\n        current += chunksize\n\nres = pd.concat([chunk.groupby('YearMade').sum() for chunk in gen(df)])\nres.groupby(res.index).sum().head()\n\n          SalePrice\nYearMade\n1000      781142325\n1919        2210750\n1920         493250\n1937          18000\n1942          51000\n```\n\nThis is equivalent to:\n\n``` python\ndf.groupby('YearMade').sum().head()\n          SalePrice\nYearMade\n1000      781142325\n1919        2210750\n1920         493250\n1937          18000\n1942          51000\n```\n\nI tried using `reduce` and `combine` functions to achieve a similar result, but I don't know enough pandas/python to understand why it was failing.  Interestingly, a time comparison of the two results shows the method using a generator is slightly faster:\n\n``` python\ndef test():\n    res = pd.concat([chunk.groupby('YearMade').sum() for chunk in gen(df)])\n    res.groupby(res.index).sum()\n\nIn [1]: %timeit test()\n10 loops, best of 3: 31.2 ms per loop\n\nIn [2]: %timeit df.groupby('YearMade').sum()\n10 loops, best of 3: 32.8 ms per loop\n```\n\nThere is a broad class of calculations that can be performed on chunks of the data and the results collated together.  The sum example shown above is one such case.  This can easily be extended to the mean, variance and standard deviation.  Median's and quantiles are a bit trickier and certain kinds of regression would have to fall back on gradient descent algorithms rather than close-form solutions.\n\nEdited to add that scikit-learn has several algorithms that implement the `partial_fit` method. These functions are designed to work on chunks of data like this. \n\nBTW, I pulled the data from Kaggle: http://www.kaggle.com/c/bluebook-for-bulldozers\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/15660389",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-15660389",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 15660389,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE1NjYwMzg5",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-03-29T20:58:40Z",
    "updated_at": "2013-03-31T00:12:10Z",
    "author_association": "CONTRIBUTOR",
    "body": "@Zelazny7 \n\nI think we could start off with an API something like this (pseudo codeish here):\n\n1)\n\nCreate pipes\n\n```\npipe = pipeline.Pipe(handle_or_iterator_or_pandas_object, \n                                 func, axis, name, chunksize=50k, \n                                  method='single_core', location=dir_to_save_data,\n                                  combine=None, partition=None))\n  handle_or_iterator_or_object : would be either an opened read_csv/HDFStore iterator\n            (or could open/close it for you when done), or a pandas-object (e.g. a Frame)\n  func : a function taking a single argument, plus passed kwargs to evaluate\n            can return a similarly sized object (e.g. a map), or a reduction\n  axis : the slicing axis (of how we partition)\n  name : this is necessary to save any intermediate data (in our location), suppose this could be optional\n              and just save in temp dirs of the location (which itself could be a temp dir)\n  chunksize : the size of a particular chunk\n  partition : an optional partitioning function (in lieu of chunksize / axis)\n  method : how to execute this evaluation, where by on a chunk-by-chunk basis (single_core),\n                 multi-process (multi_core), or distributed (TBD), this possibily could be\n                 instead set by the class (e.g. Pipe is the base, SingleCore, MultiCore, Distributed)\n  location : if saving data (e.g. in a map/transform case), need to save each chunk\n  combine : a combiner function (e.g. trivial in the case of sum, but could be somewhat complicated,\n                   maybe do this as a separate pipe)\n```\n\nEval a pipe (and return a new pipe of this computation)\n\n```\nreturned_pipe = pipe.eval(**kwargs)\n```\n\nGet results\n\n```\ndf = pipe.get_results()\npipe.write_results(output_handle)\n```\n\nCleanup (delete temporary files), how to make this cleanup nicely\n\n```\npipe.clean()\n```\n\nthe reason for the `eval` and `get_results` would be to allow chaining and potentially `lazy` evaluation\none could call `get_results` I think with the same args as `eval` (which in effect just uses a single pipe)\n\nI think could support methods of `single_core`, and probably `multi_core` using the `multi-processing` module\n\nso I think that we could right your computation like this\n\n```\npipeline.Pipe(df).get_results(lambda x: x.groupby('YearMade').sum())\n```\n\nif df happens to be on disk\n\n```\npipeline.Pipe('cool.csv').get_results(lambda x: x.groupby('YearMade').sum())\n```\n\n## Alternatively\n\n2) could have separte methods `map` (or `apply`) and `reduce` for evaluation\n3) could support specific methods e.g. `groupby, sum, mean`\n\nCould do some of all three\n\nWe usually use `apply` for a `map` type operation (and reduction for that matter)\n`eval` is currently not really used in pandas (but maybe it makes sense to introduce it\nto signal that we are doing something _special_ here\n\nany suggestions for names, pipeline just sort of random....\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/15660619",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-15660619",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 15660619,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE1NjYwNjE5",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-03-29T21:03:50Z",
    "updated_at": "2013-03-29T21:05:58Z",
    "author_association": "CONTRIBUTOR",
    "body": "This probably also should be done with a decorator, mainly to facilitate open/close context of the file\nI think I have seen syntax similar to this in `celery` (distributed task queues)\n\nhttp://docs.celeryproject.org/en/latest/getting-started/introduction.html\n\n```\n@pipeline.Pipeline(chunksize=50k, method='single_core')\n def func(x):\n      return x.groupby('YearMade').sum()\n```\n\nThen this will work (I think)\n\n```\ndf = func('cool.csv')\n```\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/15665969",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-15665969",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 15665969,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE1NjY1OTY5",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-03-29T23:51:25Z",
    "updated_at": "2013-05-02T14:34:50Z",
    "author_association": "CONTRIBUTOR",
    "body": "having `read_csv` provide start/stop parameters so we can read a specific chunk from disk, see #3221\n\nPytables evaluator, can provide simple computations thru this directly\n- http://pytables.github.com/usersguide/libref/expr_class.html\n- http://www.pytables.org/moin/ComputingKernel\n\nnumpy w/multi-processing\n- http://folk.uio.no/sturlamo/python/multiprocessing-tutorial.pdf\n- http://www.astropython.org/snippet/2010/3/Parallel-map-using-multiprocessing\n\nthings to take note:\n- the case of an already in-memory Frame is different from an on-disk Frame\n- single core vs multi-core\n- like using `numexpr`, you only want to use these strategies with minimum of rows (at least 50k),\n  although should prove a method (useful in testing) to force it\n- dynamic vs specific slice chunking/scheduling (prob don't want to get too complicated here and its hard to know how the function is going to be used in any event)\n- write_results HAS to be serialized in the parent process! though can have intermediate results written, then a combiner could deal with see\n  - Pytables using mmap / Pipe of Multiprocess: https://github.com/PyTables/PyTables/blob/develop/examples/multiprocess_access_benchmarks.py\n  - Pytables collate results: https://github.com/PyTables/PyTables/blob/develop/examples/multiprocess_access_queues.py\n- Multiprocessing with numpy\n  - http://stackoverflow.com/questions/15414027/multiprocessing-pool-makes-numpy-matrix-multiplication-slower\n  - http://stackoverflow.com/questions/5033799/how-do-i-pass-large-numpy-arrays-between-python-subprocesses-without-saving-to-d\n\nmclapply in R\n- http://www.rforge.net/doc/packages/multicore/mclapply.html\n\nCreating shared memory with multiprocessing\n\n```\nmp_arr = multiprocessing.Array(ctypes.c_double, 100)\narr = np.frombuffer(mp_arr.get_obj())\n```\n\nfrom Here: http://mail.scipy.org/pipermail/scipy-user/2013-April/034415.html\n\nAnother good Ref\nhttp://stackoverflow.com/questions/15976937/making-my-numpy-array-shared-across-processes\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/15893039",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-15893039",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 15893039,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE1ODkzMDM5",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-04-04T11:55:06Z",
    "updated_at": "2013-04-04T11:55:06Z",
    "author_association": "CONTRIBUTOR",
    "body": "answered in this question:\nhttp://stackoverflow.com/questions/15798209/pandas-group-by-query-on-large-data-in-hdfstore\n\n2 methods for groupby\n\n```\nimport numpy as np\nimport pandas as pd\nimport os\n\nfname = 'groupby.h5'\n\n# create a frame\ndf = pd.DataFrame({'A': ['foo', 'foo', 'foo', 'foo',\n                         'bar', 'bar', 'bar', 'bar',\n                         'foo', 'foo', 'foo'],\n                   'B': ['one', 'one', 'one', 'two',\n                         'one', 'one', 'one', 'two',\n                         'two', 'two', 'one'],\n                   'C': ['dull', 'dull', 'shiny', 'dull',\n                         'dull', 'shiny', 'shiny', 'dull',\n                         'shiny', 'shiny', 'shiny'],\n                   'D': np.random.randn(11),\n                   'E': np.random.randn(11),\n                   'F': np.random.randn(11)})\n\n\n# create the store and append, using data_columns where I possibily\n# could aggregate\ndf.to_hdf(fname,'df',mode='w',table=True,data_columns=['A','B','C'])\n\n# method 1\nwith pd.get_store(fname) as store:\n\n    # get the groups\n    groups = store.select_column('df','A').unique()\n\n    # iterate over the groups and apply my operations\n    l = []\n    for g in groups:\n\n        # select the group\n        grp = store.select('df',where = [ 'A=%s' % g ])\n\n        # this is a regular frame, aggregate however you would like\n        l.append(grp[['D','E','F']].sum())\n\n    print \"\\nresult (1):\\n%s\" % pd.concat(l, keys = groups)\n\n# method 2 (only going to work for sum; for mean need to track the running count as well)\nwith pd.get_store(fname) as store:\n\n    l = []\n    for chunk in store.select('df', chunksize=3):\n        l.append(chunk.groupby('A').sum())\n\n    print \"\\nresult (2):\\n%s\" % pd.concat(l).reset_index().groupby('A').sum().stack()\n```\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/16064415",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-16064415",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 16064415,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE2MDY0NDE1",
    "user": {
      "login": "Zelazny7",
      "id": 1675613,
      "node_id": "MDQ6VXNlcjE2NzU2MTM=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1675613?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Zelazny7",
      "html_url": "https://github.com/Zelazny7",
      "followers_url": "https://api.github.com/users/Zelazny7/followers",
      "following_url": "https://api.github.com/users/Zelazny7/following{/other_user}",
      "gists_url": "https://api.github.com/users/Zelazny7/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Zelazny7/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Zelazny7/subscriptions",
      "organizations_url": "https://api.github.com/users/Zelazny7/orgs",
      "repos_url": "https://api.github.com/users/Zelazny7/repos",
      "events_url": "https://api.github.com/users/Zelazny7/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Zelazny7/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-04-08T17:15:51Z",
    "updated_at": "2013-04-08T17:15:51Z",
    "author_association": "NONE",
    "body": "Functionality like the ff package from R would go a long ways towards making pandas easier to use with large-ish data: http://cran.r-project.org/web/packages/ff/index.html\n\nHDFStore is great, but not being able to transparently select columns and, more importantly, append new columns makes it difficult to use efficiently.  ff essentially stores a dataframe on disk.  Users can bring portions of the dataframe into memory using the same syntax as an in-memory dataframe.  This is why I call it transparent.  Furthermore, it is very easy to \"write\" a new column by using the same syntax to assign a new column to an in-memory dataframe.\n\nThis functionality alone - being able to access elements of an on-disk dataframe - would be a huge win for pandas and could be implemented more quickly than the sequential processes discussed above.  I think this would be a good stepping stone on the way towards out-of-core computations.  I think it's reasonable for users to pull subsets of their data into memory for manipulation.  It becomes a pain when we have to manage the i/o operations.  This would alleviate that pain-point.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/16065196",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-16065196",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 16065196,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE2MDY1MTk2",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-04-08T17:28:53Z",
    "updated_at": "2013-04-08T17:28:53Z",
    "author_association": "CONTRIBUTOR",
    "body": "you can now select columns easily `read_column` (in 0.11 - docs are not built yet)\n\nadding columns efficiently is quite difficult, you would need to store data column wise, and then adding rows would be difficult. You have to pick. (that said, we possibily _could_ support a columnar access, via `ctable`), its not that different an interface, but it suffers from exactly the same problem.\n\nThat said you can easily add columns if you disaggregate your table, e.g. store a column per group.\n\nThe functionarily your are asking for here is bascially a numpy `memmap`, This tricky issue is exactly how you would use this, can you give me a pseudo-code example of how you would go about accessing this?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/16070172",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-16070172",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 16070172,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE2MDcwMTcy",
    "user": {
      "login": "Zelazny7",
      "id": 1675613,
      "node_id": "MDQ6VXNlcjE2NzU2MTM=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1675613?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Zelazny7",
      "html_url": "https://github.com/Zelazny7",
      "followers_url": "https://api.github.com/users/Zelazny7/followers",
      "following_url": "https://api.github.com/users/Zelazny7/following{/other_user}",
      "gists_url": "https://api.github.com/users/Zelazny7/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Zelazny7/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Zelazny7/subscriptions",
      "organizations_url": "https://api.github.com/users/Zelazny7/orgs",
      "repos_url": "https://api.github.com/users/Zelazny7/repos",
      "events_url": "https://api.github.com/users/Zelazny7/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Zelazny7/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-04-08T18:46:26Z",
    "updated_at": "2013-04-08T18:46:43Z",
    "author_association": "NONE",
    "body": "In The predictive modeling world, think FICO and other credit scores, it's pretty much always a big table of heterogenous data.  I would love to use pandas like this:\n\n``` python\n# read in a csv (or other text file) using awesome pandas parser, but store on disk\ndf_on_disk = pd.read_csv('somefile.csv', dtypes=dtypes, mode='disk')\n\n# select columns into memory\ncol_in_mem = df_on_disk['col_that_I_need']\n\n# multiple columns\ncols_in_mem = df_on_disk[['col1','col2','col8']]\n\n# writing to disk using column in memory\nmodified_col_in_mem = cool_function(col_in_mem)\ndf_on_disk['new_col'] = modified_col_in_mem\n```\n\nAll of the cool indexing that pandas handles in-memory could be ignored for the on-disk dataframe.  As long as it has the same number of rows, a new column would be slapped onto the end.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/16079829",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-16079829",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 16079829,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE2MDc5ODI5",
    "user": {
      "login": "Zelazny7",
      "id": 1675613,
      "node_id": "MDQ6VXNlcjE2NzU2MTM=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1675613?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Zelazny7",
      "html_url": "https://github.com/Zelazny7",
      "followers_url": "https://api.github.com/users/Zelazny7/followers",
      "following_url": "https://api.github.com/users/Zelazny7/following{/other_user}",
      "gists_url": "https://api.github.com/users/Zelazny7/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Zelazny7/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Zelazny7/subscriptions",
      "organizations_url": "https://api.github.com/users/Zelazny7/orgs",
      "repos_url": "https://api.github.com/users/Zelazny7/repos",
      "events_url": "https://api.github.com/users/Zelazny7/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Zelazny7/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-04-08T21:25:36Z",
    "updated_at": "2013-04-08T21:25:36Z",
    "author_association": "NONE",
    "body": "I'm starting to understand the difficulty of quickly accessing and storing columns like this.  To accomplish it using pd.read_csv, one loop for every column would have to be performed.  Still, it would only have to be done once.  The columns would have to be written to a mmap file in sequential order.  Storing meta data about the columns and types would allow the mmap to be quickly accessed and return pandas objects.  I think I\"ll toy around with creating something like this and see how it goes.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/16081871",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-16081871",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 16081871,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE2MDgxODcx",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-04-08T22:01:52Z",
    "updated_at": "2013-04-08T22:01:52Z",
    "author_association": "CONTRIBUTOR",
    "body": "If I remember how your original question on SO was, you just need to write the table (as is)) to HDFStore, with whatever data columnsyou want (or multiple tables if its very wide), then you just write columns to another group/groups as needed. There is some bookeeping in order to track what is where, so when you do a query you are pulling in the correct groups (and not selecting on certain ones), but this should be quite fast. And every once in a while you could rebuild the store into a new one (conceputally 'packing' it).\n\nIts kind of like a distributed file system, you write and the _system_ stores it where convenient. It has a map to tell you where everything is (kind of like a table of contents). The user of the system doesn't care exactly _where_ the data is, just that they can do operations on it. Appends of rows are a bit more complicated because you have to split the row and append to multipel groups. Column appends are also easy. Even deletes of columns can be handled (just remember that column is deleted).\n\nYou ideally want locality of data when you select, e.g. if you typically want a bunch of columns at the same time, then store them in a table together.\n\nI think could but a wrapper around this and make it pretty transparent. Wrap the selection mechnasim and it you could effectively have an out-of-core dataframe.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/16599470",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-16599470",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 16599470,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE2NTk5NDcw",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-04-18T19:26:39Z",
    "updated_at": "2013-04-18T19:26:39Z",
    "author_association": "CONTRIBUTOR",
    "body": "@Zelazny7 I lost the question where we were talking about column-wise tables...\n\nyou might find this link interesting\nhttps://github.com/PyTables/proposal/blob/master/column-wise-pytables.rst\n\nI actually think could implement this directly (and not wait for pytables to do it) via `ctable`\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/16605983",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-16605983",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 16605983,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE2NjA1OTgz",
    "user": {
      "login": "Zelazny7",
      "id": 1675613,
      "node_id": "MDQ6VXNlcjE2NzU2MTM=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1675613?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Zelazny7",
      "html_url": "https://github.com/Zelazny7",
      "followers_url": "https://api.github.com/users/Zelazny7/followers",
      "following_url": "https://api.github.com/users/Zelazny7/following{/other_user}",
      "gists_url": "https://api.github.com/users/Zelazny7/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Zelazny7/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Zelazny7/subscriptions",
      "organizations_url": "https://api.github.com/users/Zelazny7/orgs",
      "repos_url": "https://api.github.com/users/Zelazny7/repos",
      "events_url": "https://api.github.com/users/Zelazny7/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Zelazny7/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-04-18T20:33:59Z",
    "updated_at": "2013-04-18T20:33:59Z",
    "author_association": "NONE",
    "body": "That does seem promising.  Having an on-disk, column-oriented datastore for pandas would be a huge improvement for a large class of use-cases.  Even a simple interface for storing/appending/retrieving would be enough to start using  pandas in earnest (again for my use case).  In the meantime, I've had to switch to R with the FF package.  It is pretty much exactly what is described.\n\nThe FF package only handles the storage and retrieval of R column vectors.  This is a nice addition on it's own as it allows the user to choose what is brought into memory.  However, another project quickly created the FFBase packaged which creates special forms of all the commonly used functions that specifically work on the on-disk, FF objects.\n\nPerhaps a similar approach could be taken with Pandas.  I wish I was a good enough programmer to help develop, but I can definitely help test any ideas and provide user feedback.  Thanks again for your hard work.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/17347291",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-17347291",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 17347291,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3MzQ3Mjkx",
    "user": {
      "login": "user1827356",
      "id": 2691720,
      "node_id": "MDQ6VXNlcjI2OTE3MjA=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/2691720?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/user1827356",
      "html_url": "https://github.com/user1827356",
      "followers_url": "https://api.github.com/users/user1827356/followers",
      "following_url": "https://api.github.com/users/user1827356/following{/other_user}",
      "gists_url": "https://api.github.com/users/user1827356/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/user1827356/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/user1827356/subscriptions",
      "organizations_url": "https://api.github.com/users/user1827356/orgs",
      "repos_url": "https://api.github.com/users/user1827356/repos",
      "events_url": "https://api.github.com/users/user1827356/events{/privacy}",
      "received_events_url": "https://api.github.com/users/user1827356/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-02T16:05:22Z",
    "updated_at": "2013-05-02T16:11:38Z",
    "author_association": "NONE",
    "body": "@Zelazny7 I'm not sure a column oriented database is a complete solution for the use-case you mentioned - df.groupby('YearMade').sum().head() when df is very large. For example, you would still run into the issue if each column is very large\n\nA proposal for my use cases, which may also cover yours\n\nAssume a csv with columns ['col1', 'col2', 'col3' ...]\n\nTo store this to file you would do\n\n```\npd.csv_to_chunk('<directory>', chunk_by = ['colx', 'coly' ...]) # this could also allow chunking by row number\n```\n\nThis would store it as follows:\n\n```\n<directory>/colx/coly/../col1.data\n<directory>/colx/coly/../col2.data\n...\n<directory>/colx/coly/../colN.data\n```\n\nThis would then allow you to the following:\n\n```\npd.apply_by_chunk('<directory>', processing_func_that_receives_dataframe)\n```\n\nThis is a first cut API. If you cache the 'chunk map' as metadata then, other interesting possibilities emerge\n\n```\ncf = pd.read_chunk_frame('<directory>')\ncf.apply('colx' == 'valx', processing_func_that_receives_dataframe)\n```\n\nMaking row/column insertion transparent might be tricky, but possible\nThe API can also be extended for multi-core/multi-host processing (latter assuming files are on NFS)\n\nGiven the huge effort required in an overhaul like blaze, the effort required for this might be considerably smaller\n\nCaveat: My python/big data knowledge is small but growing :) So if I've missed something obvious, feel free not to mince words\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/17347969",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-17347969",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 17347969,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3MzQ3OTY5",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-02T16:17:12Z",
    "updated_at": "2013-05-02T16:17:28Z",
    "author_association": "CONTRIBUTOR",
    "body": "@user1827356 \n\nyou can already do this with the csv iterator (or even better with the HDF iterator), both are row-oriented\n\nsee docs http://pandas.pydata.org/pandas-docs/dev/io.html#iterating-through-files-chunk-by-chunk, and http://pandas.pydata.org/pandas-docs/dev/io.html#iterator\n\n@Zelazny7 is working specifically with column oriented data, in that you need to add/delete columns which row-oriented doesn't support as well, because you a) need to track the adds/deletes (and have a layer that allows you to do this transparently), and b) brings in ALL data in a row whenever you bring that row in; in a very wide table this can be somewhat inefficient, a partial solution is this: http://pandas.pydata.org/pandas-docs/dev/io.html#multiple-table-queries\n\nwhat is the problem you are trying to address?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/17350300",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-17350300",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 17350300,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3MzUwMzAw",
    "user": {
      "login": "user1827356",
      "id": 2691720,
      "node_id": "MDQ6VXNlcjI2OTE3MjA=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/2691720?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/user1827356",
      "html_url": "https://github.com/user1827356",
      "followers_url": "https://api.github.com/users/user1827356/followers",
      "following_url": "https://api.github.com/users/user1827356/following{/other_user}",
      "gists_url": "https://api.github.com/users/user1827356/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/user1827356/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/user1827356/subscriptions",
      "organizations_url": "https://api.github.com/users/user1827356/orgs",
      "repos_url": "https://api.github.com/users/user1827356/repos",
      "events_url": "https://api.github.com/users/user1827356/events{/privacy}",
      "received_events_url": "https://api.github.com/users/user1827356/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-02T16:56:37Z",
    "updated_at": "2013-05-02T16:59:31Z",
    "author_association": "NONE",
    "body": "@jreback \n\nAs already noted (in your case a), reading chunk by chunk does not solve adding/deleting columns\n\nI was trying to point out that a column oriented data base by itself does not completely solve the problem of large data sets (as column might be too large to load into memory). \n\nHence, you need to chunk ROWS AS WELL AS COLUMNS\n\nIMO with large datasets, the challenge is to how to 'chunk' the data. If there were a generic way to do it like proposed in blaze (my opinion comes from this - http://vimeo.com/53031980) that would be great\n\nBut allowing a user who is knowledgeable about the dataset to specify dimensions in which he would want to do this would be best. To explain,\n\nScenario 1: A user like Zelazny7 might want to 'chunk' the dataset merely by row number i.e. 10000 rows etc\nScenario 2: In finance, one might want to chunk by 'stock' or by 'date' or both\n\nAs far as case (b) goes, the API I suggested would clearly help as it could be extended to allow the user to specify which of the columns she wants passed to the function\n\nIMO providing an API like above, although not generic would be easier to implement and hence, have a larger impact quicker.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/17350987",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-17350987",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 17350987,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3MzUwOTg3",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-02T17:09:12Z",
    "updated_at": "2013-05-02T17:09:12Z",
    "author_association": "CONTRIBUTOR",
    "body": "@user1827356 \n\nwhat you are suggesting is quite straightforward with HDFStore today. Write your dataset as a table, and use an iterator (possibly with a query). You will then chunk by rows (or by whatever is in your query). I do this all the time by lining my data up in the appropriate way.\n\nUsing your example I keep a panel of: `items x dates x ids`\nThen it is stored as a 2-d indexable with the columns of items, and dates x ids in the rows. Then when you chunk you get what you want.\n\nI also do this is a 4-d case, providing arbitrary axes support. See http://pandas.pydata.org/pandas-docs/dev/io.html#experimental.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/17351519",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-17351519",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 17351519,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3MzUxNTE5",
    "user": {
      "login": "user1827356",
      "id": 2691720,
      "node_id": "MDQ6VXNlcjI2OTE3MjA=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/2691720?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/user1827356",
      "html_url": "https://github.com/user1827356",
      "followers_url": "https://api.github.com/users/user1827356/followers",
      "following_url": "https://api.github.com/users/user1827356/following{/other_user}",
      "gists_url": "https://api.github.com/users/user1827356/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/user1827356/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/user1827356/subscriptions",
      "organizations_url": "https://api.github.com/users/user1827356/orgs",
      "repos_url": "https://api.github.com/users/user1827356/repos",
      "events_url": "https://api.github.com/users/user1827356/events{/privacy}",
      "received_events_url": "https://api.github.com/users/user1827356/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-02T17:20:02Z",
    "updated_at": "2013-05-02T17:20:02Z",
    "author_association": "NONE",
    "body": "@jreback \n\nTo be honest, I haven't paid much attention to HDFStore thus far. I'll go through it and seem if it fits the needs. Thanks\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/17623726",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-17623726",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 17623726,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3NjIzNzI2",
    "user": {
      "login": "user1827356",
      "id": 2691720,
      "node_id": "MDQ6VXNlcjI2OTE3MjA=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/2691720?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/user1827356",
      "html_url": "https://github.com/user1827356",
      "followers_url": "https://api.github.com/users/user1827356/followers",
      "following_url": "https://api.github.com/users/user1827356/following{/other_user}",
      "gists_url": "https://api.github.com/users/user1827356/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/user1827356/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/user1827356/subscriptions",
      "organizations_url": "https://api.github.com/users/user1827356/orgs",
      "repos_url": "https://api.github.com/users/user1827356/repos",
      "events_url": "https://api.github.com/users/user1827356/events{/privacy}",
      "received_events_url": "https://api.github.com/users/user1827356/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-08T18:14:39Z",
    "updated_at": "2013-05-09T19:57:38Z",
    "author_association": "NONE",
    "body": "@jreback - I briefly reviewed HDFStore and it seems to me that its applicable to out-of-core computing more that multi-process or multi-host processing. For example, having centralized metadata is going to be challenging when you have multiple parallel writers. Is there a way to do this with HDFStore? Or maybe there is a way to split the dataset over multiple files?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/17624757",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-17624757",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 17624757,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3NjI0NzU3",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-08T18:29:46Z",
    "updated_at": "2013-05-08T18:29:46Z",
    "author_association": "CONTRIBUTOR",
    "body": "Parallel writes are not supported (the docs warn you away from this quite a bit), but you can easily read in parallel. It is quite straightfoward to avoid parallel writes, however; if you need a combine step, then do that synchronously and continue on.\n\nI can often parrallelize (either multi-core of distributed) processes. collect the results, combine, then spin off more processes to do the next step. This is a fairly typical way of doing things.\n\nHDFStore could easily support this method of computation. So I am not exactly sure what you are asking.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/17625914",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-17625914",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 17625914,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3NjI1OTE0",
    "user": {
      "login": "user1827356",
      "id": 2691720,
      "node_id": "MDQ6VXNlcjI2OTE3MjA=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/2691720?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/user1827356",
      "html_url": "https://github.com/user1827356",
      "followers_url": "https://api.github.com/users/user1827356/followers",
      "following_url": "https://api.github.com/users/user1827356/following{/other_user}",
      "gists_url": "https://api.github.com/users/user1827356/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/user1827356/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/user1827356/subscriptions",
      "organizations_url": "https://api.github.com/users/user1827356/orgs",
      "repos_url": "https://api.github.com/users/user1827356/repos",
      "events_url": "https://api.github.com/users/user1827356/events{/privacy}",
      "received_events_url": "https://api.github.com/users/user1827356/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-08T18:47:21Z",
    "updated_at": "2013-05-08T18:47:21Z",
    "author_association": "NONE",
    "body": "I don't follow\n- 'quite straight forward to avoid parallel writes'\n- 'collect the results, combine'\n\nWhen you parallelize the task, how do you store the intermediate results and combine them? One way would be to store them as different files and then combine them on read. Is that the suggestion?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/17626255",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-17626255",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 17626255,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3NjI2MjU1",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-08T18:52:43Z",
    "updated_at": "2013-05-08T18:52:43Z",
    "author_association": "CONTRIBUTOR",
    "body": "yes, that's exactly what you would do. I don't know exactly what problem you are attempting, but for instance:\n\n1) split data, write to work file\n2) apply function to work file, generate results file\n3) combine results into new file\n4) repeat\n\n1 and 3 are not parallellizable, but 2 could be distribution, multi-core, or single-core\n(I am generalizing a bit, it is possible 1 and 3 possibily _could_ be done in parallel, but too complicated)\n\nThis is essentially a map-reduce paradigm\n\nand here's my 2c. solve your problem in a general way single-core, then , and only if its not efficient, distribute/multi-core. too often people are prematurely optimizing!\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/17628408",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-17628408",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 17628408,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3NjI4NDA4",
    "user": {
      "login": "user1827356",
      "id": 2691720,
      "node_id": "MDQ6VXNlcjI2OTE3MjA=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/2691720?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/user1827356",
      "html_url": "https://github.com/user1827356",
      "followers_url": "https://api.github.com/users/user1827356/followers",
      "following_url": "https://api.github.com/users/user1827356/following{/other_user}",
      "gists_url": "https://api.github.com/users/user1827356/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/user1827356/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/user1827356/subscriptions",
      "organizations_url": "https://api.github.com/users/user1827356/orgs",
      "repos_url": "https://api.github.com/users/user1827356/repos",
      "events_url": "https://api.github.com/users/user1827356/events{/privacy}",
      "received_events_url": "https://api.github.com/users/user1827356/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-08T19:29:12Z",
    "updated_at": "2013-05-08T19:29:12Z",
    "author_association": "NONE",
    "body": "OK, what you describe is close to the workflow I have. One of the differences is I use csv files instead of HDFStore\n\nThe problems I have with this workflow are:\n- Its too manual, for different data sets (I have a some parts of it as re-usable, but still hacked together)\n- It doesn't work intuitively with pandas (Interface should be like groupby-apply)\n- It doesn't work with multiple backends i.e. ipython, custom grid, multiprocessing etc.\n\nI think it'll be a great addition to pandas to provide a transparent way to achieve this. In short, an out of the box interface that allows this would be great\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/17629318",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-17629318",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 17629318,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3NjI5MzE4",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-08T19:46:57Z",
    "updated_at": "2013-05-08T19:46:57Z",
    "author_association": "CONTRIBUTOR",
    "body": "the intent of this issue was to provide discussion related to the features of this, to in fact make it easier. This is a non-trivial problem as data computations of this nature can take several different forms.\n\nI think supporting csv would be nice, but certainly won't be on the first go (nor would you really want it anyhow, for a big file this is very inefficient, you need to be able to slice chunks out of it - good thing is that this is easy to fix but splitting the file or putting in HDF5)\n\nwhat do you mean by multiple backends? you really mean multiple methods of computation / distribution, correct?\n\nwhat would help is a pseudo code like specification (may with some sample data) to get a good idea of what different workflows are\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/17677733",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-17677733",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 17677733,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3Njc3NzMz",
    "user": {
      "login": "user1827356",
      "id": 2691720,
      "node_id": "MDQ6VXNlcjI2OTE3MjA=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/2691720?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/user1827356",
      "html_url": "https://github.com/user1827356",
      "followers_url": "https://api.github.com/users/user1827356/followers",
      "following_url": "https://api.github.com/users/user1827356/following{/other_user}",
      "gists_url": "https://api.github.com/users/user1827356/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/user1827356/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/user1827356/subscriptions",
      "organizations_url": "https://api.github.com/users/user1827356/orgs",
      "repos_url": "https://api.github.com/users/user1827356/repos",
      "events_url": "https://api.github.com/users/user1827356/events{/privacy}",
      "received_events_url": "https://api.github.com/users/user1827356/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-09T17:34:58Z",
    "updated_at": "2013-05-09T19:57:57Z",
    "author_association": "NONE",
    "body": "@jreback \nBy different backends I do mean multiple methods of distribution\n\n---\n\nI also briefly reviewed this paper http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/36632.pdf\n\nIMO, a lot of it's complexity has to do with the fact that the data they deal with is sparse. If we assume the data we deal with is dense, Figure 1 proposes a 'columnar representation of nested data'. I find it very interesting. I understand it might have some similarities to HDFStore, but the differences I see are:\n- It's columnar and hence, can support addition/deletion of columns\n- It's distributed and hence, can support multiple writes\n- To quote - 'As an added benet, data in a le system can be conveniently manipulated using standard tools, e.g., to transfer to another cluster, change access privileges, or identify a subset of data\n  for analysis based on le names'\n\n---\n\nAs far as the API goes, let's assume a simple financial data problem\ndate, stock, price, qty\n5/9/2013, A, 55.55, 100\n5/9/2013, B, 99.99, 300\n...\n5/10/2013, A, 55.65, 100\n5/10/2013, B, 99.89, 100\n\nNow, I want to calculate VWAP (volume weighted average price per stock/per day)\ndf.groupby(['date', 'stock']).apply(lambda sdf: (sdf.price \\* sdf.qty)/sdf.qty.sum())\n\nUsually there are ~8000 stocks, ~250 million rows/day, 60+ days of data\n\nIdeally, I still want to keep a similar interface\n\ndef vwap(sdf): return (sdf.price \\* sdf.qty)/sdf.qty.sum()\n\nres = df.groupby(['date', 'stock']).apply(vwap, [optional arg(s) for distribution])\n\nSo \n- df should encapsulate data distributed over multiple files\n- apply should distribute data in grouped chunks over different jobs\n- apply should collate returned values if any\n- apply should optionally persist columns added/changed to sdf by function equivalent to vwap\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/20012946",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-20012946",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 20012946,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIwMDEyOTQ2",
    "user": {
      "login": "lrq3000",
      "id": 1118942,
      "node_id": "MDQ6VXNlcjExMTg5NDI=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1118942?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/lrq3000",
      "html_url": "https://github.com/lrq3000",
      "followers_url": "https://api.github.com/users/lrq3000/followers",
      "following_url": "https://api.github.com/users/lrq3000/following{/other_user}",
      "gists_url": "https://api.github.com/users/lrq3000/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/lrq3000/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/lrq3000/subscriptions",
      "organizations_url": "https://api.github.com/users/lrq3000/orgs",
      "repos_url": "https://api.github.com/users/lrq3000/repos",
      "events_url": "https://api.github.com/users/lrq3000/events{/privacy}",
      "received_events_url": "https://api.github.com/users/lrq3000/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-06-25T22:37:40Z",
    "updated_at": "2013-06-25T22:38:55Z",
    "author_association": "NONE",
    "body": "In my case, the approach presented here using HDF stores is not working when one need to do computations on a whole column or a whole line from a huge store since Pandas will retrieve the row/column and store it in memory to do the processing.\nIsn't there any way to overload the low-level cell/row/column getter/setter to make them work directly on/from a file on the disk rather than loading the full data in memory?\nI mean for example, instead of providing computation functions with a DataFrame, would it be possible to provide them with a meta-DataFrame describing the DataFrame, but only providing the value of a cell on-access. Eg: when computing the mean, or any operation in fact, you only need to retrieve the scalar value from 1 cell at a time.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/20023271",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-20023271",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 20023271,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIwMDIzMjcx",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-06-26T02:11:16Z",
    "updated_at": "2013-06-26T02:11:16Z",
    "author_association": "CONTRIBUTOR",
    "body": "retrieveing only 1 value at a time is extremely inefficient. you can certainly slice a `HDFStore` now using `start` and `stop` and shard the columns out to different groups so as to effectively have a column store, while being able to piecewise do computations.\n\nThis trick is that only a small group of computations can actually be done this way (e.g. even `mean` is tricky because you need to accumulate the sum and also the count).\n\nThe point of this PR is to wrap some of this functionality up in a way that is more transparent to the user.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/54357946",
    "html_url": "https://github.com/pandas-dev/pandas/issues/3202#issuecomment-54357946",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/3202",
    "id": 54357946,
    "node_id": "MDEyOklzc3VlQ29tbWVudDU0MzU3OTQ2",
    "user": {
      "login": "datnamer",
      "id": 8646471,
      "node_id": "MDQ6VXNlcjg2NDY0NzE=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/8646471?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/datnamer",
      "html_url": "https://github.com/datnamer",
      "followers_url": "https://api.github.com/users/datnamer/followers",
      "following_url": "https://api.github.com/users/datnamer/following{/other_user}",
      "gists_url": "https://api.github.com/users/datnamer/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/datnamer/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/datnamer/subscriptions",
      "organizations_url": "https://api.github.com/users/datnamer/orgs",
      "repos_url": "https://api.github.com/users/datnamer/repos",
      "events_url": "https://api.github.com/users/datnamer/events{/privacy}",
      "received_events_url": "https://api.github.com/users/datnamer/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-09-03T20:10:24Z",
    "updated_at": "2014-09-03T20:10:24Z",
    "author_association": "NONE",
    "body": "Just wanted to enthusiastically  +1 this. \n\nMight be helpful also to take a look at the following aspects of the blaze project: \n\nDynd: https://github.com/ContinuumIO/dynd-python \n\nBcolz: https://github.com/Blosc/bcolz \n"
  }
]
