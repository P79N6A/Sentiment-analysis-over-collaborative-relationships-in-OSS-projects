[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/69920025",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9246#issuecomment-69920025",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9246",
    "id": 69920025,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY5OTIwMDI1",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-01-14T14:06:17Z",
    "updated_at": "2015-01-14T14:07:10Z",
    "author_association": "CONTRIBUTOR",
    "body": "You cannot do this with HDF5. Instead you can see below, where you create an in-memory store. I thought this was docced somewhere but I guess not.\n\n```\nIn [15]: df = DataFrame(1,columns=list('ab'),index=range(3))\n\nIn [17]: store = pd.HDFStore('foo.h5',driver='H5FD_CORE',driver_core_backing_store=0)\n\nIn [18]: !ls -ltr *.h5\n-rw-rw-r--  1 jreback  staff  27472602 Dec  2 06:54 test2.h5\n-rw-rw-r--  1 jreback  staff  88416100 Jan  4 14:58 test.h5\n\nIn [19]: store.append('df',df)\n\nIn [20]: store.select('df')\nOut[20]: \n   a  b\n0  1  1\n1  1  1\n2  1  1\n\nIn [21]: store.close()\n\nIn [22]: !ls -ltr *.h5\n-rw-rw-r--  1 jreback  staff  27472602 Dec  2 06:54 test2.h5\n-rw-rw-r--  1 jreback  staff  88416100 Jan  4 14:58 test.h5\n```\n\nNote that in general these in-memory stores are a fair bit slower than just keeping the data in-memory. So not a whole lot of reason to use them (except perhaps to identical selection semantics to an on-disk store).\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/69970976",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9246#issuecomment-69970976",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9246",
    "id": 69970976,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY5OTcwOTc2",
    "user": {
      "login": "jennolsen84",
      "id": 10360673,
      "node_id": "MDQ6VXNlcjEwMzYwNjcz",
      "avatar_url": "https://avatars0.githubusercontent.com/u/10360673?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jennolsen84",
      "html_url": "https://github.com/jennolsen84",
      "followers_url": "https://api.github.com/users/jennolsen84/followers",
      "following_url": "https://api.github.com/users/jennolsen84/following{/other_user}",
      "gists_url": "https://api.github.com/users/jennolsen84/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jennolsen84/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jennolsen84/subscriptions",
      "organizations_url": "https://api.github.com/users/jennolsen84/orgs",
      "repos_url": "https://api.github.com/users/jennolsen84/repos",
      "events_url": "https://api.github.com/users/jennolsen84/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jennolsen84/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-01-14T19:01:49Z",
    "updated_at": "2015-01-14T19:01:49Z",
    "author_association": "CONTRIBUTOR",
    "body": "Is there a way to get the contents of that file as bytes?  I would like to send it over the network to another machine, without writing it to disk.\n\nIf not HDF5, do you have any other recommendations that could possibly compress (ideally using blosc) the DataFrame and give us the bytes to send over the wire.  I guess writing to a ramdisk is an option, but it seems overkill.  msgpack doesn't seem to support compression on the read yet.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/70016412",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9246#issuecomment-70016412",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9246",
    "id": 70016412,
    "node_id": "MDEyOklzc3VlQ29tbWVudDcwMDE2NDEy",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-01-14T23:52:20Z",
    "updated_at": "2015-01-14T23:52:20Z",
    "author_association": "CONTRIBUTOR",
    "body": "you can use msgpack thrn compress the bytes \n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/70153506",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9246#issuecomment-70153506",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9246",
    "id": 70153506,
    "node_id": "MDEyOklzc3VlQ29tbWVudDcwMTUzNTA2",
    "user": {
      "login": "jennolsen84",
      "id": 10360673,
      "node_id": "MDQ6VXNlcjEwMzYwNjcz",
      "avatar_url": "https://avatars0.githubusercontent.com/u/10360673?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jennolsen84",
      "html_url": "https://github.com/jennolsen84",
      "followers_url": "https://api.github.com/users/jennolsen84/followers",
      "following_url": "https://api.github.com/users/jennolsen84/following{/other_user}",
      "gists_url": "https://api.github.com/users/jennolsen84/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jennolsen84/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jennolsen84/subscriptions",
      "organizations_url": "https://api.github.com/users/jennolsen84/orgs",
      "repos_url": "https://api.github.com/users/jennolsen84/repos",
      "events_url": "https://api.github.com/users/jennolsen84/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jennolsen84/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-01-15T20:14:29Z",
    "updated_at": "2015-01-15T20:14:29Z",
    "author_association": "CONTRIBUTOR",
    "body": "Yes, that kind of works -- though not ideal.  I found out that you can just do:\n\nbindata = store._handle.get_file_image()\n\nand that might be a better work around.  Can we expose that as part of an official api somehow so that it doesn't break the next time I upgrade pandas?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/70255022",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9246#issuecomment-70255022",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9246",
    "id": 70255022,
    "node_id": "MDEyOklzc3VlQ29tbWVudDcwMjU1MDIy",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-01-16T13:45:10Z",
    "updated_at": "2015-01-16T13:45:10Z",
    "author_association": "CONTRIBUTOR",
    "body": "Let' start off with a cookbook recipe for this. Would you like to do a PR for that?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/71353282",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9246#issuecomment-71353282",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9246",
    "id": 71353282,
    "node_id": "MDEyOklzc3VlQ29tbWVudDcxMzUzMjgy",
    "user": {
      "login": "jennolsen84",
      "id": 10360673,
      "node_id": "MDQ6VXNlcjEwMzYwNjcz",
      "avatar_url": "https://avatars0.githubusercontent.com/u/10360673?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jennolsen84",
      "html_url": "https://github.com/jennolsen84",
      "followers_url": "https://api.github.com/users/jennolsen84/followers",
      "following_url": "https://api.github.com/users/jennolsen84/following{/other_user}",
      "gists_url": "https://api.github.com/users/jennolsen84/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jennolsen84/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jennolsen84/subscriptions",
      "organizations_url": "https://api.github.com/users/jennolsen84/orgs",
      "repos_url": "https://api.github.com/users/jennolsen84/repos",
      "events_url": "https://api.github.com/users/jennolsen84/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jennolsen84/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-01-25T05:21:02Z",
    "updated_at": "2015-01-25T05:21:02Z",
    "author_association": "CONTRIBUTOR",
    "body": "Sure\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/74041497",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9246#issuecomment-74041497",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9246",
    "id": 74041497,
    "node_id": "MDEyOklzc3VlQ29tbWVudDc0MDQxNDk3",
    "user": {
      "login": "filmor",
      "id": 30848,
      "node_id": "MDQ6VXNlcjMwODQ4",
      "avatar_url": "https://avatars3.githubusercontent.com/u/30848?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/filmor",
      "html_url": "https://github.com/filmor",
      "followers_url": "https://api.github.com/users/filmor/followers",
      "following_url": "https://api.github.com/users/filmor/following{/other_user}",
      "gists_url": "https://api.github.com/users/filmor/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/filmor/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/filmor/subscriptions",
      "organizations_url": "https://api.github.com/users/filmor/orgs",
      "repos_url": "https://api.github.com/users/filmor/repos",
      "events_url": "https://api.github.com/users/filmor/events{/privacy}",
      "received_events_url": "https://api.github.com/users/filmor/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-02-12T09:30:51Z",
    "updated_at": "2015-02-12T09:30:51Z",
    "author_association": "CONTRIBUTOR",
    "body": "There is a (not merged) pull request that implemented just that: https://github.com/pydata/pandas/pull/6519\n\nI've been using the following functions in production for quite a while:\n\n```\ndef read_hdf_from_buffer(buffer, key=\"/data\"):\n     from pandas import get_store\n     with get_store(\n            \"data.h5\",\n            mode=\"r\",\n            driver=\"H5FD_CORE\",\n            driver_core_backing_store=0,\n            driver_core_image=buffer.read()\n            ) as store:\n        return store[key]\n\ndef write_hdf_to_buffer(df):\n    from pandas import get_store\n    with get_store(\n            \"data.h5\", mode=\"a\", driver=\"H5FD_CORE\",\n            driver_core_backing_store=0\n            ) as out:\n        out[\"/data\"] = df\n        return out._handle.get_file_image()\n```\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/302089476",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9246#issuecomment-302089476",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9246",
    "id": 302089476,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjA4OTQ3Ng==",
    "user": {
      "login": "argoneus",
      "id": 365334,
      "node_id": "MDQ6VXNlcjM2NTMzNA==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/365334?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/argoneus",
      "html_url": "https://github.com/argoneus",
      "followers_url": "https://api.github.com/users/argoneus/followers",
      "following_url": "https://api.github.com/users/argoneus/following{/other_user}",
      "gists_url": "https://api.github.com/users/argoneus/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/argoneus/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/argoneus/subscriptions",
      "organizations_url": "https://api.github.com/users/argoneus/orgs",
      "repos_url": "https://api.github.com/users/argoneus/repos",
      "events_url": "https://api.github.com/users/argoneus/events{/privacy}",
      "received_events_url": "https://api.github.com/users/argoneus/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-17T13:26:42Z",
    "updated_at": "2017-05-17T13:26:42Z",
    "author_association": "NONE",
    "body": "@filmor I've implemented this and it appears to work well, thanks!  One issue that I've noticed is that returning the `get_file_image()` works for a single key, but you can't simply concatenate the binary file data together from multiple calls to `write_hdf_to_buffer()`.  How have you accounted for this, or are you simply writing a single key and associated data to individual files?"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/302091875",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9246#issuecomment-302091875",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9246",
    "id": 302091875,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjA5MTg3NQ==",
    "user": {
      "login": "filmor",
      "id": 30848,
      "node_id": "MDQ6VXNlcjMwODQ4",
      "avatar_url": "https://avatars3.githubusercontent.com/u/30848?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/filmor",
      "html_url": "https://github.com/filmor",
      "followers_url": "https://api.github.com/users/filmor/followers",
      "following_url": "https://api.github.com/users/filmor/following{/other_user}",
      "gists_url": "https://api.github.com/users/filmor/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/filmor/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/filmor/subscriptions",
      "organizations_url": "https://api.github.com/users/filmor/orgs",
      "repos_url": "https://api.github.com/users/filmor/repos",
      "events_url": "https://api.github.com/users/filmor/events{/privacy}",
      "received_events_url": "https://api.github.com/users/filmor/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-17T13:35:23Z",
    "updated_at": "2017-05-17T13:35:23Z",
    "author_association": "CONTRIBUTOR",
    "body": "@argoneus I'm not sure I get what you mean. The particular functions given are implemented for just a single key, but nothing prevents you from getting/storing more than one key as long as the `store` object exists. What exactly are you trying to do?"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/302100057",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9246#issuecomment-302100057",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9246",
    "id": 302100057,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjEwMDA1Nw==",
    "user": {
      "login": "argoneus",
      "id": 365334,
      "node_id": "MDQ6VXNlcjM2NTMzNA==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/365334?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/argoneus",
      "html_url": "https://github.com/argoneus",
      "followers_url": "https://api.github.com/users/argoneus/followers",
      "following_url": "https://api.github.com/users/argoneus/following{/other_user}",
      "gists_url": "https://api.github.com/users/argoneus/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/argoneus/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/argoneus/subscriptions",
      "organizations_url": "https://api.github.com/users/argoneus/orgs",
      "repos_url": "https://api.github.com/users/argoneus/repos",
      "events_url": "https://api.github.com/users/argoneus/events{/privacy}",
      "received_events_url": "https://api.github.com/users/argoneus/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-17T14:03:29Z",
    "updated_at": "2017-05-17T14:05:00Z",
    "author_association": "NONE",
    "body": "@filmor I'm writing an HDF5 file in Azure Data Lake (ADL), and am using their Python APIs to open a buffer.  I'd like to write multiple keys and associated data to the same file.  Using your `write_hdf_to_buffer()`, it appears that the binary data is being written, but the keys aren't getting updated in the file.  In my test, I have 4 pandas data frames with 4 unique keys, and want to write these to an .h5 file in ADL.  The `write()`s seemingly work properly, but the resulting file only has one key when I do a `hdf.keys()` instead of 4.  When I run the test dataset and write to local disk (using standard `df.to_hdf()`), the keys are all there of course.  The file sizes of the ADL file and the local file match, which leads me to believe the df contents are written, but the keys aren't being updated, since I'm seeing 1 instead of 4 for the ADL .h5 file.  I'll keep experimenting with it; was just curious if you had to do anything extra to support multiple keys in the same file with your approach."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/302103586",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9246#issuecomment-302103586",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9246",
    "id": 302103586,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjEwMzU4Ng==",
    "user": {
      "login": "filmor",
      "id": 30848,
      "node_id": "MDQ6VXNlcjMwODQ4",
      "avatar_url": "https://avatars3.githubusercontent.com/u/30848?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/filmor",
      "html_url": "https://github.com/filmor",
      "followers_url": "https://api.github.com/users/filmor/followers",
      "following_url": "https://api.github.com/users/filmor/following{/other_user}",
      "gists_url": "https://api.github.com/users/filmor/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/filmor/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/filmor/subscriptions",
      "organizations_url": "https://api.github.com/users/filmor/orgs",
      "repos_url": "https://api.github.com/users/filmor/repos",
      "events_url": "https://api.github.com/users/filmor/events{/privacy}",
      "received_events_url": "https://api.github.com/users/filmor/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-17T14:15:03Z",
    "updated_at": "2017-05-17T14:15:03Z",
    "author_association": "CONTRIBUTOR",
    "body": "Just return the store directly in the `read` function and write multiple frames in the `write`:\r\n\r\n    def write_hdf_to_buffer(frames):\r\n        from pandas import get_store\r\n        with get_store(\r\n                \"data.h5\", mode=\"a\", driver=\"H5FD_CORE\",\r\n                driver_core_backing_store=0\r\n                ) as out:\r\n            for key, df in frames.items():\r\n                out[key] = df\r\n            return out._handle.get_file_image()\r\n\r\n     def read_hdf_from_buffer(buffer):\r\n          from pandas import get_store\r\n          return get_store(\r\n                 \"data.h5\",\r\n                 mode=\"r\",\r\n                 driver=\"H5FD_CORE\",\r\n                 driver_core_backing_store=0,\r\n                 driver_core_image=buffer.read()\r\n                 )"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/302151593",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9246#issuecomment-302151593",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9246",
    "id": 302151593,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjE1MTU5Mw==",
    "user": {
      "login": "argoneus",
      "id": 365334,
      "node_id": "MDQ6VXNlcjM2NTMzNA==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/365334?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/argoneus",
      "html_url": "https://github.com/argoneus",
      "followers_url": "https://api.github.com/users/argoneus/followers",
      "following_url": "https://api.github.com/users/argoneus/following{/other_user}",
      "gists_url": "https://api.github.com/users/argoneus/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/argoneus/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/argoneus/subscriptions",
      "organizations_url": "https://api.github.com/users/argoneus/orgs",
      "repos_url": "https://api.github.com/users/argoneus/repos",
      "events_url": "https://api.github.com/users/argoneus/events{/privacy}",
      "received_events_url": "https://api.github.com/users/argoneus/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-05-17T16:51:15Z",
    "updated_at": "2017-05-17T18:24:01Z",
    "author_association": "NONE",
    "body": "Yes, that works -- reading the `store` file contents each time and returning to `write_hdf_to_buffer()` prior to adding the new dataframe.  However, I was trying to do this without having to read the entire contents (from ADL) each time -- this is inefficient if processing a few hundred keys.  I'm using Python's `multiprocessing` library and handling a key per process (reading data from input, processing, writing to output HDF5), so can't easily concatenate the dataframes and write all content at once. \r\n\r\nI was envisioning something akin to:\r\nwrite first group dataframe to buffer, creating HDF5\r\nwrite second group dataframe to buffer, appending to HDF5 (in 'ab' mode) and update keys (HDF5 root group).  Here is where I'm not sure that it's possible in append mode, if the keys are stored in the file header.\r\n....\r\n\r\nThis would avoid the re-read each time and simply append the new groups content to the file, updating the keys as appropriate.  Not sure if it's possible though."
  }
]
