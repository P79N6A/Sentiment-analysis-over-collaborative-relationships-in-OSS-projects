[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/369793711",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19958#issuecomment-369793711",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19958",
    "id": 369793711,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTc5MzcxMQ==",
    "user": {
      "login": "bobhaffner",
      "id": 8531623,
      "node_id": "MDQ6VXNlcjg1MzE2MjM=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/8531623?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/bobhaffner",
      "html_url": "https://github.com/bobhaffner",
      "followers_url": "https://api.github.com/users/bobhaffner/followers",
      "following_url": "https://api.github.com/users/bobhaffner/following{/other_user}",
      "gists_url": "https://api.github.com/users/bobhaffner/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/bobhaffner/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/bobhaffner/subscriptions",
      "organizations_url": "https://api.github.com/users/bobhaffner/orgs",
      "repos_url": "https://api.github.com/users/bobhaffner/repos",
      "events_url": "https://api.github.com/users/bobhaffner/events{/privacy}",
      "received_events_url": "https://api.github.com/users/bobhaffner/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-02T01:51:15Z",
    "updated_at": "2018-03-02T02:05:39Z",
    "author_association": "CONTRIBUTOR",
    "body": "Hi, thanks for submitting this issue\r\n\r\nYeah, concats are usually the way to go, but that's usually under the assumption that all the DataFrame columns are the same.  Which is the case in your frames_sameindex but not in frames_difindex.  \r\n\r\nI was going to look at the concat source to determine why it's slow for frames_difindex, but it's extensive and I don't have the time right now.   I'm guessing there's a lot of column additions being done for each of the 1,000 frames\r\n\r\nOut of curiosity, why are you looking to concat DataFrames that are seemingly so different?\r\n\r\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/369798812",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19958#issuecomment-369798812",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19958",
    "id": 369798812,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTc5ODgxMg==",
    "user": {
      "login": "jeremywhelchel",
      "id": 835184,
      "node_id": "MDQ6VXNlcjgzNTE4NA==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/835184?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jeremywhelchel",
      "html_url": "https://github.com/jeremywhelchel",
      "followers_url": "https://api.github.com/users/jeremywhelchel/followers",
      "following_url": "https://api.github.com/users/jeremywhelchel/following{/other_user}",
      "gists_url": "https://api.github.com/users/jeremywhelchel/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jeremywhelchel/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jeremywhelchel/subscriptions",
      "organizations_url": "https://api.github.com/users/jeremywhelchel/orgs",
      "repos_url": "https://api.github.com/users/jeremywhelchel/repos",
      "events_url": "https://api.github.com/users/jeremywhelchel/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jeremywhelchel/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-02T02:21:47Z",
    "updated_at": "2018-03-02T02:21:47Z",
    "author_association": "NONE",
    "body": "Thank you for taking a look!\r\n\r\nMy use case was that I had a timeseries of many data points leading up to some event for each these 46k records. The timeseries was represented in a wide format, so columns were N=1,2,3,4,5,... Some frames only had a handful of events (cols 1-20 say), while others had the whole window (cols 1-600).\r\nThese timeseries were all generated in a parallel pipeline, hence why they were individual dataframes.\r\n\r\nI came up with a good workaround, which was just to hard-code the max window size I want and reindex the columns upstream to that: df.reindex(RangeIndex(0,600), axis=1). So when it comes time to concat the column index is all the same. Doesn't take too long in that case.\r\n\r\nAdmittedly this is an odd use-case, though I feel like I've had other performance issues with concat'ing dissimilar frames before. I guess the code path is vastly different when the indices aren't equal. There might be some optimization there. Perhaps a pass to reindex everything to the union of all input indices first, then do the concat. For larger sets that may speed things up.\r\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/369803668",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19958#issuecomment-369803668",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19958",
    "id": 369803668,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTgwMzY2OA==",
    "user": {
      "login": "jschendel",
      "id": 5332445,
      "node_id": "MDQ6VXNlcjUzMzI0NDU=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/5332445?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jschendel",
      "html_url": "https://github.com/jschendel",
      "followers_url": "https://api.github.com/users/jschendel/followers",
      "following_url": "https://api.github.com/users/jschendel/following{/other_user}",
      "gists_url": "https://api.github.com/users/jschendel/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jschendel/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jschendel/subscriptions",
      "organizations_url": "https://api.github.com/users/jschendel/orgs",
      "repos_url": "https://api.github.com/users/jschendel/repos",
      "events_url": "https://api.github.com/users/jschendel/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jschendel/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-02T02:48:19Z",
    "updated_at": "2018-03-02T02:48:19Z",
    "author_association": "MEMBER",
    "body": "Yeah, looks like there should be some way to speed situations like this up, but I'm not super familiar with this part of the codebase.\r\n\r\nSome quickly put together timings using a generic approach that forces reindexing:\r\n\r\n```python\r\n\r\nIn [2]: from pandas.core.indexes.api import _union_indexes\r\n   ...:\r\n   ...: def concat_force_reindex(dfs):\r\n   ...:     all_cols = _union_indexes([df.columns for df in dfs])\r\n   ...:     dfs = (df.reindex(columns=all_cols) for df in dfs)\r\n   ...:     return pd.concat(dfs)\r\n\r\nIn [3]: a = pd.concat(frames_difindex)\r\n\r\nIn [4]: b = concat_force_reindex(frames_difindex)\r\n\r\nIn [5]: a.equals(b)\r\nOut[5]: True\r\n\r\nIn [6]: %timeit concat_force_reindex(frames_difindex)\r\n667 ms ± 3.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [7]: %timeit pd.concat(frames_difindex)\r\n26.8 s ± 587 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [8]: %timeit pd.concat(frames_sameindex)\r\n50.3 ms ± 349 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\r\n```\r\n\r\nSo force reindexing is ~13x slower than concating with common columns, but it's better than the baseline ~530x difference."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/369814382",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19958#issuecomment-369814382",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19958",
    "id": 369814382,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTgxNDM4Mg==",
    "user": {
      "login": "bobhaffner",
      "id": 8531623,
      "node_id": "MDQ6VXNlcjg1MzE2MjM=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/8531623?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/bobhaffner",
      "html_url": "https://github.com/bobhaffner",
      "followers_url": "https://api.github.com/users/bobhaffner/followers",
      "following_url": "https://api.github.com/users/bobhaffner/following{/other_user}",
      "gists_url": "https://api.github.com/users/bobhaffner/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/bobhaffner/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/bobhaffner/subscriptions",
      "organizations_url": "https://api.github.com/users/bobhaffner/orgs",
      "repos_url": "https://api.github.com/users/bobhaffner/repos",
      "events_url": "https://api.github.com/users/bobhaffner/events{/privacy}",
      "received_events_url": "https://api.github.com/users/bobhaffner/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-02T04:08:30Z",
    "updated_at": "2018-03-02T04:08:30Z",
    "author_association": "CONTRIBUTOR",
    "body": "I wonder if you saved your frames in \"long form\" then concat and then performed a pivot?\r\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/369976720",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19958#issuecomment-369976720",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19958",
    "id": 369976720,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTk3NjcyMA==",
    "user": {
      "login": "jeremywhelchel",
      "id": 835184,
      "node_id": "MDQ6VXNlcjgzNTE4NA==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/835184?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jeremywhelchel",
      "html_url": "https://github.com/jeremywhelchel",
      "followers_url": "https://api.github.com/users/jeremywhelchel/followers",
      "following_url": "https://api.github.com/users/jeremywhelchel/following{/other_user}",
      "gists_url": "https://api.github.com/users/jeremywhelchel/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jeremywhelchel/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jeremywhelchel/subscriptions",
      "organizations_url": "https://api.github.com/users/jeremywhelchel/orgs",
      "repos_url": "https://api.github.com/users/jeremywhelchel/repos",
      "events_url": "https://api.github.com/users/jeremywhelchel/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jeremywhelchel/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-02T16:39:50Z",
    "updated_at": "2018-03-02T16:39:50Z",
    "author_association": "NONE",
    "body": "The reindexing method certainly is a huge improvement. If that were standard behavior for concat, I don't think I would be nearly so bothered. Though I doubt you'd want to apply it in every case, and I'm not sure how to determine when you'd want to use it and when not.\r\n\r\nI did try the long form concat you suggested as well. With my 50k-frame dataset, it was just never completing. I tried it with this smaller 10k row contrived dataset and got it to finish reasonably:\r\n```python\r\nsameindex_t = [df.T for df in frames_sameindex]\r\ndifindex_t = [df.T for df in frames_difindex]\r\n%%timeit\r\nsc = pd.concat(sameindex_t, axis=1)\r\n# Output\r\n# 1 loop, best of 3: 385 ms per loop\r\n%%timeit\r\ndc = pd.concat(difindex_t, axis=1)\r\n# Output\r\n# 1 loop, best of 3: 1.88 s per loop\r\n```\r\n\r\nAlso it seems there may be some memory issues with just pd.concat with different indices. With 10k rows my ```pd.concat(frames_difindex)``` call is causing the test colab I'm using to fail, and I suspect because it hit its memory limit. I haven't looked much into it though. TBH I never found a good way to measure memory usage."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/370046123",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19958#issuecomment-370046123",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19958",
    "id": 370046123,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDA0NjEyMw==",
    "user": {
      "login": "gfyoung",
      "id": 9273653,
      "node_id": "MDQ6VXNlcjkyNzM2NTM=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/9273653?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/gfyoung",
      "html_url": "https://github.com/gfyoung",
      "followers_url": "https://api.github.com/users/gfyoung/followers",
      "following_url": "https://api.github.com/users/gfyoung/following{/other_user}",
      "gists_url": "https://api.github.com/users/gfyoung/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/gfyoung/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/gfyoung/subscriptions",
      "organizations_url": "https://api.github.com/users/gfyoung/orgs",
      "repos_url": "https://api.github.com/users/gfyoung/repos",
      "events_url": "https://api.github.com/users/gfyoung/events{/privacy}",
      "received_events_url": "https://api.github.com/users/gfyoung/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-02T20:40:30Z",
    "updated_at": "2018-03-02T20:40:30Z",
    "author_association": "MEMBER",
    "body": "Always welcome performance improvements!  I think you could certainly check if the indices are equivalent before moving on before reindexing, though not sure if that's the way we would want to go.\r\n\r\ncc @jreback @jorisvandenbossche "
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/370050585",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19958#issuecomment-370050585",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19958",
    "id": 370050585,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDA1MDU4NQ==",
    "user": {
      "login": "jschendel",
      "id": 5332445,
      "node_id": "MDQ6VXNlcjUzMzI0NDU=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/5332445?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jschendel",
      "html_url": "https://github.com/jschendel",
      "followers_url": "https://api.github.com/users/jschendel/followers",
      "following_url": "https://api.github.com/users/jschendel/following{/other_user}",
      "gists_url": "https://api.github.com/users/jschendel/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jschendel/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jschendel/subscriptions",
      "organizations_url": "https://api.github.com/users/jschendel/orgs",
      "repos_url": "https://api.github.com/users/jschendel/repos",
      "events_url": "https://api.github.com/users/jschendel/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jschendel/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-02T20:58:55Z",
    "updated_at": "2018-03-02T20:58:55Z",
    "author_association": "MEMBER",
    "body": "Yeah, wasn't suggesting that we should always use the reindexing method, or even that we should use it at all. More so just a proof of concept that improving the performance is possible, and providing a benchmark comparison for whatever solution is implemented."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/370053068",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19958#issuecomment-370053068",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19958",
    "id": 370053068,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDA1MzA2OA==",
    "user": {
      "login": "jorisvandenbossche",
      "id": 1020496,
      "node_id": "MDQ6VXNlcjEwMjA0OTY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1020496?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jorisvandenbossche",
      "html_url": "https://github.com/jorisvandenbossche",
      "followers_url": "https://api.github.com/users/jorisvandenbossche/followers",
      "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}",
      "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions",
      "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs",
      "repos_url": "https://api.github.com/users/jorisvandenbossche/repos",
      "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-02T21:09:29Z",
    "updated_at": "2018-03-02T21:09:29Z",
    "author_association": "MEMBER",
    "body": "concat should basically do such a reindex under the hood, so I think it should certainly be possible to improve the performance to more or less that level of speed. "
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/407696865",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19958#issuecomment-407696865",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19958",
    "id": 407696865,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzY5Njg2NQ==",
    "user": {
      "login": "TKlerx",
      "id": 1579185,
      "node_id": "MDQ6VXNlcjE1NzkxODU=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/1579185?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/TKlerx",
      "html_url": "https://github.com/TKlerx",
      "followers_url": "https://api.github.com/users/TKlerx/followers",
      "following_url": "https://api.github.com/users/TKlerx/following{/other_user}",
      "gists_url": "https://api.github.com/users/TKlerx/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/TKlerx/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/TKlerx/subscriptions",
      "organizations_url": "https://api.github.com/users/TKlerx/orgs",
      "repos_url": "https://api.github.com/users/TKlerx/repos",
      "events_url": "https://api.github.com/users/TKlerx/events{/privacy}",
      "received_events_url": "https://api.github.com/users/TKlerx/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-25T09:41:26Z",
    "updated_at": "2018-07-25T09:41:26Z",
    "author_association": "NONE",
    "body": "Is there any chance of incorporating @jschendel 's code into pandas as @jorisvandenbossche suggested?"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/407700611",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19958#issuecomment-407700611",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19958",
    "id": 407700611,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzcwMDYxMQ==",
    "user": {
      "login": "jorisvandenbossche",
      "id": 1020496,
      "node_id": "MDQ6VXNlcjEwMjA0OTY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1020496?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jorisvandenbossche",
      "html_url": "https://github.com/jorisvandenbossche",
      "followers_url": "https://api.github.com/users/jorisvandenbossche/followers",
      "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}",
      "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions",
      "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs",
      "repos_url": "https://api.github.com/users/jorisvandenbossche/repos",
      "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-25T09:54:49Z",
    "updated_at": "2018-07-25T09:54:49Z",
    "author_association": "MEMBER",
    "body": "That would need somebody diving into the code to see how this performance bottleneck can be improved (it would probably not be simple adding of the code snippet to `concat`, it needs to be integrated into the existing code base). \r\nBut somebody looking into this and PRs are certainly very welcome!"
  }
]
