[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/221840600",
    "html_url": "https://github.com/pandas-dev/pandas/issues/13289#issuecomment-221840600",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/13289",
    "id": 221840600,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIyMTg0MDYwMA==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-05-26T10:58:22Z",
    "updated_at": "2016-05-26T10:58:22Z",
    "author_association": "CONTRIBUTOR",
    "body": "this is by-definition. Specifying `min_itemsize` stores this as a separate fixed-width column (as opposed to storing with other string columns).\n\nNot sure you should actually care about this as its an implementation detail. The warnings are because you are truncating. You are ultimately storing in a fixed-width column, so you must truncate (or you can pre-truncate).\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/221879738",
    "html_url": "https://github.com/pandas-dev/pandas/issues/13289#issuecomment-221879738",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/13289",
    "id": 221879738,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIyMTg3OTczOA==",
    "user": {
      "login": "cchrysostomou",
      "id": 2927161,
      "node_id": "MDQ6VXNlcjI5MjcxNjE=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/2927161?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/cchrysostomou",
      "html_url": "https://github.com/cchrysostomou",
      "followers_url": "https://api.github.com/users/cchrysostomou/followers",
      "following_url": "https://api.github.com/users/cchrysostomou/following{/other_user}",
      "gists_url": "https://api.github.com/users/cchrysostomou/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/cchrysostomou/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/cchrysostomou/subscriptions",
      "organizations_url": "https://api.github.com/users/cchrysostomou/orgs",
      "repos_url": "https://api.github.com/users/cchrysostomou/repos",
      "events_url": "https://api.github.com/users/cchrysostomou/events{/privacy}",
      "received_events_url": "https://api.github.com/users/cchrysostomou/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-05-26T14:03:24Z",
    "updated_at": "2016-05-26T14:03:24Z",
    "author_association": "NONE",
    "body": "All I am aiming to do is store very large tables containing millions of rows and about 10 string columns (strings in most columns will vary in string length) to a file, and later retrieve subsets of all of the information very rapidly using a well defined fixed column (not the long string columns mentioned above). But currently performance seems not as fast as just dumping everything to a csv file and then sorting that csv file on the specific column outside of python...I don't really want to do this because its hacky.\n\nBased on your response, I feel as though I may be using the wrong strategy/implementing to_hdf incorrectly. \n\n1) First, when you say \"I must truncate\"  I assume this means that, if i truncate, when I try to retrieve that record at a later time, I will only be shown a truncated version of the string? If so that is not an option for me as the entire string is important, but in the past when I have set the min_itemsize for a column string to an extremely long length then I get an error, not a warning.\n\n2) With regard to it being an implementation detail, will adding these large strings as data_columns impact my query performance if I am querying on a separate column? Or is the warning message about I/O performance and memory usage not actually relevant in this situation.\n\n3) In the end is there any way to use  to_hdf but have a string column not be a data column and its length be 'variable'?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/221916174",
    "html_url": "https://github.com/pandas-dev/pandas/issues/13289#issuecomment-221916174",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/13289",
    "id": 221916174,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIyMTkxNjE3NA==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-05-26T16:03:40Z",
    "updated_at": "2016-05-26T16:03:40Z",
    "author_association": "CONTRIBUTOR",
    "body": "@costas821 if the warnings bother you then you can truncate. If you don't it WILL be truncated. So you need to choose a min_itemsize that encompasses your entire width you want to save. These are saved as fixed-width strings. Storing variable length strings is not supported.\n\nYou can try using categoricals if you have some repeats.\n\nmaking this a data_column will not be a problem for query performance.\n\nYou must have something funny going on. HDF write perf is WAY faster than csv.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/221947790",
    "html_url": "https://github.com/pandas-dev/pandas/issues/13289#issuecomment-221947790",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/13289",
    "id": 221947790,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIyMTk0Nzc5MA==",
    "user": {
      "login": "cchrysostomou",
      "id": 2927161,
      "node_id": "MDQ6VXNlcjI5MjcxNjE=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/2927161?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/cchrysostomou",
      "html_url": "https://github.com/cchrysostomou",
      "followers_url": "https://api.github.com/users/cchrysostomou/followers",
      "following_url": "https://api.github.com/users/cchrysostomou/following{/other_user}",
      "gists_url": "https://api.github.com/users/cchrysostomou/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/cchrysostomou/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/cchrysostomou/subscriptions",
      "organizations_url": "https://api.github.com/users/cchrysostomou/orgs",
      "repos_url": "https://api.github.com/users/cchrysostomou/repos",
      "events_url": "https://api.github.com/users/cchrysostomou/events{/privacy}",
      "received_events_url": "https://api.github.com/users/cchrysostomou/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-05-26T18:02:23Z",
    "updated_at": "2016-05-26T18:02:23Z",
    "author_association": "NONE",
    "body": "@jreback \n\nMaybe you can see if Im doing something wrong. I did some time tests below to get an idea of what to expect\n\ndf.shape = (235114, 12)\n\nJust using to_csv: ~ 18 seconds\n\n```\nt1 = time.time()\nfor i in range(5):\n    if i == 0:\n        df.to_csv('testdf.txt', sep='\\t')\n    else:\n        df.to_csv('testdf.txt', sep='\\t', header=False, mode='a')\nt2 = time.time()\nt2-t1\n```\n\nUsing to_hdf where we set item_sizes: ~ 115 seconds\n\n```\n# I only really care about indexing columns c5-c8, but will ignore for now\nmax_item_sizes = {\n    'c1': 60,\n    'c2': 10,\n    'c3': 1000,\n    'c4': 1000,\n    'c5': 10,\n    'c6': 10,\n    'c7': 15,\n    'c8': 15,\n    'c9': 25,\n    'c10': 25\n}\nt1 = time.time()\nfor i in range(5):\n    df.to_hdf(\"testhdf.h5\", 'df', min_itemsize=max_item_sizes,  data_columns=None, append=True, format='t')\nt2 = time.time()\nt2-t1\n```\n\nUsing to_hdf where we ignore item_sizes (raises an error if appending columns with different lengths later on): ~ 46 seconds\n\n```\nt1 = time.time()\nfor i in range(5):\n    df.to_hdf(\"testhdf2.h5\", 'df', data_columns=None, append=True, format='t')\nt2 = time.time()\nt2-t1\n```\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/222033943",
    "html_url": "https://github.com/pandas-dev/pandas/issues/13289#issuecomment-222033943",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/13289",
    "id": 222033943,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIyMjAzMzk0Mw==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-05-27T00:50:06Z",
    "updated_at": "2016-05-27T00:50:06Z",
    "author_association": "CONTRIBUTOR",
    "body": "@costas821 you should need to show something that creates a facimile of your data.\n\nSo here's a little sample. You basically are exceeding the limits of PyTables. Storing very wide text data (which you are doing) is not that efficient, nor fast. This is designed really for mostly numbers. That said it will work.\n\n```\nIn [10]: df = DataFrame({ c:''.join(['A'] * v) for c, v in max_item_sizes.items()},index=range(10000))\n\nIn [11]: df.info()\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000 entries, 0 to 9999\nData columns (total 10 columns):\nc1     10000 non-null object\nc10    10000 non-null object\nc2     10000 non-null object\nc3     10000 non-null object\nc4     10000 non-null object\nc5     10000 non-null object\nc6     10000 non-null object\nc7     10000 non-null object\nc8     10000 non-null object\nc9     10000 non-null object\ndtypes: object(10)\nmemory usage: 859.4+ KB\n\nIn [12]: df.head()\nOut[12]: \n                                                  c1                        c10          c2                                                 c3                                                 c4  \\\n0  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...  AAAAAAAAAAAAAAAAAAAAAAAAA  AAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...   \n1  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...  AAAAAAAAAAAAAAAAAAAAAAAAA  AAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...   \n2  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...  AAAAAAAAAAAAAAAAAAAAAAAAA  AAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...   \n3  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...  AAAAAAAAAAAAAAAAAAAAAAAAA  AAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...   \n4  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...  AAAAAAAAAAAAAAAAAAAAAAAAA  AAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...   \n\n           c5          c6               c7               c8                         c9  \n0  AAAAAAAAAA  AAAAAAAAAA  AAAAAAAAAAAAAAA  AAAAAAAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAAAA  \n1  AAAAAAAAAA  AAAAAAAAAA  AAAAAAAAAAAAAAA  AAAAAAAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAAAA  \n2  AAAAAAAAAA  AAAAAAAAAA  AAAAAAAAAAAAAAA  AAAAAAAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAAAA  \n3  AAAAAAAAAA  AAAAAAAAAA  AAAAAAAAAAAAAAA  AAAAAAAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAAAA  \n4  AAAAAAAAAA  AAAAAAAAAA  AAAAAAAAAAAAAAA  AAAAAAAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAAAA  \n\nIn [13]: %timeit df.to_csv('test.csv')\n1 loop, best of 3: 317 ms per loop\n```\n\n```\ndef f():\n    store = pd.HDFStore('test.h5',mode='w')\n\n    for c, mi in max_item_sizes.items():\n        store.append(c,df[c],min_itemsize=mi)\n\n    store.close()\n\nIn [29]: f()\n\nIn [30]: %timeit f()\n1 loop, best of 3: 508 ms per loop\n\n```\n"
  }
]
