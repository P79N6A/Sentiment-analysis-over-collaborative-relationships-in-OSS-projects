[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/457767287",
    "html_url": "https://github.com/pandas-dev/pandas/issues/24938#issuecomment-457767287",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/24938",
    "id": 457767287,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ1Nzc2NzI4Nw==",
    "user": {
      "login": "WillAyd",
      "id": 609873,
      "node_id": "MDQ6VXNlcjYwOTg3Mw==",
      "avatar_url": "https://avatars0.githubusercontent.com/u/609873?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/WillAyd",
      "html_url": "https://github.com/WillAyd",
      "followers_url": "https://api.github.com/users/WillAyd/followers",
      "following_url": "https://api.github.com/users/WillAyd/following{/other_user}",
      "gists_url": "https://api.github.com/users/WillAyd/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/WillAyd/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/WillAyd/subscriptions",
      "organizations_url": "https://api.github.com/users/WillAyd/orgs",
      "repos_url": "https://api.github.com/users/WillAyd/repos",
      "events_url": "https://api.github.com/users/WillAyd/events{/privacy}",
      "received_events_url": "https://api.github.com/users/WillAyd/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-01-25T23:13:58Z",
    "updated_at": "2019-01-25T23:13:58Z",
    "author_association": "MEMBER",
    "body": "What is the cardinality of the datasets? Certainly possible to use a lot of memory if the sets are N to N\r\n\r\nWe typically ask for examples to be self-contained, i.e. not requiring external files. If there's a way to simulate the issue that way would be preferable"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/457772290",
    "html_url": "https://github.com/pandas-dev/pandas/issues/24938#issuecomment-457772290",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/24938",
    "id": 457772290,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ1Nzc3MjI5MA==",
    "user": {
      "login": "pybokeh",
      "id": 3989299,
      "node_id": "MDQ6VXNlcjM5ODkyOTk=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/3989299?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/pybokeh",
      "html_url": "https://github.com/pybokeh",
      "followers_url": "https://api.github.com/users/pybokeh/followers",
      "following_url": "https://api.github.com/users/pybokeh/following{/other_user}",
      "gists_url": "https://api.github.com/users/pybokeh/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/pybokeh/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/pybokeh/subscriptions",
      "organizations_url": "https://api.github.com/users/pybokeh/orgs",
      "repos_url": "https://api.github.com/users/pybokeh/repos",
      "events_url": "https://api.github.com/users/pybokeh/events{/privacy}",
      "received_events_url": "https://api.github.com/users/pybokeh/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-01-25T23:40:40Z",
    "updated_at": "2019-01-25T23:40:40Z",
    "author_association": "NONE",
    "body": "@WillAyd In both csv files, there were only about 80K rows and less than 10 columns and they both took up less than 10MB of memory per df.info() output.  Was running the script in Windows 10.  I ran the script with task manager running to monitor memory usage and noticed that I still have plenty of available memory (over 50%) when the memory error occurred.  So I find it strange to get a memory error.  With that said, I just realized there is a new version of pandas, I've upgraded to 0.24 and I no longer have the problem."
  }
]
