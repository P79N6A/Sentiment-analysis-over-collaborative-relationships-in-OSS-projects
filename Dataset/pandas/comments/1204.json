[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/10015552",
    "html_url": "https://github.com/pandas-dev/pandas/issues/1204#issuecomment-10015552",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/1204",
    "id": 10015552,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEwMDE1NTUy",
    "user": {
      "login": "wesm",
      "id": 329591,
      "node_id": "MDQ6VXNlcjMyOTU5MQ==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/329591?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/wesm",
      "html_url": "https://github.com/wesm",
      "followers_url": "https://api.github.com/users/wesm/followers",
      "following_url": "https://api.github.com/users/wesm/following{/other_user}",
      "gists_url": "https://api.github.com/users/wesm/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/wesm/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wesm/subscriptions",
      "organizations_url": "https://api.github.com/users/wesm/orgs",
      "repos_url": "https://api.github.com/users/wesm/repos",
      "events_url": "https://api.github.com/users/wesm/events{/privacy}",
      "received_events_url": "https://api.github.com/users/wesm/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-11-02T14:14:58Z",
    "updated_at": "2012-11-02T14:14:58Z",
    "author_association": "MEMBER",
    "body": "Thousands handling is handled at a low level in the new parser. Have to think about the comments argument and how to do it without slowing down the tokenizer\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/10784899",
    "html_url": "https://github.com/pandas-dev/pandas/issues/1204#issuecomment-10784899",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/1204",
    "id": 10784899,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEwNzg0ODk5",
    "user": {
      "login": "wesm",
      "id": 329591,
      "node_id": "MDQ6VXNlcjMyOTU5MQ==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/329591?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/wesm",
      "html_url": "https://github.com/wesm",
      "followers_url": "https://api.github.com/users/wesm/followers",
      "following_url": "https://api.github.com/users/wesm/following{/other_user}",
      "gists_url": "https://api.github.com/users/wesm/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/wesm/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/wesm/subscriptions",
      "organizations_url": "https://api.github.com/users/wesm/orgs",
      "repos_url": "https://api.github.com/users/wesm/repos",
      "events_url": "https://api.github.com/users/wesm/events{/privacy}",
      "received_events_url": "https://api.github.com/users/wesm/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-11-28T00:47:41Z",
    "updated_at": "2012-11-28T00:47:41Z",
    "author_association": "MEMBER",
    "body": "Thousands handling is way faster, but the C parser doesn't do comments yet. Should be simple (ish) to add to the tokenizer.\n"
  }
]
