[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/65324679",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-65324679",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 65324679,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1MzI0Njc5",
    "user": {
      "login": "jorisvandenbossche",
      "id": 1020496,
      "node_id": "MDQ6VXNlcjEwMjA0OTY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1020496?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jorisvandenbossche",
      "html_url": "https://github.com/jorisvandenbossche",
      "followers_url": "https://api.github.com/users/jorisvandenbossche/followers",
      "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}",
      "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions",
      "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs",
      "repos_url": "https://api.github.com/users/jorisvandenbossche/repos",
      "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-12-02T23:21:53Z",
    "updated_at": "2014-12-02T23:21:53Z",
    "author_association": "MEMBER",
    "body": "This seems reasonable. Thanks for investigating this!\n\nFor the implementation, it will depend on how sqlalchemy deals with database flavors that does not support this (I can't test this at the moment, but it seems that sqlalchemy raises an error (eg http://stackoverflow.com/questions/23886764/multiple-insert-statements-in-mssql-with-sqlalchemy). Also, if it has the consequence that a lot of people will have to set chunksize, this is indeed not a good idea to do as default (unless we set chunksize to a value by default). \nSo adding a keyword seems maybe better.\n\n@artemyk @mangecoeur @hayd @danielballan\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/65326189",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-65326189",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 65326189,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1MzI2MTg5",
    "user": {
      "login": "artemyk",
      "id": 833768,
      "node_id": "MDQ6VXNlcjgzMzc2OA==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/833768?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/artemyk",
      "html_url": "https://github.com/artemyk",
      "followers_url": "https://api.github.com/users/artemyk/followers",
      "following_url": "https://api.github.com/users/artemyk/following{/other_user}",
      "gists_url": "https://api.github.com/users/artemyk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/artemyk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/artemyk/subscriptions",
      "organizations_url": "https://api.github.com/users/artemyk/orgs",
      "repos_url": "https://api.github.com/users/artemyk/repos",
      "events_url": "https://api.github.com/users/artemyk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/artemyk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-12-02T23:34:57Z",
    "updated_at": "2014-12-02T23:34:57Z",
    "author_association": "CONTRIBUTOR",
    "body": "Apparently SQLAlchemy has a flag `dialect.supports_multivalues_insert` (see e.g. http://pydoc.net/Python/SQLAlchemy/0.8.3/sqlalchemy.sql.compiler/ , possibly called `supports_multirow_insert` in other versions, https://www.mail-archive.com/mediawiki-commits@lists.wikimedia.org/msg202880.html ).\n\nSince this has the potential to speed up inserts a lot, and we can check for support easily, I'm thinking maybe we could do it by default, and also set chunksize to a default value (e.g. 16kb chunks... not sure what's too big in most situations).  If the multirow insert fails, we could throw an exception suggesting lowering the chunksize?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/65416360",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-65416360",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 65416360,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1NDE2MzYw",
    "user": {
      "login": "maxgrenderjones",
      "id": 6059365,
      "node_id": "MDQ6VXNlcjYwNTkzNjU=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/6059365?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/maxgrenderjones",
      "html_url": "https://github.com/maxgrenderjones",
      "followers_url": "https://api.github.com/users/maxgrenderjones/followers",
      "following_url": "https://api.github.com/users/maxgrenderjones/following{/other_user}",
      "gists_url": "https://api.github.com/users/maxgrenderjones/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/maxgrenderjones/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/maxgrenderjones/subscriptions",
      "organizations_url": "https://api.github.com/users/maxgrenderjones/orgs",
      "repos_url": "https://api.github.com/users/maxgrenderjones/repos",
      "events_url": "https://api.github.com/users/maxgrenderjones/events{/privacy}",
      "received_events_url": "https://api.github.com/users/maxgrenderjones/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-12-03T14:36:19Z",
    "updated_at": "2014-12-03T14:36:19Z",
    "author_association": "CONTRIBUTOR",
    "body": "Now I just need to persuade the SQLAlchemy folks to set `supports_multivalues_insert` to true on SQL Server >2005 (I hacked it into the code and it works fine, but it's not on by default).\n\nOn a more on-topic note, I think the chunksize could be tricky. On my mysql setup (which I probably configured to allow large packets), I can set chunksize=5000, on my SQLServer setup, 500 was too large, but 100 worked fine. However, it's probably true that most of the benefits from this technique come from going from inserting 1 row at a time to 100, rather than 100 to 1000.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/65420082",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-65420082",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 65420082,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1NDIwMDgy",
    "user": {
      "login": "danielballan",
      "id": 2279598,
      "node_id": "MDQ6VXNlcjIyNzk1OTg=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/2279598?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/danielballan",
      "html_url": "https://github.com/danielballan",
      "followers_url": "https://api.github.com/users/danielballan/followers",
      "following_url": "https://api.github.com/users/danielballan/following{/other_user}",
      "gists_url": "https://api.github.com/users/danielballan/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/danielballan/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/danielballan/subscriptions",
      "organizations_url": "https://api.github.com/users/danielballan/orgs",
      "repos_url": "https://api.github.com/users/danielballan/repos",
      "events_url": "https://api.github.com/users/danielballan/events{/privacy}",
      "received_events_url": "https://api.github.com/users/danielballan/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-12-03T14:56:48Z",
    "updated_at": "2014-12-03T14:56:48Z",
    "author_association": "CONTRIBUTOR",
    "body": "What if `chunksize=None` meant \"Adaptively choose a chunksize\"? Attempt something like 5000, 500, 50, 1. Users could turn this off by specifying a chunksize. If the overhead from these attempts is too large, I like @maxgrenderjones suggestion: `chunksize=10` is a better default than `chunksize=1`.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/65422214",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-65422214",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 65422214,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1NDIyMjE0",
    "user": {
      "login": "jorisvandenbossche",
      "id": 1020496,
      "node_id": "MDQ6VXNlcjEwMjA0OTY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1020496?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jorisvandenbossche",
      "html_url": "https://github.com/jorisvandenbossche",
      "followers_url": "https://api.github.com/users/jorisvandenbossche/followers",
      "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}",
      "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions",
      "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs",
      "repos_url": "https://api.github.com/users/jorisvandenbossche/repos",
      "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-12-03T15:07:48Z",
    "updated_at": "2014-12-03T15:09:06Z",
    "author_association": "MEMBER",
    "body": "On that last comment \"`chunksize=10` is a better default than `chunksize=1`\" -> that is not fully true I think. The current situation is to do _one_ execute statement that consists of multiline single-row insert statements (which is not a chunksize of 1), while `chunksize=10` would mean doing a lot of execute statements with each time one multi-row insert. \nAnd I don't know if this is necessarily faster, but much depends on the situation. For example with the current code and with a local sqlite database:\n\n```\nIn [4]: engine = create_engine('sqlite:///:memory:') #, echo='debug')\n\nIn [5]: df = pd.DataFrame(np.random.randn(50000, 10))\n\nIn [6]: %timeit df.to_sql('test_default', engine, if_exists='replace')\n1 loops, best of 3: 956 ms per loop\n\nIn [7]: %timeit df.to_sql('test_default', engine, if_exists='replace', chunksize=10)\n1 loops, best of 3: 2.23 s per loop\n```\n\nBut of course this does not use the multi-row feature\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/76139975",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-76139975",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 76139975,
    "node_id": "MDEyOklzc3VlQ29tbWVudDc2MTM5OTc1",
    "user": {
      "login": "nhockham",
      "id": 7823421,
      "node_id": "MDQ6VXNlcjc4MjM0MjE=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/7823421?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/nhockham",
      "html_url": "https://github.com/nhockham",
      "followers_url": "https://api.github.com/users/nhockham/followers",
      "following_url": "https://api.github.com/users/nhockham/following{/other_user}",
      "gists_url": "https://api.github.com/users/nhockham/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/nhockham/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/nhockham/subscriptions",
      "organizations_url": "https://api.github.com/users/nhockham/orgs",
      "repos_url": "https://api.github.com/users/nhockham/repos",
      "events_url": "https://api.github.com/users/nhockham/events{/privacy}",
      "received_events_url": "https://api.github.com/users/nhockham/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-02-26T08:31:02Z",
    "updated_at": "2015-02-26T08:31:02Z",
    "author_association": "NONE",
    "body": "We've figured out how to monkey patch - might be useful to someone else. Have this code before importing pandas.\n\n```\nfrom pandas.io.sql import SQLTable\n\ndef _execute_insert(self, conn, keys, data_iter):\n    print \"Using monkey-patched _execute_insert\"\n    data = [dict((k, v) for k, v in zip(keys, row)) for row in data_iter]\n    conn.execute(self.insert_statement().values(data))\n\nSQLTable._execute_insert = _execute_insert\n\n```\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/76170068",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-76170068",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 76170068,
    "node_id": "MDEyOklzc3VlQ29tbWVudDc2MTcwMDY4",
    "user": {
      "login": "jorisvandenbossche",
      "id": 1020496,
      "node_id": "MDQ6VXNlcjEwMjA0OTY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1020496?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jorisvandenbossche",
      "html_url": "https://github.com/jorisvandenbossche",
      "followers_url": "https://api.github.com/users/jorisvandenbossche/followers",
      "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}",
      "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions",
      "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs",
      "repos_url": "https://api.github.com/users/jorisvandenbossche/repos",
      "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-02-26T12:29:58Z",
    "updated_at": "2015-02-26T12:29:58Z",
    "author_association": "MEMBER",
    "body": "Maybe we can just start with adding this feature through a new `multirow=True` keyword (with a default of False for now), and then we can later always see if we can enable it by default?\n\n@maxgrenderjones @nhockham interested to do a PR to add this?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/76172659",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-76172659",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 76172659,
    "node_id": "MDEyOklzc3VlQ29tbWVudDc2MTcyNjU5",
    "user": {
      "login": "mangecoeur",
      "id": 743508,
      "node_id": "MDQ6VXNlcjc0MzUwOA==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/743508?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/mangecoeur",
      "html_url": "https://github.com/mangecoeur",
      "followers_url": "https://api.github.com/users/mangecoeur/followers",
      "following_url": "https://api.github.com/users/mangecoeur/following{/other_user}",
      "gists_url": "https://api.github.com/users/mangecoeur/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/mangecoeur/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/mangecoeur/subscriptions",
      "organizations_url": "https://api.github.com/users/mangecoeur/orgs",
      "repos_url": "https://api.github.com/users/mangecoeur/repos",
      "events_url": "https://api.github.com/users/mangecoeur/events{/privacy}",
      "received_events_url": "https://api.github.com/users/mangecoeur/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-02-26T12:50:25Z",
    "updated_at": "2015-02-26T12:50:25Z",
    "author_association": "CONTRIBUTOR",
    "body": "@jorisvandenbossche I think it's risky to start adding keyword arguments to address specific performance profiles. If you can guarantee that it's faster in all cases (if necessary by having it determine the best method based on the inputs) then you don't need a flag at all.\n\nDifferent DB-setups may have different performance optimizations (different DB perf profiles, local vs network, big memory vs fast SSD, etc, etc), if you start adding keyword flags for each it becomes a mess.\n\nI would suggest creating subclasses of SQLDatabase and SQLTable to address performance specific implementations, they would be used through the object-oriented API. Perhaps a \"backend switching\" method could be added but frankly using the OO api is very simple so this is probably overkill for what is already a specialized use-case.\n\nI created such a sub-class for loading large datasets to Postgres (it's actually much faster to save data to CSV then use the built-in non-standard COPY FROM sql commands than to use inserts, see https://gist.github.com/mangecoeur/1fbd63d4758c2ba0c470#file-pandas_postgres-py). To use it you just do `PgSQLDatabase(engine, <args>).to_sql(frame, name,<kwargs>)`\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/76220606",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-76220606",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 76220606,
    "node_id": "MDEyOklzc3VlQ29tbWVudDc2MjIwNjA2",
    "user": {
      "login": "artemyk",
      "id": 833768,
      "node_id": "MDQ6VXNlcjgzMzc2OA==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/833768?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/artemyk",
      "html_url": "https://github.com/artemyk",
      "followers_url": "https://api.github.com/users/artemyk/followers",
      "following_url": "https://api.github.com/users/artemyk/following{/other_user}",
      "gists_url": "https://api.github.com/users/artemyk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/artemyk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/artemyk/subscriptions",
      "organizations_url": "https://api.github.com/users/artemyk/orgs",
      "repos_url": "https://api.github.com/users/artemyk/repos",
      "events_url": "https://api.github.com/users/artemyk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/artemyk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-02-26T17:13:17Z",
    "updated_at": "2015-02-26T17:13:17Z",
    "author_association": "CONTRIBUTOR",
    "body": "Just for reference,  I tried running the code by @jorisvandenbossche (Dec 3rd post) using the multirow feature.  It's quite a bit slower.  So the speed-tradeoffs here is not trivial:\n\n```\nIn [4]: engine = create_engine('sqlite:///:memory:') #, echo='debug')\n\nIn [5]: df = pd.DataFrame(np.random.randn(50000, 10))\n\nIn [6]: \n\nIn [6]: %timeit df.to_sql('test_default', engine, if_exists='replace')\n1 loops, best of 3: 1.05 s per loop\n\nIn [7]: \n\nIn [7]: from pandas.io.sql import SQLTable\n\nIn [8]: \n\nIn [8]: def _execute_insert(self, conn, keys, data_iter):\n   ...:         data = [dict((k, v) for k, v in zip(keys, row)) for row in data_iter]\n   ...:         conn.execute(self.insert_statement().values(data))\n   ...:     \n\nIn [9]: SQLTable._execute_insert = _execute_insert\n\nIn [10]: \n\nIn [10]: reload(pd)\nOut[10]: <module 'pandas' from '/usr/local/lib/python2.7/site-packages/pandas/__init__.pyc'>\n\nIn [11]: \n\nIn [11]: %timeit df.to_sql('test_default', engine, if_exists='replace', chunksize=10)\n1 loops, best of 3: 9.9 s per loop\n```\n\nAlso, I agree that adding keyword parameters is risky.  However, the multirow feature seems pretty fundamental.  Also, 'monkey-patching' is probably not more robust to API changes than keyword parameters.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/76224063",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-76224063",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 76224063,
    "node_id": "MDEyOklzc3VlQ29tbWVudDc2MjI0MDYz",
    "user": {
      "login": "mangecoeur",
      "id": 743508,
      "node_id": "MDQ6VXNlcjc0MzUwOA==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/743508?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/mangecoeur",
      "html_url": "https://github.com/mangecoeur",
      "followers_url": "https://api.github.com/users/mangecoeur/followers",
      "following_url": "https://api.github.com/users/mangecoeur/following{/other_user}",
      "gists_url": "https://api.github.com/users/mangecoeur/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/mangecoeur/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/mangecoeur/subscriptions",
      "organizations_url": "https://api.github.com/users/mangecoeur/orgs",
      "repos_url": "https://api.github.com/users/mangecoeur/repos",
      "events_url": "https://api.github.com/users/mangecoeur/events{/privacy}",
      "received_events_url": "https://api.github.com/users/mangecoeur/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-02-26T17:31:22Z",
    "updated_at": "2015-02-26T17:31:22Z",
    "author_association": "CONTRIBUTOR",
    "body": "Its as i suspected. Monkey patching isn't the solution I was suggesting - rather that we ship a number of performance oriented subclasses that the informed user could use through the OO interface (to avoid loading the functional api with too many options)\n\n-----Original Message-----\nFrom: \"Artemy Kolchinsky\" notifications@github.com\nSent: ‎26/‎02/‎2015 17:13\nTo: \"pydata/pandas\" pandas@noreply.github.com\nCc: \"mangecoeur\" jon.chambers3001@gmail.com\nSubject: Re: [pandas] Use multi-row inserts for massive speedups on to_sqlover high latency connections (#8953)\n\nJust for reference, I tried running the code by @jorisvandenbossche (Dec 3rd post) using the multirow feature. It's quite a bit slower. So the speed-tradeoffs here is not trivial:\nIn [4]: engine = create_engine('sqlite:///:memory:') #, echo='debug')\n\nIn [5]: df = pd.DataFrame(np.random.randn(50000, 10))\n\nIn [6]: \n\nIn [6]: %timeit df.to_sql('test_default', engine, if_exists='replace')\n1 loops, best of 3: 1.05 s per loop\n\nIn [7]: \n\nIn [7]: from pandas.io.sql import SQLTable\n\nIn [8]: \n\nIn [8]: def _execute_insert(self, conn, keys, data_iter):\n   ...:         data = [dict((k, v) for k, v in zip(keys, row)) for row in data_iter]\n   ...:         conn.execute(self.insert_statement().values(data))\n   ...:     \n\nIn [9]: SQLTable._execute_insert = _execute_insert\n\nIn [10]: \n\nIn [10]: reload(pd)\nOut[10]: <module 'pandas' from '/usr/local/lib/python2.7/site-packages/pandas/__init__.pyc'>\n\nIn [11]: \n\nIn [11]: %timeit df.to_sql('test_default', engine, if_exists='replace', chunksize=10)\n1 loops, best of 3: 9.9 s per loop\nAlso, I agree that adding keyword parameters is risky. However, the multirow feature seems pretty fundamental. Also, 'monkey-patching' is probably not more robust to API changes than keyword parameters.\n—\nReply to this email directly or view it on GitHub.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/81757741",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-81757741",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 81757741,
    "node_id": "MDEyOklzc3VlQ29tbWVudDgxNzU3NzQx",
    "user": {
      "login": "maxgrenderjones",
      "id": 6059365,
      "node_id": "MDQ6VXNlcjYwNTkzNjU=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/6059365?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/maxgrenderjones",
      "html_url": "https://github.com/maxgrenderjones",
      "followers_url": "https://api.github.com/users/maxgrenderjones/followers",
      "following_url": "https://api.github.com/users/maxgrenderjones/following{/other_user}",
      "gists_url": "https://api.github.com/users/maxgrenderjones/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/maxgrenderjones/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/maxgrenderjones/subscriptions",
      "organizations_url": "https://api.github.com/users/maxgrenderjones/orgs",
      "repos_url": "https://api.github.com/users/maxgrenderjones/repos",
      "events_url": "https://api.github.com/users/maxgrenderjones/events{/privacy}",
      "received_events_url": "https://api.github.com/users/maxgrenderjones/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2015-03-16T15:55:25Z",
    "updated_at": "2015-03-16T15:55:25Z",
    "author_association": "CONTRIBUTOR",
    "body": "As per the initial ticket title, I don't think this approach is going to be preferable in all cases, so I wouldn't make it the default. However, without it, the pandas `to_sql` unusable for me, so it's important enough for me to continue to request the change. (It's also become the first thing I change when I upgrade my pandas version). As for sensible `chunksize` values, I don't think there is one true `n`, as the packet size will depend on how many columns there are (and what's in them) in hard to predict ways. Unfortunately SQLServer fails with an error message that looks totally unrelated (but isn't) if you set the `chunksize` too high (which is probably why multirow inserts aren't turned on except with a patch in SQLAlchemy), but it works fine with `mysql`. Users may need to experiment to determine what value of `n` is likely to result in an acceptably large packet size (for whatever their backing database is). Having pandas chose `n` is likely to get land us way further down in the implementation details than we want to be (i.e. the opposite direction from the maximum-possible-abstraction SQLALchemy approach)\n\nIn short, my recommendation would be to add it as a keyword, with some helpful commentary about how to use it. This wouldn't be the first time a keyword was used to select an implementation (see: http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html) but that perhaps isn't the best example, as I haven't the first idea about what `raw=` means, even having read the explanation!\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/234172638",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-234172638",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 234172638,
    "node_id": "MDEyOklzc3VlQ29tbWVudDIzNDE3MjYzOA==",
    "user": {
      "login": "dragonator4",
      "id": 8196363,
      "node_id": "MDQ6VXNlcjgxOTYzNjM=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/8196363?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dragonator4",
      "html_url": "https://github.com/dragonator4",
      "followers_url": "https://api.github.com/users/dragonator4/followers",
      "following_url": "https://api.github.com/users/dragonator4/following{/other_user}",
      "gists_url": "https://api.github.com/users/dragonator4/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dragonator4/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dragonator4/subscriptions",
      "organizations_url": "https://api.github.com/users/dragonator4/orgs",
      "repos_url": "https://api.github.com/users/dragonator4/repos",
      "events_url": "https://api.github.com/users/dragonator4/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dragonator4/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-07-21T06:57:29Z",
    "updated_at": "2016-07-21T06:57:29Z",
    "author_association": "NONE",
    "body": "I have noticed that it also consumes a huge amount of memory. Like a 1.6+ GB DataFrame with some 700,000 rows and 301 columns requires almost 34 GB during insert! That is like over the top inefficient. Any ideas on why that might be the case? Here is a screen clip:\n\n![image](https://cloud.githubusercontent.com/assets/8196363/17013812/02b51276-4ede-11e6-8006-9fd32cb01e14.png)\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/254753342",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-254753342",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 254753342,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI1NDc1MzM0Mg==",
    "user": {
      "login": "andreacassioli",
      "id": 6704112,
      "node_id": "MDQ6VXNlcjY3MDQxMTI=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/6704112?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/andreacassioli",
      "html_url": "https://github.com/andreacassioli",
      "followers_url": "https://api.github.com/users/andreacassioli/followers",
      "following_url": "https://api.github.com/users/andreacassioli/following{/other_user}",
      "gists_url": "https://api.github.com/users/andreacassioli/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/andreacassioli/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/andreacassioli/subscriptions",
      "organizations_url": "https://api.github.com/users/andreacassioli/orgs",
      "repos_url": "https://api.github.com/users/andreacassioli/repos",
      "events_url": "https://api.github.com/users/andreacassioli/events{/privacy}",
      "received_events_url": "https://api.github.com/users/andreacassioli/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-10-19T08:52:29Z",
    "updated_at": "2016-10-19T08:52:29Z",
    "author_association": "NONE",
    "body": "Hi guys,\nany progress on this issue?\n\nI am try to insert around 200K rows using to_sql but it takes forever and consume a huge amount of memory! Using chuncksize helps with the memory but still the speed is very slow. \n\nMy impression, looking at the MSSQL DBase trace is that the insertion is actually performed one row at the time. \n\nThe only viable approach now is to dump to a csv file on a shared folder and use BULK INSERT. But it very annoying and inelegant!\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/255148529",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-255148529",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 255148529,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI1NTE0ODUyOQ==",
    "user": {
      "login": "ostrokach",
      "id": 5614375,
      "node_id": "MDQ6VXNlcjU2MTQzNzU=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/5614375?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ostrokach",
      "html_url": "https://github.com/ostrokach",
      "followers_url": "https://api.github.com/users/ostrokach/followers",
      "following_url": "https://api.github.com/users/ostrokach/following{/other_user}",
      "gists_url": "https://api.github.com/users/ostrokach/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ostrokach/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ostrokach/subscriptions",
      "organizations_url": "https://api.github.com/users/ostrokach/orgs",
      "repos_url": "https://api.github.com/users/ostrokach/repos",
      "events_url": "https://api.github.com/users/ostrokach/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ostrokach/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-10-20T15:57:37Z",
    "updated_at": "2016-10-20T15:58:33Z",
    "author_association": "NONE",
    "body": "@andreacassioli You can use [odo](http://odo.pydata.org/) to insert a DataFrame into an SQL database through an intermediary CSV file. See [Loading CSVs into SQL Databases](http://odo.pydata.org/en/latest/perf.html). \n\nI don't think you can come even close to `BULK INSERT` performance using ODBC.\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/255212602",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-255212602",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 255212602,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI1NTIxMjYwMg==",
    "user": {
      "login": "andreacassioli",
      "id": 6704112,
      "node_id": "MDQ6VXNlcjY3MDQxMTI=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/6704112?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/andreacassioli",
      "html_url": "https://github.com/andreacassioli",
      "followers_url": "https://api.github.com/users/andreacassioli/followers",
      "following_url": "https://api.github.com/users/andreacassioli/following{/other_user}",
      "gists_url": "https://api.github.com/users/andreacassioli/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/andreacassioli/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/andreacassioli/subscriptions",
      "organizations_url": "https://api.github.com/users/andreacassioli/orgs",
      "repos_url": "https://api.github.com/users/andreacassioli/repos",
      "events_url": "https://api.github.com/users/andreacassioli/events{/privacy}",
      "received_events_url": "https://api.github.com/users/andreacassioli/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-10-20T20:03:35Z",
    "updated_at": "2016-10-20T20:03:35Z",
    "author_association": "NONE",
    "body": "@ostrokach thank you, indeed I am using csv files now. If I could get close, I would trade a bit of time for simplicity!\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/283852531",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-283852531",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 283852531,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI4Mzg1MjUzMQ==",
    "user": {
      "login": "indera",
      "id": 1429741,
      "node_id": "MDQ6VXNlcjE0Mjk3NDE=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/1429741?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/indera",
      "html_url": "https://github.com/indera",
      "followers_url": "https://api.github.com/users/indera/followers",
      "following_url": "https://api.github.com/users/indera/following{/other_user}",
      "gists_url": "https://api.github.com/users/indera/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/indera/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/indera/subscriptions",
      "organizations_url": "https://api.github.com/users/indera/orgs",
      "repos_url": "https://api.github.com/users/indera/repos",
      "events_url": "https://api.github.com/users/indera/events{/privacy}",
      "received_events_url": "https://api.github.com/users/indera/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-03-03T03:05:07Z",
    "updated_at": "2017-03-03T03:05:07Z",
    "author_association": "NONE",
    "body": "I thought this might help somebody:\r\nhttp://docs.sqlalchemy.org/en/latest/faq/performance.html#i-m-inserting-400-000-rows-with-the-orm-and-it-s-really-slow\r\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/283909919",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-283909919",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 283909919,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI4MzkwOTkxOQ==",
    "user": {
      "login": "jorisvandenbossche",
      "id": 1020496,
      "node_id": "MDQ6VXNlcjEwMjA0OTY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1020496?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jorisvandenbossche",
      "html_url": "https://github.com/jorisvandenbossche",
      "followers_url": "https://api.github.com/users/jorisvandenbossche/followers",
      "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}",
      "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions",
      "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs",
      "repos_url": "https://api.github.com/users/jorisvandenbossche/repos",
      "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-03-03T09:41:26Z",
    "updated_at": "2017-03-03T09:41:26Z",
    "author_association": "MEMBER",
    "body": "@indera pandas does not use the ORM, only sqlalchemy Core (which is what the doc entry there suggests to use for large inserts)"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/284425244",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-284425244",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 284425244,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDQyNTI0NA==",
    "user": {
      "login": "russlamb",
      "id": 19935045,
      "node_id": "MDQ6VXNlcjE5OTM1MDQ1",
      "avatar_url": "https://avatars3.githubusercontent.com/u/19935045?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/russlamb",
      "html_url": "https://github.com/russlamb",
      "followers_url": "https://api.github.com/users/russlamb/followers",
      "following_url": "https://api.github.com/users/russlamb/following{/other_user}",
      "gists_url": "https://api.github.com/users/russlamb/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/russlamb/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/russlamb/subscriptions",
      "organizations_url": "https://api.github.com/users/russlamb/orgs",
      "repos_url": "https://api.github.com/users/russlamb/repos",
      "events_url": "https://api.github.com/users/russlamb/events{/privacy}",
      "received_events_url": "https://api.github.com/users/russlamb/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-03-06T15:17:29Z",
    "updated_at": "2017-03-06T15:17:29Z",
    "author_association": "NONE",
    "body": "is there any consensus on how to work around this in the meantime?  I'm inserting a several million rows into postgres and it takes forever.  Is CSV / odo the way to go?"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/284430137",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-284430137",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 284430137,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDQzMDEzNw==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-03-06T15:31:50Z",
    "updated_at": "2017-03-06T15:31:50Z",
    "author_association": "CONTRIBUTOR",
    "body": "@russlamb a practical way to solve this problem is simply to bulk upload. This is someone db specific though, so ``odo`` has solutions for ``postgresl`` (and may be ``mysql``) I think. for something like sqlserver you have to 'do this yourself' (IOW you have to write it). "
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/284432086",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-284432086",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 284432086,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDQzMjA4Ng==",
    "user": {
      "login": "indera",
      "id": 1429741,
      "node_id": "MDQ6VXNlcjE0Mjk3NDE=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/1429741?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/indera",
      "html_url": "https://github.com/indera",
      "followers_url": "https://api.github.com/users/indera/followers",
      "following_url": "https://api.github.com/users/indera/following{/other_user}",
      "gists_url": "https://api.github.com/users/indera/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/indera/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/indera/subscriptions",
      "organizations_url": "https://api.github.com/users/indera/orgs",
      "repos_url": "https://api.github.com/users/indera/repos",
      "events_url": "https://api.github.com/users/indera/events{/privacy}",
      "received_events_url": "https://api.github.com/users/indera/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-03-06T15:37:37Z",
    "updated_at": "2017-03-06T15:55:36Z",
    "author_association": "NONE",
    "body": "For sqlserver I used the FreeTDS driver (http://www.freetds.org/software.html and https://github.com/mkleehammer/pyodbc ) with SQLAlchemy entities which resulted in very fast inserts (20K rows per data frame):\r\n\r\n```\r\nfrom sqlalchemy.ext.declarative import declarative_base\r\nBase = declarative_base()\r\n\r\n\r\nclass DemographicEntity(Base):\r\n    __tablename__ = 'DEMOGRAPHIC'\r\n\r\n    patid = db.Column(\"PATID\", db.Text, primary_key=True)\r\n    \"\"\"\r\n    patid = db.Column(\"PATID\", db.Text, primary_key=True, autoincrement=False, nullable=True)\r\n    birth_date = db.Column(\"BIRTH_DATE\", db.Date)\r\n    birth_time = db.Column(\"BIRTH_TIME\", db.Text(5))\r\n    sex = db.Column(\"SEX\", db.Text(2))\r\n\r\ndef get_db_url(db_host, db_port, db_name, db_user, db_pass):\r\n    params = parse.quote(\r\n        \"Driver={{FreeTDS}};Server={};Port={};\"\r\n        \"Database={};UID={};PWD={};\"\r\n        .format(db_host, db_port, db_name, db_user, db_pass))\r\n    return 'mssql+pyodbc:///?odbc_connect={}'.format(params)\r\n\r\ndef get_db_pool():\r\n    \"\"\"\r\n    Create the database engine connection.\r\n    @see http://docs.sqlalchemy.org/en/latest/core/engines.html\r\n\r\n    :return: Dialect object which can either be used directly\r\n            to interact with the database, or can be passed to\r\n            a Session object to work with the ORM.\r\n    \"\"\"\r\n    global DB_POOL\r\n\r\n    if DB_POOL is None:\r\n        url = get_db_url(db_host=DB_HOST, db_port=DB_PORT, db_name=DB_NAME,\r\n                         db_user=DB_USER, db_pass=DB_PASS)\r\n        DB_POOL = db.create_engine(url,\r\n                                   pool_size=10,\r\n                                   max_overflow=5,\r\n                                   pool_recycle=3600)\r\n\r\n    try:\r\n        DB_POOL.execute(\"USE {db}\".format(db=DB_NAME))\r\n    except db.exc.OperationalError:\r\n        logger.error('Database {db} does not exist.'.format(db=DB_NAME))\r\n\r\n    return DB_POOL\r\n\r\n\r\ndef save_frame():\r\n    db_pool = get_db_pool()\r\n    records = df.to_dict(orient='records')\r\n    result = db_pool.execute(entity.__table__.insert(), records)\r\n \r\n```"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/284433844",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-284433844",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 284433844,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDQzMzg0NA==",
    "user": {
      "login": "jorisvandenbossche",
      "id": 1020496,
      "node_id": "MDQ6VXNlcjEwMjA0OTY=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/1020496?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jorisvandenbossche",
      "html_url": "https://github.com/jorisvandenbossche",
      "followers_url": "https://api.github.com/users/jorisvandenbossche/followers",
      "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}",
      "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions",
      "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs",
      "repos_url": "https://api.github.com/users/jorisvandenbossche/repos",
      "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-03-06T15:42:43Z",
    "updated_at": "2017-03-06T15:42:43Z",
    "author_association": "MEMBER",
    "body": ">  Is CSV / odo the way to go?\r\n\r\nThis solution will almost always be faster I think, regardless of the multi-row / chunksize settings. \r\n\r\nBut, @russlamb, it is always interesting to hear whether such a multi-row keyword would be an improvement in your case. See eg https://github.com/pandas-dev/pandas/issues/8953#issuecomment-76139975 on a way to easily test this out.\r\n\r\nI think there is agreement that we want to have a way to specify this (without necessarily changing the default). So if somebody wants to make a PR for this, that is certainly welcome. \r\nThere was only some discussion on how to add this ability (new keyword vs subclass using OO api)."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/284437587",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-284437587",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 284437587,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDQzNzU4Nw==",
    "user": {
      "login": "indera",
      "id": 1429741,
      "node_id": "MDQ6VXNlcjE0Mjk3NDE=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/1429741?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/indera",
      "html_url": "https://github.com/indera",
      "followers_url": "https://api.github.com/users/indera/followers",
      "following_url": "https://api.github.com/users/indera/following{/other_user}",
      "gists_url": "https://api.github.com/users/indera/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/indera/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/indera/subscriptions",
      "organizations_url": "https://api.github.com/users/indera/orgs",
      "repos_url": "https://api.github.com/users/indera/repos",
      "events_url": "https://api.github.com/users/indera/events{/privacy}",
      "received_events_url": "https://api.github.com/users/indera/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-03-06T15:53:28Z",
    "updated_at": "2017-06-01T20:48:57Z",
    "author_association": "NONE",
    "body": "@jorisvandenbossche The document I linked above mentions \"Alternatively, the SQLAlchemy ORM offers the Bulk Operations suite of methods, which provide hooks into subsections of the unit of work process in order to emit Core-level INSERT and UPDATE constructs with a small degree of ORM-based automation.\"\r\n\r\nWhat I am suggesting is to implement a sqlserver specific version for `to_sql` which under the hood uses the SQLAlchemy ORMs for speedups as in the code I posted above."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/284618933",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-284618933",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 284618933,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDYxODkzMw==",
    "user": {
      "login": "mangecoeur",
      "id": 743508,
      "node_id": "MDQ6VXNlcjc0MzUwOA==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/743508?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/mangecoeur",
      "html_url": "https://github.com/mangecoeur",
      "followers_url": "https://api.github.com/users/mangecoeur/followers",
      "following_url": "https://api.github.com/users/mangecoeur/following{/other_user}",
      "gists_url": "https://api.github.com/users/mangecoeur/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/mangecoeur/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/mangecoeur/subscriptions",
      "organizations_url": "https://api.github.com/users/mangecoeur/orgs",
      "repos_url": "https://api.github.com/users/mangecoeur/repos",
      "events_url": "https://api.github.com/users/mangecoeur/events{/privacy}",
      "received_events_url": "https://api.github.com/users/mangecoeur/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-03-07T04:31:10Z",
    "updated_at": "2017-03-07T04:31:10Z",
    "author_association": "CONTRIBUTOR",
    "body": "This was proposed before. The way you go is to implement an pandas sql\nclass optimised for a backend. I posted a gist in the past for using\npostgres COPY FROM command which is much faster. However something similar\nis now available in odo, and built in a more robust way. There isn't much\npoint IMHO in duplicating work from odo.\n\n\n\nOn 7 Mar 2017 00:53, \"Andrei Sura\" <notifications@github.com> wrote:\n\n> @jorisvandenbossche <https://github.com/jorisvandenbossche> The document\n> I linked above mentions \"Alternatively, the SQLAlchemy ORM offers the Bulk\n> Operations suite of methods, which provide hooks into subsections of the\n> unit of work process in order to emit Core-level INSERT and UPDATE\n> constructs with a small degree of ORM-based automation.\"\n>\n> What I am suggesting is to implement a sqlserver specific version for\n> \"to_sql\" which under the hood uses the SQLAlchemy core for speedups.\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/8953#issuecomment-284437587>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAtYVDXKLuTlsh9ycpMQvU5C0hs_RxuYks5rjCwBgaJpZM4DCjLh>\n> .\n>\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/284619235",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-284619235",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 284619235,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDYxOTIzNQ==",
    "user": {
      "login": "mangecoeur",
      "id": 743508,
      "node_id": "MDQ6VXNlcjc0MzUwOA==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/743508?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/mangecoeur",
      "html_url": "https://github.com/mangecoeur",
      "followers_url": "https://api.github.com/users/mangecoeur/followers",
      "following_url": "https://api.github.com/users/mangecoeur/following{/other_user}",
      "gists_url": "https://api.github.com/users/mangecoeur/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/mangecoeur/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/mangecoeur/subscriptions",
      "organizations_url": "https://api.github.com/users/mangecoeur/orgs",
      "repos_url": "https://api.github.com/users/mangecoeur/repos",
      "events_url": "https://api.github.com/users/mangecoeur/events{/privacy}",
      "received_events_url": "https://api.github.com/users/mangecoeur/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-03-07T04:33:45Z",
    "updated_at": "2017-03-07T04:33:45Z",
    "author_association": "CONTRIBUTOR",
    "body": "Also noticed you mentioned sqlalchemy could core instead. Unless something\nhas changed a lot, only sqlalchemy core is used in any case, no orm. If you\nwant to speed up more than using core you have to go to lower level, db\nspecific optimisation\n\nOn 7 Mar 2017 00:53, \"Andrei Sura\" <notifications@github.com> wrote:\n\n> @jorisvandenbossche <https://github.com/jorisvandenbossche> The document\n> I linked above mentions \"Alternatively, the SQLAlchemy ORM offers the Bulk\n> Operations suite of methods, which provide hooks into subsections of the\n> unit of work process in order to emit Core-level INSERT and UPDATE\n> constructs with a small degree of ORM-based automation.\"\n>\n> What I am suggesting is to implement a sqlserver specific version for\n> \"to_sql\" which under the hood uses the SQLAlchemy core for speedups.\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pandas-dev/pandas/issues/8953#issuecomment-284437587>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAtYVDXKLuTlsh9ycpMQvU5C0hs_RxuYks5rjCwBgaJpZM4DCjLh>\n> .\n>\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/305615627",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-305615627",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 305615627,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTYxNTYyNw==",
    "user": {
      "login": "dfernan",
      "id": 1617067,
      "node_id": "MDQ6VXNlcjE2MTcwNjc=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/1617067?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dfernan",
      "html_url": "https://github.com/dfernan",
      "followers_url": "https://api.github.com/users/dfernan/followers",
      "following_url": "https://api.github.com/users/dfernan/following{/other_user}",
      "gists_url": "https://api.github.com/users/dfernan/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dfernan/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dfernan/subscriptions",
      "organizations_url": "https://api.github.com/users/dfernan/orgs",
      "repos_url": "https://api.github.com/users/dfernan/repos",
      "events_url": "https://api.github.com/users/dfernan/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dfernan/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-01T20:47:29Z",
    "updated_at": "2017-06-01T20:47:29Z",
    "author_association": "NONE",
    "body": "Is this getting fixed/taken care of? As of now inserting pandas dataframes into a SQL db is extremely slow unless it's a toy dataframe.  Let's decide on a solution and push it forward?"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/306073041",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-306073041",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 306073041,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjA3MzA0MQ==",
    "user": {
      "login": "ostrokach",
      "id": 5614375,
      "node_id": "MDQ6VXNlcjU2MTQzNzU=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/5614375?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ostrokach",
      "html_url": "https://github.com/ostrokach",
      "followers_url": "https://api.github.com/users/ostrokach/followers",
      "following_url": "https://api.github.com/users/ostrokach/following{/other_user}",
      "gists_url": "https://api.github.com/users/ostrokach/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ostrokach/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ostrokach/subscriptions",
      "organizations_url": "https://api.github.com/users/ostrokach/orgs",
      "repos_url": "https://api.github.com/users/ostrokach/repos",
      "events_url": "https://api.github.com/users/ostrokach/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ostrokach/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-06-04T22:57:09Z",
    "updated_at": "2017-06-04T22:57:09Z",
    "author_association": "NONE",
    "body": "@dfernan As mentioned above, you may want to look at the [odo](http://odo.pydata.org/en/latest/perf.html). Using an intermediary CSV file will always be orders of magnitude faster that going through sqlalchemy, no matter what kind of improvements happen here..."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/330951534",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-330951534",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 330951534,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDk1MTUzNA==",
    "user": {
      "login": "markschwarz",
      "id": 2159573,
      "node_id": "MDQ6VXNlcjIxNTk1NzM=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/2159573?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/markschwarz",
      "html_url": "https://github.com/markschwarz",
      "followers_url": "https://api.github.com/users/markschwarz/followers",
      "following_url": "https://api.github.com/users/markschwarz/following{/other_user}",
      "gists_url": "https://api.github.com/users/markschwarz/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/markschwarz/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/markschwarz/subscriptions",
      "organizations_url": "https://api.github.com/users/markschwarz/orgs",
      "repos_url": "https://api.github.com/users/markschwarz/repos",
      "events_url": "https://api.github.com/users/markschwarz/events{/privacy}",
      "received_events_url": "https://api.github.com/users/markschwarz/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-09-20T19:10:27Z",
    "updated_at": "2017-09-20T19:16:18Z",
    "author_association": "NONE",
    "body": "@ostrokach, I'm unconvinced that odo's behavior is what the typical Pandas user wants.  Multi-row inserts over ODBC are probably fast enough for most analysts.\r\n\r\nTo speak for myself, I just spent a few hours switching from the monkey patch above to odo.  Plain pandas runtime was 10+ hours, RBAR.  The monkey patch runs in 2 on the same data set. \r\n The odo/CSV route was faster, as expected, but not by enough to make it worth the effort.  I fiddled with CSV conversion issues I didn't care much about, all in the name of avoiding the monkey patch.   I'm importing 250K rows from ~10 mysql and PG DBs into a common area in Postgres, for NLP analysis.  \r\n\r\nI'm intimately familiar with the bulk loading approaches odo espouses.  I've used them for years where I'm starting with CSV data.  There are key limitations to them:\r\n\r\n1. For the df->CSV->Postgres case, shell access and a scp step is needed to get the CSV on the PG host.  It looks like @mangecoeur has gotten around this with a stream to STDIN.\r\n2. For my purpose (250K rows of comments, with lots of special cases in the text content) I struggled to get CSV parameters right.  I didn't want performance gains badly enough to keep investing in this.\r\n\r\nI switch back to the the patch, so I could get on to analysis work.  \r\n\r\nI agree with @jorisvandenbossche, @maxgrenderjones.  An option (not a default) to choose this would be immensely useful.  @artemyk's point about dialect.supports_multivalues_insert might even make this a reasonable default.\r\n\r\nI'm glad to submit a PR if that'd move this forward."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/331063273",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-331063273",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 331063273,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTA2MzI3Mw==",
    "user": {
      "login": "s-trooper",
      "id": 2750085,
      "node_id": "MDQ6VXNlcjI3NTAwODU=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/2750085?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/s-trooper",
      "html_url": "https://github.com/s-trooper",
      "followers_url": "https://api.github.com/users/s-trooper/followers",
      "following_url": "https://api.github.com/users/s-trooper/following{/other_user}",
      "gists_url": "https://api.github.com/users/s-trooper/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/s-trooper/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/s-trooper/subscriptions",
      "organizations_url": "https://api.github.com/users/s-trooper/orgs",
      "repos_url": "https://api.github.com/users/s-trooper/repos",
      "events_url": "https://api.github.com/users/s-trooper/events{/privacy}",
      "received_events_url": "https://api.github.com/users/s-trooper/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-09-21T06:24:06Z",
    "updated_at": "2017-09-21T06:24:06Z",
    "author_association": "NONE",
    "body": "just to add my experience with odo, it has not worked for MS Sql bulk inserts because of known issue with encoding.  imho m-row insert are good practical solution for most ppl. "
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/331452484",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-331452484",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 331452484,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTQ1MjQ4NA==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-09-22T13:51:11Z",
    "updated_at": "2017-09-22T13:51:11Z",
    "author_association": "CONTRIBUTOR",
    "body": "@markschwarz an option to enable this to work faster would be very welcome!"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/332724278",
    "html_url": "https://github.com/pandas-dev/pandas/issues/8953#issuecomment-332724278",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/8953",
    "id": 332724278,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMzMjcyNDI3OA==",
    "user": {
      "login": "makmanalp",
      "id": 161965,
      "node_id": "MDQ6VXNlcjE2MTk2NQ==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/161965?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/makmanalp",
      "html_url": "https://github.com/makmanalp",
      "followers_url": "https://api.github.com/users/makmanalp/followers",
      "following_url": "https://api.github.com/users/makmanalp/following{/other_user}",
      "gists_url": "https://api.github.com/users/makmanalp/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/makmanalp/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/makmanalp/subscriptions",
      "organizations_url": "https://api.github.com/users/makmanalp/orgs",
      "repos_url": "https://api.github.com/users/makmanalp/repos",
      "events_url": "https://api.github.com/users/makmanalp/events{/privacy}",
      "received_events_url": "https://api.github.com/users/makmanalp/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-09-28T04:25:53Z",
    "updated_at": "2017-09-28T04:25:53Z",
    "author_association": "CONTRIBUTOR",
    "body": "Tracing the queries using sqlite, I do seem to be getting in multi-inserts when using `chunksize`:\r\n\r\n```\r\n2017-09-28 00:21:39,007 INFO sqlalchemy.engine.base.Engine INSERT INTO country_hsproduct_year (location_id, product_id, year, export_rca, import_value, cog, export_value, distance, location_level, product_level) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\r\n2017-09-28 00:21:39,007 INFO sqlalchemy.engine.base.Engine ((75, 1237, 1996, 1.7283086776733398, 273487116.0, 0.0, 514320160.0, 0.5413745641708374, 'country', '4digit'), (75, 1237, 1997, 1.7167805433273315, 312047528.0, 0.0, 592372864.0, 0.5314807891845703, 'country', '4digit'), (75, 1237, 1998, 1.2120152711868286, 341676961.0, 0.0, 468860608.0, 0.5472233295440674, 'country', '4digit'), (75, 1237, 1999, 1.236651062965393, 334604240.0, 0.0, 440722336.0, 0.5695921182632446, 'country', '4digit'), (75, 1237, 2000, 1.189828872680664, 383555023.0, 0.0, 426384832.0, 0.5794379711151123, 'country', '4digit'), (75, 1237, 2001, 0.9920380115509033, 374157144.0, 0.3462945520877838, 327031392.0, 0.6234743595123291, 'country', '4digit'), (75, 1237, 2002, 1.0405025482177734, 471456583.0, 0.0, 377909376.0, 0.6023964285850525, 'country', '4digit'), (75, 1237, 2003, 1.147829532623291, 552441401.0, 0.0, 481313504.0, 0.5896202325820923, 'country', '4digit')  ... displaying 10 of 100000 total bound parameter sets ...  (79, 1024, 2015, 0.0, None, 0.8785018920898438, 0.0, 0.9823430776596069, 'country', '4digit'), (79, 1025, 1995, 0.0, None, 0.5624096989631653, 0.0, 0.9839603304862976, 'country', '4digit'))\r\n```"
  }
]
