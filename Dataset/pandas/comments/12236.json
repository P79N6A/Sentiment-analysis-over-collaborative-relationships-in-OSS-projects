[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/183800351",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-183800351",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 183800351,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE4MzgwMDM1MQ==",
    "user": {
      "login": "toobaz",
      "id": 1224492,
      "node_id": "MDQ6VXNlcjEyMjQ0OTI=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/1224492?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/toobaz",
      "html_url": "https://github.com/toobaz",
      "followers_url": "https://api.github.com/users/toobaz/followers",
      "following_url": "https://api.github.com/users/toobaz/following{/other_user}",
      "gists_url": "https://api.github.com/users/toobaz/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/toobaz/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/toobaz/subscriptions",
      "organizations_url": "https://api.github.com/users/toobaz/orgs",
      "repos_url": "https://api.github.com/users/toobaz/repos",
      "events_url": "https://api.github.com/users/toobaz/events{/privacy}",
      "received_events_url": "https://api.github.com/users/toobaz/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-02-14T02:25:11Z",
    "updated_at": "2016-02-14T02:25:11Z",
    "author_association": "MEMBER",
    "body": "[I agree!](https://github.com/pydata/pandas/issues/9641#event-251904150): although [my code](http://stackoverflow.com/questions/22522551/pandas-hdf5-as-a-database/29014295#29014295) assumes only write access, and so it would require some fixing and integration, do you think the approach makes sense?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/184257292",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-184257292",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 184257292,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE4NDI1NzI5Mg==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-02-15T15:38:11Z",
    "updated_at": "2016-02-15T15:39:34Z",
    "author_association": "CONTRIBUTOR",
    "body": "@toobaz your soln 'works', but that is not a recommended way to do it at all. HDF5 by definition should only every have a single writer.\n\nThis helps with the SWMR case: https://www.hdfgroup.org/HDF5/docNewFeatures/NewFeaturesSwmrDocs.html\n\nall that said `read_hdf` is not thread-safe for reading I think because the file handle needs to be opened in the master thread.\n\nHere's an example using a multi-proc threadpool; needs testing with a regular threadpool as well; this currently segfaults\n\n```\nimport numpy as np\nimport pandas as pd\nfrom pandas.util import testing as tm\nfrom multiprocessing.pool import ThreadPool\n\npath = 'test.hdf'\nnum_rows = 100000\nnum_tasks = 4\n\ndef make_df(num_rows=10000):\n\n    df = pd.DataFrame(np.random.rand(num_rows, 5), columns=list('abcde'))\n    df['foo'] = 'foo'\n    df['bar'] = 'bar'\n    df['baz'] = 'baz'\n    df['date'] = pd.date_range('20000101 09:00:00',\n                               periods=num_rows,\n                               freq='s')\n    df['int'] = np.arange(num_rows, dtype='int64')\n    return df\n\nprint(\"writing df\")\ndf = make_df(num_rows=num_rows)\ndf.to_hdf(path, 'df', format='table')\n\n# single threaded\nprint(\"reading df - single threaded\")\nresult = pd.read_hdf(path, 'df')\ntm.assert_frame_equal(result, df)\n\n# multip\ndef reader(arg):\n    start, nrows = arg\n\n    return pd.read_hdf(path,\n                       'df',\n                       mode='r',\n                       start=start,\n                       stop=start+nrows)\n\ntasks = [\n    (num_rows * i / num_tasks,\n     num_rows / num_tasks) for i in range(num_tasks)\n    ]\n\npool = ThreadPool(processes=num_tasks)\n\nprint(\"reading df - multi threaded\")\nresults = pool.map(reader, tasks)\nresult = pd.concat(results)\n\ntm.assert_frame_equal(result, df)\n```\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/261665593",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-261665593",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 261665593,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI2MTY2NTU5Mw==",
    "user": {
      "login": "dragonator4",
      "id": 8196363,
      "node_id": "MDQ6VXNlcjgxOTYzNjM=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/8196363?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dragonator4",
      "html_url": "https://github.com/dragonator4",
      "followers_url": "https://api.github.com/users/dragonator4/followers",
      "following_url": "https://api.github.com/users/dragonator4/following{/other_user}",
      "gists_url": "https://api.github.com/users/dragonator4/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dragonator4/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dragonator4/subscriptions",
      "organizations_url": "https://api.github.com/users/dragonator4/orgs",
      "repos_url": "https://api.github.com/users/dragonator4/repos",
      "events_url": "https://api.github.com/users/dragonator4/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dragonator4/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2016-11-18T23:06:49Z",
    "updated_at": "2016-11-18T23:06:49Z",
    "author_association": "NONE",
    "body": "Yes this is a duplicate of #14692. I have a small contribution to this discussion:\n\n```\ndef reader(arg):\n    start, nrows = arg\n    with pd.HDFStore(path, 'r') as store:\n        df = pd.read_hdf(store,\n                         'df',\n                         mode='r',\n                         start=int(start),\n                         stop=int(start+nrows))\n    return df\n```\n\nWorks without any problems in the code example above. It seems that as long as multiple connections are made to the same store, and only one query is placed through those connections, there is no error. Looking at the example I provided in #14692 again, and also the original version of `reader` given by @jreback above, multiple queries from the same connection cause all problems.\n\nThe fix may not involve placing locks on the store. Perhaps allowing `df.read_hdf` or other variants of accessing data from a store in read mode to make as many connection as required is the fix...\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/276413711",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-276413711",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 276413711,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI3NjQxMzcxMQ==",
    "user": {
      "login": "jtf621",
      "id": 2312781,
      "node_id": "MDQ6VXNlcjIzMTI3ODE=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/2312781?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jtf621",
      "html_url": "https://github.com/jtf621",
      "followers_url": "https://api.github.com/users/jtf621/followers",
      "following_url": "https://api.github.com/users/jtf621/following{/other_user}",
      "gists_url": "https://api.github.com/users/jtf621/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jtf621/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jtf621/subscriptions",
      "organizations_url": "https://api.github.com/users/jtf621/orgs",
      "repos_url": "https://api.github.com/users/jtf621/repos",
      "events_url": "https://api.github.com/users/jtf621/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jtf621/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-01-31T16:28:20Z",
    "updated_at": "2017-01-31T16:28:20Z",
    "author_association": "NONE",
    "body": "I found another example of the issue.  See #15274.\r\n\r\nNote the segfault failure rate depends on the presence or absence of compression on the hdf file.\r\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/296421087",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-296421087",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 296421087,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjQyMTA4Nw==",
    "user": {
      "login": "rbiswas4",
      "id": 2281358,
      "node_id": "MDQ6VXNlcjIyODEzNTg=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/2281358?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/rbiswas4",
      "html_url": "https://github.com/rbiswas4",
      "followers_url": "https://api.github.com/users/rbiswas4/followers",
      "following_url": "https://api.github.com/users/rbiswas4/following{/other_user}",
      "gists_url": "https://api.github.com/users/rbiswas4/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/rbiswas4/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/rbiswas4/subscriptions",
      "organizations_url": "https://api.github.com/users/rbiswas4/orgs",
      "repos_url": "https://api.github.com/users/rbiswas4/repos",
      "events_url": "https://api.github.com/users/rbiswas4/events{/privacy}",
      "received_events_url": "https://api.github.com/users/rbiswas4/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-04-23T05:37:10Z",
    "updated_at": "2017-04-23T05:37:10Z",
    "author_association": "NONE",
    "body": "I am running into this issue on both OSX and Linux (centos), trying to parallelize using joblib. even when I am not trying any write process part of SWMR.  Is there a different way for a readonly solution? Is this a pandas issue or a h5.c issue? Thanks.\r\n\r\nIn my application I am trying to read different groups on different processors, and running into this error. \r\nI am using\r\n```\r\npandas                    0.19.1\r\npytables                  3.4.2\r\njoblib                    0.11\r\n```\r\nThe code which works when `n_jobs=1` and fails when `n_jobs=2` is 2 or more. \r\n```\r\nstore = pd.HDFStore('ddf_lcs.hdf', mode='r')\r\nparams = pd.read_hdf('ddf_params.hdf')\r\n\r\ntileIDs = params.tileID.dropna().unique().astype(np.int)\r\n\r\ndef runGroup(i, store=store, minion_params=params):\r\n      print('group ', i)\r\n      if i is np.nan:\r\n          return\r\n      low = i - 0.5\r\n      high = i + 0.5\r\n      print('i, low and high are ', i, low, high)\r\n      key = str(int(i))\r\n      lcdf = store.select(key)\r\n      print('this group has {0} rows and {1} unique values of var'.format(len(lcdf), lcdf.snid.unique().size))\r\nParallel(n_jobs=1)(delayed(runGroup)(t) for t in tileIDs[:10])\r\nstore.close()\r\n```\r\nThe error message I get is : \r\n```\r\n/usr/local/miniconda/lib/python2.7/site-packages/tables/group.py:1213: UserWarning: problems loading leaf ``/282748/table``::\r\n\r\n  HDF5 error back trace\r\n\r\n  File \"H5Dio.c\", line 173, in H5Dread\r\n    can't read data\r\n  File \"H5Dio.c\", line 554, in H5D__read\r\n    can't read data\r\n  File \"H5Dchunk.c\", line 1856, in H5D__chunk_read\r\n    error looking up chunk address\r\n  File \"H5Dchunk.c\", line 2441, in H5D__chunk_lookup\r\n    can't query chunk address\r\n  File \"H5Dbtree.c\", line 998, in H5D__btree_idx_get_addr\r\n    can't get chunk info\r\n  File \"H5B.c\", line 340, in H5B_find\r\n    unable to load B-tree node\r\n  File \"H5AC.c\", line 1262, in H5AC_protect\r\n    H5C_protect() failed.\r\n  File \"H5C.c\", line 3574, in H5C_protect\r\n    can't load entry\r\n  File \"H5C.c\", line 7954, in H5C_load_entry\r\n    unable to load entry\r\n File \"H5Bcache.c\", line 143, in H5B__load\r\n    wrong B-tree signature\r\n\r\nEnd of HDF5 error back trace\r\n...\r\n```\r\n\r\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/296444044",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-296444044",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 296444044,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjQ0NDA0NA==",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-04-23T13:38:49Z",
    "updated_at": "2017-04-23T13:38:49Z",
    "author_association": "CONTRIBUTOR",
    "body": "@rbiswas4 \r\nyou can't pass the handles in like this. you can try to open *inside* the passed function (as read-only) might work. this is a non-trivial problem (reading against HDF5 in a threadsafe manner via multiple processes). You can have a look at how [dask](https://github.com/dask/dask/search?q=HDF5&type=Issues&utf8=%E2%9C%93) solved (and still dealing with this)."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/296459975",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-296459975",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 296459975,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjQ1OTk3NQ==",
    "user": {
      "login": "rbiswas4",
      "id": 2281358,
      "node_id": "MDQ6VXNlcjIyODEzNTg=",
      "avatar_url": "https://avatars2.githubusercontent.com/u/2281358?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/rbiswas4",
      "html_url": "https://github.com/rbiswas4",
      "followers_url": "https://api.github.com/users/rbiswas4/followers",
      "following_url": "https://api.github.com/users/rbiswas4/following{/other_user}",
      "gists_url": "https://api.github.com/users/rbiswas4/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/rbiswas4/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/rbiswas4/subscriptions",
      "organizations_url": "https://api.github.com/users/rbiswas4/orgs",
      "repos_url": "https://api.github.com/users/rbiswas4/repos",
      "events_url": "https://api.github.com/users/rbiswas4/events{/privacy}",
      "received_events_url": "https://api.github.com/users/rbiswas4/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-04-23T17:21:10Z",
    "updated_at": "2017-04-23T17:21:23Z",
    "author_association": "NONE",
    "body": ">  you can try to open inside the passed function (as read-only) \r\n\r\n@jreback  Interesting. I thought I had trouble with that and had hence moved to `hdfstore` but it looks like I should be able to pass the filename to the function and read it with `pd.read_hdf(filename, key=key)` inside the function. Some initial tests seem to suggest that this is true indeed. That would solve the problem of reading. So if I am guessing correctly, the threadsafe mechanism is essentially in a lock on the `open file` process, and not on retrieval processes like `get` or `select`, which is what I thought `HDFStore` was for. \r\n\r\nAlso thank you for the pointer to the dask issues which discuss parallel write/save mechanisms. \r\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/328261527",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-328261527",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 328261527,
    "node_id": "MDEyOklzc3VlQ29tbWVudDMyODI2MTUyNw==",
    "user": {
      "login": "grantstephens",
      "id": 2863673,
      "node_id": "MDQ6VXNlcjI4NjM2NzM=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/2863673?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/grantstephens",
      "html_url": "https://github.com/grantstephens",
      "followers_url": "https://api.github.com/users/grantstephens/followers",
      "following_url": "https://api.github.com/users/grantstephens/following{/other_user}",
      "gists_url": "https://api.github.com/users/grantstephens/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/grantstephens/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/grantstephens/subscriptions",
      "organizations_url": "https://api.github.com/users/grantstephens/orgs",
      "repos_url": "https://api.github.com/users/grantstephens/repos",
      "events_url": "https://api.github.com/users/grantstephens/events{/privacy}",
      "received_events_url": "https://api.github.com/users/grantstephens/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-09-09T07:51:08Z",
    "updated_at": "2017-09-09T07:51:08Z",
    "author_association": "NONE",
    "body": "So I think I have just run into this issue but in a slightly different use case, I think.\r\nI have multiple threads that each do things with different hdf files using with pd.HDFStore. It seems to read the files fine, but when it comes to writing to them it fails with what looks like an error about the compression from pytables. Error:\r\n`if complib is not None and complib not in tables.filters.all_complibs:\r\nAttributeError: module 'tables' has no attribute 'filters'` \r\nI must just stress this, they are completely different files but I am trying to write to them using compression from different threads. I think it may be that I am trying to use the compression library at the same time, but any hints or advice would be appreciated."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/352672928",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-352672928",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 352672928,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM1MjY3MjkyOA==",
    "user": {
      "login": "lafrech",
      "id": 1767010,
      "node_id": "MDQ6VXNlcjE3NjcwMTA=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/1767010?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/lafrech",
      "html_url": "https://github.com/lafrech",
      "followers_url": "https://api.github.com/users/lafrech/followers",
      "following_url": "https://api.github.com/users/lafrech/following{/other_user}",
      "gists_url": "https://api.github.com/users/lafrech/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/lafrech/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/lafrech/subscriptions",
      "organizations_url": "https://api.github.com/users/lafrech/orgs",
      "repos_url": "https://api.github.com/users/lafrech/repos",
      "events_url": "https://api.github.com/users/lafrech/events{/privacy}",
      "received_events_url": "https://api.github.com/users/lafrech/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2017-12-19T08:26:22Z",
    "updated_at": "2017-12-19T08:26:22Z",
    "author_association": "NONE",
    "body": "For the record, this is what I do. It is probably obvious, but just in case it could help anyone, here it is.\r\n\r\n```python\r\nHDF_LOCK = threading.Lock()\r\nHDF_FILEPATH = '/my_file_path'\r\n\r\n@contextmanager\r\ndef locked_store():\r\n    with HDF_LOCK:\r\n        with pd.HDFStore(HDF_FILEPATH) as store:\r\n            yield store\r\n```\r\n\r\nThen\r\n\r\n```python\r\n    def get(self, ts_id, t_start, t_end):\r\n        with locked_store() as store:\r\n            where = 'index>=t_start and index<t_end'\r\n            return store.select(ts_id, where=where)\r\n```\r\n\r\nIf several files are used, a dict of locks can be used to avoid locking the whole application. It can be a `defaultdict` with a new `Lock` as default value. The key should be the absolute filepath with no symlink, to be sure a file can't be accessed with two different locks."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/375100429",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-375100429",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 375100429,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTEwMDQyOQ==",
    "user": {
      "login": "makmanalp",
      "id": 161965,
      "node_id": "MDQ6VXNlcjE2MTk2NQ==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/161965?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/makmanalp",
      "html_url": "https://github.com/makmanalp",
      "followers_url": "https://api.github.com/users/makmanalp/followers",
      "following_url": "https://api.github.com/users/makmanalp/following{/other_user}",
      "gists_url": "https://api.github.com/users/makmanalp/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/makmanalp/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/makmanalp/subscriptions",
      "organizations_url": "https://api.github.com/users/makmanalp/orgs",
      "repos_url": "https://api.github.com/users/makmanalp/repos",
      "events_url": "https://api.github.com/users/makmanalp/events{/privacy}",
      "received_events_url": "https://api.github.com/users/makmanalp/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-21T21:23:14Z",
    "updated_at": "2018-03-21T21:23:14Z",
    "author_association": "CONTRIBUTOR",
    "body": "@lafrech you could even make HDF_FILEPATH just an argument to the context manager and make it more generic:\r\n\r\n```python\r\n@contextmanager\r\ndef locked_store(*args, lock=threading.Lock, **kwargs):\r\n    with lock():\r\n        with pd.HDFStore(*args, **kwargs) as store:\r\n            yield store\r\n```"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/375103275",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-375103275",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 375103275,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTEwMzI3NQ==",
    "user": {
      "login": "lafrech",
      "id": 1767010,
      "node_id": "MDQ6VXNlcjE3NjcwMTA=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/1767010?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/lafrech",
      "html_url": "https://github.com/lafrech",
      "followers_url": "https://api.github.com/users/lafrech/followers",
      "following_url": "https://api.github.com/users/lafrech/following{/other_user}",
      "gists_url": "https://api.github.com/users/lafrech/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/lafrech/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/lafrech/subscriptions",
      "organizations_url": "https://api.github.com/users/lafrech/orgs",
      "repos_url": "https://api.github.com/users/lafrech/repos",
      "events_url": "https://api.github.com/users/lafrech/events{/privacy}",
      "received_events_url": "https://api.github.com/users/lafrech/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-21T21:33:20Z",
    "updated_at": "2018-03-21T21:33:20Z",
    "author_association": "NONE",
    "body": "@makmanalp, won't this create a new lock for each call (thus defeating the purpose of the lock)?\r\n\r\nWe need to be sure that calls to the same file share the same lock and calls to different files don't.\r\n\r\nI don't see how this is achieved in your example.\r\n\r\nI assume I have to map the name, path or whatever identifier of the file to a lock instance.\r\n\r\nOr maybe I am missing something."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/375768956",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-375768956",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 375768956,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTc2ODk1Ng==",
    "user": {
      "login": "makmanalp",
      "id": 161965,
      "node_id": "MDQ6VXNlcjE2MTk2NQ==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/161965?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/makmanalp",
      "html_url": "https://github.com/makmanalp",
      "followers_url": "https://api.github.com/users/makmanalp/followers",
      "following_url": "https://api.github.com/users/makmanalp/following{/other_user}",
      "gists_url": "https://api.github.com/users/makmanalp/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/makmanalp/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/makmanalp/subscriptions",
      "organizations_url": "https://api.github.com/users/makmanalp/orgs",
      "repos_url": "https://api.github.com/users/makmanalp/repos",
      "events_url": "https://api.github.com/users/makmanalp/events{/privacy}",
      "received_events_url": "https://api.github.com/users/makmanalp/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-23T19:05:03Z",
    "updated_at": "2018-03-23T19:05:03Z",
    "author_association": "CONTRIBUTOR",
    "body": "Er, my mistake entirely - I was just trying to point out that one can make it more reusable. Should be something like:\r\n\r\n```python\r\ndef make_locked_store(lock, filepath):\r\n    @contextmanager\r\n    def locked_store(*args, **kwargs):\r\n        with lock:\r\n            with pd.HDFStore(filepath, *args, **kwargs) as store:\r\n                yield store\r\n    return locked_store\r\n```"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/426544012",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-426544012",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 426544012,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjU0NDAxMg==",
    "user": {
      "login": "lafrech",
      "id": 1767010,
      "node_id": "MDQ6VXNlcjE3NjcwMTA=",
      "avatar_url": "https://avatars0.githubusercontent.com/u/1767010?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/lafrech",
      "html_url": "https://github.com/lafrech",
      "followers_url": "https://api.github.com/users/lafrech/followers",
      "following_url": "https://api.github.com/users/lafrech/following{/other_user}",
      "gists_url": "https://api.github.com/users/lafrech/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/lafrech/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/lafrech/subscriptions",
      "organizations_url": "https://api.github.com/users/lafrech/orgs",
      "repos_url": "https://api.github.com/users/lafrech/repos",
      "events_url": "https://api.github.com/users/lafrech/events{/privacy}",
      "received_events_url": "https://api.github.com/users/lafrech/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-10-03T07:52:09Z",
    "updated_at": "2018-10-03T07:52:09Z",
    "author_association": "NONE",
    "body": "> If several files are used, a dict of locks can be used to avoid locking the whole application.\r\n\r\n:warning: Warning :warning:\r\n\r\nI've been trying this and had issues. It looks like the whole hdf5 lib is not thread-safe and it fails even when accessing two different files concurrently. I didn't take the time to check that, so I may be totally wrong, but beware if you try that.\r\n\r\nI reverted my change, and I keep a single lock in the application rather than a lock per hdf5 file.\r\n\r\nSince hdf5 is hierarchical, I guess lot of users put everything in a single file anyway. We ended up using lots of files mainly because hdf5 files tend to get corrupted so when restoring daily backups, we only loose data from a single file rather than from the whole base."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/463271025",
    "html_url": "https://github.com/pandas-dev/pandas/issues/12236#issuecomment-463271025",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/12236",
    "id": 463271025,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ2MzI3MTAyNQ==",
    "user": {
      "login": "schneiderfelipe",
      "id": 37125,
      "node_id": "MDQ6VXNlcjM3MTI1",
      "avatar_url": "https://avatars1.githubusercontent.com/u/37125?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/schneiderfelipe",
      "html_url": "https://github.com/schneiderfelipe",
      "followers_url": "https://api.github.com/users/schneiderfelipe/followers",
      "following_url": "https://api.github.com/users/schneiderfelipe/following{/other_user}",
      "gists_url": "https://api.github.com/users/schneiderfelipe/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/schneiderfelipe/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/schneiderfelipe/subscriptions",
      "organizations_url": "https://api.github.com/users/schneiderfelipe/orgs",
      "repos_url": "https://api.github.com/users/schneiderfelipe/repos",
      "events_url": "https://api.github.com/users/schneiderfelipe/events{/privacy}",
      "received_events_url": "https://api.github.com/users/schneiderfelipe/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-02-13T16:38:25Z",
    "updated_at": "2019-02-13T16:38:25Z",
    "author_association": "NONE",
    "body": "I just got the very same problem with PyTables 3.4.4 and Pandas 0.24.1.\r\nDowngrading to the ones provided by Ubuntu (3.4.2-4 and 0.22.0-4, respectively) solved the issue."
  }
]
