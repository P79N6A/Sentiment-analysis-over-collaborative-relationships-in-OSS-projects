[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/65864176",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9017#issuecomment-65864176",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9017",
    "id": 65864176,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1ODY0MTc2",
    "user": {
      "login": "arogozhnikov",
      "id": 6318811,
      "node_id": "MDQ6VXNlcjYzMTg4MTE=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/6318811?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/arogozhnikov",
      "html_url": "https://github.com/arogozhnikov",
      "followers_url": "https://api.github.com/users/arogozhnikov/followers",
      "following_url": "https://api.github.com/users/arogozhnikov/following{/other_user}",
      "gists_url": "https://api.github.com/users/arogozhnikov/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/arogozhnikov/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/arogozhnikov/subscriptions",
      "organizations_url": "https://api.github.com/users/arogozhnikov/orgs",
      "repos_url": "https://api.github.com/users/arogozhnikov/repos",
      "events_url": "https://api.github.com/users/arogozhnikov/events{/privacy}",
      "received_events_url": "https://api.github.com/users/arogozhnikov/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-12-05T22:22:41Z",
    "updated_at": "2014-12-05T22:22:41Z",
    "author_association": "NONE",
    "body": "And yes, I know about `TextFileReader` - probably worth extending it?\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/65865012",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9017#issuecomment-65865012",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9017",
    "id": 65865012,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1ODY1MDEy",
    "user": {
      "login": "jreback",
      "id": 953992,
      "node_id": "MDQ6VXNlcjk1Mzk5Mg==",
      "avatar_url": "https://avatars2.githubusercontent.com/u/953992?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jreback",
      "html_url": "https://github.com/jreback",
      "followers_url": "https://api.github.com/users/jreback/followers",
      "following_url": "https://api.github.com/users/jreback/following{/other_user}",
      "gists_url": "https://api.github.com/users/jreback/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jreback/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jreback/subscriptions",
      "organizations_url": "https://api.github.com/users/jreback/orgs",
      "repos_url": "https://api.github.com/users/jreback/repos",
      "events_url": "https://api.github.com/users/jreback/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jreback/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-12-05T22:29:41Z",
    "updated_at": "2014-12-05T22:29:52Z",
    "author_association": "CONTRIBUTOR",
    "body": "you can easily chunk read csvs, see [here](http://pandas.pydata.org/pandas-docs/stable/io.html#iterating-through-files-chunk-by-chunk) to process arbitrarily sized data.\n\n[blaze](http://blaze.pydata.org/docs/latest/index.html) uses this feature to process things like this\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/65909915",
    "html_url": "https://github.com/pandas-dev/pandas/issues/9017#issuecomment-65909915",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/9017",
    "id": 65909915,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1OTA5OTE1",
    "user": {
      "login": "arogozhnikov",
      "id": 6318811,
      "node_id": "MDQ6VXNlcjYzMTg4MTE=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/6318811?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/arogozhnikov",
      "html_url": "https://github.com/arogozhnikov",
      "followers_url": "https://api.github.com/users/arogozhnikov/followers",
      "following_url": "https://api.github.com/users/arogozhnikov/following{/other_user}",
      "gists_url": "https://api.github.com/users/arogozhnikov/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/arogozhnikov/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/arogozhnikov/subscriptions",
      "organizations_url": "https://api.github.com/users/arogozhnikov/orgs",
      "repos_url": "https://api.github.com/users/arogozhnikov/repos",
      "events_url": "https://api.github.com/users/arogozhnikov/events{/privacy}",
      "received_events_url": "https://api.github.com/users/arogozhnikov/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-12-06T19:13:15Z",
    "updated_at": "2014-12-06T19:13:15Z",
    "author_association": "NONE",
    "body": "> you can easily chunk read csvs, see here to process arbitrarily sized data.\n\nI know about this possibility, but this is not enough for my purposes\n\n> blaze uses this feature to process things like this\n\nMany thanks for this link! \nBlaze with pandas backend seems to be unable to operate big datasets, however it provides some abstraction for query that can be evaluated, and probably I'll be able to adopt the solution with streaming python backend.\n\nI'll keep this issue open for a week to hear about other solutions / links / proposals and then close.\n"
  }
]
