[
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/368332528",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19886#issuecomment-368332528",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19886",
    "id": 368332528,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM2ODMzMjUyOA==",
    "user": {
      "login": "chris-b1",
      "id": 1924092,
      "node_id": "MDQ6VXNlcjE5MjQwOTI=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/1924092?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/chris-b1",
      "html_url": "https://github.com/chris-b1",
      "followers_url": "https://api.github.com/users/chris-b1/followers",
      "following_url": "https://api.github.com/users/chris-b1/following{/other_user}",
      "gists_url": "https://api.github.com/users/chris-b1/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/chris-b1/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/chris-b1/subscriptions",
      "organizations_url": "https://api.github.com/users/chris-b1/orgs",
      "repos_url": "https://api.github.com/users/chris-b1/repos",
      "events_url": "https://api.github.com/users/chris-b1/events{/privacy}",
      "received_events_url": "https://api.github.com/users/chris-b1/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-02-25T18:33:34Z",
    "updated_at": "2018-02-25T18:33:34Z",
    "author_association": "CONTRIBUTOR",
    "body": "You're certainly welcome to look into this - as discussed in #2741 - might still be some parts of the parser that uses `NUL` to manage state.  Though in practice may be easier to pre-process your data, stripping out nulls before passing to pandas."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/370013114",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19886#issuecomment-370013114",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19886",
    "id": 370013114,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDAxMzExNA==",
    "user": {
      "login": "WillAyd",
      "id": 609873,
      "node_id": "MDQ6VXNlcjYwOTg3Mw==",
      "avatar_url": "https://avatars0.githubusercontent.com/u/609873?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/WillAyd",
      "html_url": "https://github.com/WillAyd",
      "followers_url": "https://api.github.com/users/WillAyd/followers",
      "following_url": "https://api.github.com/users/WillAyd/following{/other_user}",
      "gists_url": "https://api.github.com/users/WillAyd/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/WillAyd/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/WillAyd/subscriptions",
      "organizations_url": "https://api.github.com/users/WillAyd/orgs",
      "repos_url": "https://api.github.com/users/WillAyd/repos",
      "events_url": "https://api.github.com/users/WillAyd/events{/privacy}",
      "received_events_url": "https://api.github.com/users/WillAyd/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-02T18:38:19Z",
    "updated_at": "2018-03-02T18:38:19Z",
    "author_association": "MEMBER",
    "body": "A little out of my area but isn't this just how strings work in C? For instance, if I compile the below program\r\n\r\n```c\r\n\r\n#include <stdio.h>\r\n\r\nint main () {\r\n  char myarr[8] = \"foo\\0test\";\r\n  printf(\"%s\\n\", myarr);\r\n}\r\n```\r\n\r\nAnd execute only `foo` gets printed, skipping over all of the characters after the null"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/370017072",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19886#issuecomment-370017072",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19886",
    "id": 370017072,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDAxNzA3Mg==",
    "user": {
      "login": "chris-b1",
      "id": 1924092,
      "node_id": "MDQ6VXNlcjE5MjQwOTI=",
      "avatar_url": "https://avatars1.githubusercontent.com/u/1924092?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/chris-b1",
      "html_url": "https://github.com/chris-b1",
      "followers_url": "https://api.github.com/users/chris-b1/followers",
      "following_url": "https://api.github.com/users/chris-b1/following{/other_user}",
      "gists_url": "https://api.github.com/users/chris-b1/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/chris-b1/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/chris-b1/subscriptions",
      "organizations_url": "https://api.github.com/users/chris-b1/orgs",
      "repos_url": "https://api.github.com/users/chris-b1/repos",
      "events_url": "https://api.github.com/users/chris-b1/events{/privacy}",
      "received_events_url": "https://api.github.com/users/chris-b1/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-02T18:52:08Z",
    "updated_at": "2018-03-02T18:52:08Z",
    "author_association": "CONTRIBUTOR",
    "body": "Yes & no.  You are correct that a null byte `'\\0'` is used to mark the end of a \"c string,\" which is really just a `char` array with that convention.\r\n\r\nBut I believe the parser is working with a sized buffer (i.e. knows that `myarr` is length 8 in your example) - so wouldn't necessarily need to stop on the null byte.  That said,  I'm not familiar with low level workings of the parser, could be wrong on this."
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/370044524",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19886#issuecomment-370044524",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19886",
    "id": 370044524,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDA0NDUyNA==",
    "user": {
      "login": "WillAyd",
      "id": 609873,
      "node_id": "MDQ6VXNlcjYwOTg3Mw==",
      "avatar_url": "https://avatars0.githubusercontent.com/u/609873?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/WillAyd",
      "html_url": "https://github.com/WillAyd",
      "followers_url": "https://api.github.com/users/WillAyd/followers",
      "following_url": "https://api.github.com/users/WillAyd/following{/other_user}",
      "gists_url": "https://api.github.com/users/WillAyd/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/WillAyd/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/WillAyd/subscriptions",
      "organizations_url": "https://api.github.com/users/WillAyd/orgs",
      "repos_url": "https://api.github.com/users/WillAyd/repos",
      "events_url": "https://api.github.com/users/WillAyd/events{/privacy}",
      "received_events_url": "https://api.github.com/users/WillAyd/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-02T20:33:44Z",
    "updated_at": "2018-03-02T20:33:44Z",
    "author_association": "MEMBER",
    "body": "Hmm OK. Well from what I can tell with VERBOSITY set I think the tokenizer interprets this correctly. Here's a small excerpt from the last example provided, where I believe `Iter 8` is actually pushing the NULL byte without triggering a field end\r\n\r\n```bash\r\n\r\ntokenize_bytes - Iter: 4 Char: 0x74 Line 2 field_count 0, state 0\r\nPUSH_CHAR: Pushing t, slen= 4, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 5 Char: 0x65 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing e, slen= 5, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 6 Char: 0x73 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing s, slen= 6, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 7 Char: 0x74 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing t, slen= 7, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 8 Char: 0x0 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing , slen= 8, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 9 Char: 0x74 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing t, slen= 9, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 10 Char: 0x65 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing e, slen= 10, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 11 Char: 0x73 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing s, slen= 11, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 12 Char: 0x74 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing t, slen= 12, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 13 Char: 0x2c Line 2 field_count 0, state 3\r\npush_char: self->stream[14] = 0, stream_cap=128\r\nend_field: Char diff: 4\r\nend_field: Saw word test at: 4. Total: 3\r\n\r\n```\r\n\r\nMy guess is the issue is in the parser using `PyUnicode_FromString` instead of `PyUnicode_FromStringAndSize` and providing the length of the field, which should have been detected in some form or another by the tokenizer\r\n\r\nhttps://github.com/pandas-dev/pandas/blob/a7a7f8c1101aed1a9d37abbbcd80f77da414f0a8/pandas/_libs/parsers.pyx#L768\r\n\r\nAgain haven't dug this deep into the C side of things before so could be way off, but figured I'd share in case it helps anyone else looking at it\r\n\r\nFWIW here's the full verbose output of parsing the third example provided in the original post\r\n\r\n<details>\r\n\r\n```bash\r\n\r\n_tokenize_helper: Asked to tokenize 2 rows, datapos=0, datalen=0\r\nparser_buffer_bytes self->cb_io: nbytes=262144, datalen: 38, status=0\r\ndatalen: 38\r\n_tokenize_helper: Trying to process 38 bytes, datalen=38, datapos= 0\r\n\r\n\r\nmake_stream_space: nbytes = 38.  grow_buffer(self->stream...)\r\nsafe_realloc: buffer = 0x7fbbff62c2e0, size = 64, result = 0x7fbbff63a090\r\nsafe_realloc: buffer = 0x7fbbff63a090, size = 128, result = 0x7fbbff63a090\r\nmake_stream_space: self->stream=0x7fbbff63a090, self->stream_len = 0, self->stream_cap=128, status=0\r\nsafe_realloc: buffer = 0x7fbbff6279f0, size = 48, result = 0x7fbbff63a110\r\nsafe_realloc: buffer = 0x7fbbff63a110, size = 96, result = 0x7fbbff63a110\r\nsafe_realloc: buffer = 0x7fbbff63a110, size = 192, result = 0x7fbbff63dec0\r\nsafe_realloc: buffer = 0x7fbbff63dec0, size = 384, result = 0x7fbbff63dec0\r\nmake_stream_space: grow_buffer(self->self->words, 0, 48, 38, 0)\r\nmake_stream_space: cap != self->words_cap, nbytes = 38, self->words_cap=48\r\nsafe_realloc: buffer = 0x7fbbff60a100, size = 384, result = 0x7fbbff63f5f0\r\nsafe_realloc: buffer = 0x7fbbff632b20, size = 48, result = 0x7fbbff62c2e0\r\nsafe_realloc: buffer = 0x7fbbff62c2e0, size = 96, result = 0x7fbbff63a110\r\nsafe_realloc: buffer = 0x7fbbff63a110, size = 192, result = 0x7fbbff63e040\r\nsafe_realloc: buffer = 0x7fbbff63e040, size = 384, result = 0x7fbbff63f770\r\nmake_stream_space: grow_buffer(self->line_start, 1, 48, 38, 0)\r\nmake_stream_space: cap != self->lines_cap, nbytes = 38\r\nsafe_realloc: buffer = 0x7fbbff632b40, size = 384, result = 0x7fbbff63f8f0\r\nx,y\r\ntest\r\ntokenize_bytes - Iter: 0 Char: 0x78 Line 1 field_count 0, state 0\r\nPUSH_CHAR: Pushing x, slen= 0, stream_cap=128, stream_len=0\r\ntokenize_bytes - Iter: 1 Char: 0x2c Line 1 field_count 0, state 3\r\npush_char: self->stream[2] = 0, stream_cap=128\r\nend_field: Char diff: 0\r\nend_field: Saw word x at: 0. Total: 1\r\ntokenize_bytes - Iter: 2 Char: 0x79 Line 1 field_count 1, state 1\r\nPUSH_CHAR: Pushing y, slen= 2, stream_cap=128, stream_len=2\r\ntokenize_bytes - Iter: 3 Char: 0xa Line 1 field_count 1, state 3\r\npush_char: self->stream[4] = 0, stream_cap=128\r\nend_field: Char diff: 2\r\nend_field: Saw word y at: 2. Total: 2\r\nend_line: Line end, nfields: 2\r\nend_line: lines: 0\r\nend_line: ex_fields: -1\r\nend_line: new line start: 2\r\nend_line: Finished line, at 1\r\ntokenize_bytes - Iter: 4 Char: 0x74 Line 2 field_count 0, state 0\r\nPUSH_CHAR: Pushing t, slen= 4, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 5 Char: 0x65 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing e, slen= 5, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 6 Char: 0x73 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing s, slen= 6, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 7 Char: 0x74 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing t, slen= 7, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 8 Char: 0x0 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing , slen= 8, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 9 Char: 0x74 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing t, slen= 9, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 10 Char: 0x65 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing e, slen= 10, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 11 Char: 0x73 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing s, slen= 11, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 12 Char: 0x74 Line 2 field_count 0, state 3\r\nPUSH_CHAR: Pushing t, slen= 12, stream_cap=128, stream_len=4\r\ntokenize_bytes - Iter: 13 Char: 0x2c Line 2 field_count 0, state 3\r\npush_char: self->stream[14] = 0, stream_cap=128\r\nend_field: Char diff: 4\r\nend_field: Saw word test at: 4. Total: 3\r\ntokenize_bytes - Iter: 14 Char: 0x52 Line 2 field_count 1, state 1\r\nPUSH_CHAR: Pushing R, slen= 14, stream_cap=128, stream_len=14\r\ntokenize_bytes - Iter: 15 Char: 0x65 Line 2 field_count 1, state 3\r\nPUSH_CHAR: Pushing e, slen= 15, stream_cap=128, stream_len=14\r\ntokenize_bytes - Iter: 16 Char: 0x67 Line 2 field_count 1, state 3\r\nPUSH_CHAR: Pushing g, slen= 16, stream_cap=128, stream_len=14\r\ntokenize_bytes - Iter: 17 Char: 0xa Line 2 field_count 1, state 3\r\npush_char: self->stream[18] = 0, stream_cap=128\r\nend_field: Char diff: 14\r\nend_field: Saw word Reg at: 14. Total: 4\r\nend_line: Line end, nfields: 2\r\nend_line: lines: 1\r\nend_line: ex_fields: 2\r\nend_line: new line start: 4\r\nend_line: Finished line, at 2\r\n_TOKEN_CLEANUP: datapos: 18, datalen: 38\r\nleaving tokenize_helper\r\n_tokenize_helper: Asked to tokenize 262143 rows, datapos=18, datalen=38\r\n_tokenize_helper: Trying to process 20 bytes, datalen=38, datapos= 18\r\n\r\n\r\nmake_stream_space: nbytes = 20.  grow_buffer(self->stream...)\r\nmake_stream_space: self->stream=0x7fbbff63a090, self->stream_len = 18, self->stream_cap=128, status=0\r\nmake_stream_space: grow_buffer(self->self->words, 4, 48, 20, 0)\r\nmake_stream_space: grow_buffer(self->line_start, 3, 48, 20, 0)\r\n\r\ntokenize_bytes - Iter: 18 Char: 0x0 Line 3 field_count 0, state 0\r\nPUSH_CHAR: Pushing , slen= 18, stream_cap=128, stream_len=18\r\ntokenize_bytes - Iter: 19 Char: 0x0 Line 3 field_count 0, state 3\r\nPUSH_CHAR: Pushing , slen= 19, stream_cap=128, stream_len=18\r\ntokenize_bytes - Iter: 20 Char: 0x0 Line 3 field_count 0, state 3\r\nPUSH_CHAR: Pushing , slen= 20, stream_cap=128, stream_len=18\r\ntokenize_bytes - Iter: 21 Char: 0x2c Line 3 field_count 0, state 3\r\npush_char: self->stream[22] = 0, stream_cap=128\r\nend_field: Char diff: 18\r\nend_field: Saw word  at: 18. Total: 5\r\ntokenize_bytes - Iter: 22 Char: 0x52 Line 3 field_count 1, state 1\r\nPUSH_CHAR: Pushing R, slen= 22, stream_cap=128, stream_len=22\r\ntokenize_bytes - Iter: 23 Char: 0x65 Line 3 field_count 1, state 3\r\nPUSH_CHAR: Pushing e, slen= 23, stream_cap=128, stream_len=22\r\ntokenize_bytes - Iter: 24 Char: 0x67 Line 3 field_count 1, state 3\r\nPUSH_CHAR: Pushing g, slen= 24, stream_cap=128, stream_len=22\r\ntokenize_bytes - Iter: 25 Char: 0xa Line 3 field_count 1, state 3\r\npush_char: self->stream[26] = 0, stream_cap=128\r\nend_field: Char diff: 22\r\nend_field: Saw word Reg at: 22. Total: 6\r\nend_line: Line end, nfields: 2\r\nend_line: lines: 2\r\nend_line: ex_fields: 2\r\nend_line: new line start: 6\r\nend_line: Finished line, at 3\r\ntokenize_bytes - Iter: 26 Char: 0x49 Line 4 field_count 0, state 0\r\nPUSH_CHAR: Pushing I, slen= 26, stream_cap=128, stream_len=26\r\ntokenize_bytes - Iter: 27 Char: 0x2c Line 4 field_count 0, state 3\r\npush_char: self->stream[28] = 0, stream_cap=128\r\nend_field: Char diff: 26\r\nend_field: Saw word I at: 26. Total: 7\r\ntokenize_bytes - Iter: 28 Char: 0x53 Line 4 field_count 1, state 1\r\nPUSH_CHAR: Pushing S, slen= 28, stream_cap=128, stream_len=28\r\ntokenize_bytes - Iter: 29 Char: 0x77 Line 4 field_count 1, state 3\r\nPUSH_CHAR: Pushing w, slen= 29, stream_cap=128, stream_len=28\r\ntokenize_bytes - Iter: 30 Char: 0x70 Line 4 field_count 1, state 3\r\nPUSH_CHAR: Pushing p, slen= 30, stream_cap=128, stream_len=28\r\ntokenize_bytes - Iter: 31 Char: 0xa Line 4 field_count 1, state 3\r\npush_char: self->stream[32] = 0, stream_cap=128\r\nend_field: Char diff: 28\r\nend_field: Saw word Swp at: 28. Total: 8\r\nend_line: Line end, nfields: 2\r\nend_line: lines: 3\r\nend_line: ex_fields: 2\r\nend_line: new line start: 8\r\nend_line: Finished line, at 4\r\ntokenize_bytes - Iter: 32 Char: 0x49 Line 5 field_count 0, state 0\r\nPUSH_CHAR: Pushing I, slen= 32, stream_cap=128, stream_len=32\r\ntokenize_bytes - Iter: 33 Char: 0x2c Line 5 field_count 0, state 3\r\npush_char: self->stream[34] = 0, stream_cap=128\r\nend_field: Char diff: 32\r\nend_field: Saw word I at: 32. Total: 9\r\ntokenize_bytes - Iter: 34 Char: 0x53 Line 5 field_count 1, state 1\r\nPUSH_CHAR: Pushing S, slen= 34, stream_cap=128, stream_len=34\r\ntokenize_bytes - Iter: 35 Char: 0x77 Line 5 field_count 1, state 3\r\nPUSH_CHAR: Pushing w, slen= 35, stream_cap=128, stream_len=34\r\ntokenize_bytes - Iter: 36 Char: 0x70 Line 5 field_count 1, state 3\r\nPUSH_CHAR: Pushing p, slen= 36, stream_cap=128, stream_len=34\r\ntokenize_bytes - Iter: 37 Char: 0xa Line 5 field_count 1, state 3\r\npush_char: self->stream[38] = 0, stream_cap=128\r\nend_field: Char diff: 34\r\nend_field: Saw word Swp at: 34. Total: 10\r\nend_line: Line end, nfields: 2\r\nend_line: lines: 4\r\nend_line: ex_fields: 2\r\nend_line: new line start: 10\r\nend_line: Finished line, at 5\r\n_TOKEN_CLEANUP: datapos: 38, datalen: 38\r\nFinished tokenizing input\r\nparser_buffer_bytes self->cb_io: nbytes=262144, datalen: 0, status=1\r\ndatalen: 0\r\nhandling eof, datalen: 0, pstate: 0\r\nleaving tokenize_helper\r\nparser_consume_rows: Deleting 8 words, 32 chars\r\nparser_trim_buffers: new_cap < self->words_cap\r\nsafe_realloc: buffer = 0x7fbbff63dec0, size = 24, result = 0x7fbbff63dec0\r\nsafe_realloc: buffer = 0x7fbbff63f5f0, size = 24, result = 0x7fbbff63f5f0\r\nparser_trim_buffers: new_cap = 9, stream_cap = 128, lines_cap = 48\r\nparser_trim_buffers: new_cap < self->stream_cap, calling safe_realloc\r\nsafe_realloc: buffer = 0x7fbbff63a090, size = 9, result = 0x7fbbff63a090\r\nparser_trim_buffers: new_cap < self->lines_cap\r\nsafe_realloc: buffer = 0x7fbbff63f770, size = 16, result = 0x7fbbff63f770\r\nsafe_realloc: buffer = 0x7fbbff63f8f0, size = 16, result = 0x7fbbff63f8f0\r\nfree_if_not_null 0x0\r\nfree_if_not_null 0x0\r\nfree_if_not_null 0x7fbbff63a090\r\nfree_if_not_null 0x7fbbff63dec0\r\nfree_if_not_null 0x7fbbff63f5f0\r\nfree_if_not_null 0x7fbbff63f770\r\nfree_if_not_null 0x7fbbff63f8f0\r\nfree_if_not_null 0x0\r\nfree_if_not_null 0x0\r\nfree_if_not_null 0x0\r\nfree_if_not_null 0x0\r\nfree_if_not_null 0x0\r\nfree_if_not_null 0x0\r\nfree_if_not_null 0x0\r\n\r\n```\r\n\r\n</details>"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/370064551",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19886#issuecomment-370064551",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19886",
    "id": 370064551,
    "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDA2NDU1MQ==",
    "user": {
      "login": "WillAyd",
      "id": 609873,
      "node_id": "MDQ6VXNlcjYwOTg3Mw==",
      "avatar_url": "https://avatars0.githubusercontent.com/u/609873?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/WillAyd",
      "html_url": "https://github.com/WillAyd",
      "followers_url": "https://api.github.com/users/WillAyd/followers",
      "following_url": "https://api.github.com/users/WillAyd/following{/other_user}",
      "gists_url": "https://api.github.com/users/WillAyd/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/WillAyd/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/WillAyd/subscriptions",
      "organizations_url": "https://api.github.com/users/WillAyd/orgs",
      "repos_url": "https://api.github.com/users/WillAyd/repos",
      "events_url": "https://api.github.com/users/WillAyd/events{/privacy}",
      "received_events_url": "https://api.github.com/users/WillAyd/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-03-02T21:57:49Z",
    "updated_at": "2018-03-02T21:57:49Z",
    "author_association": "MEMBER",
    "body": "Trying to confirm my suspicion, I modified the linked block above to look as follows:\r\n\r\n```python\r\nfor i in range(field_count):\r\n    word = self.parser.words[start + i]\r\n    if start + i == self.parser.words_len:  # Handle last item\r\n        word_len = self.parser.datalen - self.parser.word_starts[start + i] - 1\r\n    else:\r\n        word_len = self.parser.word_starts[start + i + 1]  - self.parser.word_starts[start + i] - 1\r\n\r\n    if path == CSTRING:\r\n        name = PyBytes_FromString(word)\r\n    elif path == UTF8:\r\n        name = PyUnicode_FromStringAndSize(word, word_len)\r\n    elif path == ENCODED:\r\n        name = PyUnicode_Decode(word, strlen(word),\r\n                                self.c_encoding, errors)\r\n\r\n```\r\n\r\nI noticed the code was actually only in a block to parse the header, but if I injected null bytes into the header it would read the entire field\r\n\r\n```python\r\n\r\nIn [7]: data = '\\x00x,\\x00y\\n\\x00test,Reg\\n\\x00\\x00\\x00,Reg\\nI,Swp\\nI,Swp\\n'\r\nIn [7]: df = pd.read_csv(StringIO(data), engine='c')\r\nIn [7]: df.columns[0]\r\nOut[7]: '\\x00x'\r\nIn [6]: df.columns[1]\r\nOut[6]: '\\x00y'\r\n\r\n```\r\n\r\nI'll dig a little further into the parsing of the body of data but pretty sure this could fix the issue. Will submit a PR if it comes so far\r\n"
  },
  {
    "url": "https://api.github.com/repos/pandas-dev/pandas/issues/comments/405002143",
    "html_url": "https://github.com/pandas-dev/pandas/issues/19886#issuecomment-405002143",
    "issue_url": "https://api.github.com/repos/pandas-dev/pandas/issues/19886",
    "id": 405002143,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTAwMjE0Mw==",
    "user": {
      "login": "changhiskhan",
      "id": 759245,
      "node_id": "MDQ6VXNlcjc1OTI0NQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/759245?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/changhiskhan",
      "html_url": "https://github.com/changhiskhan",
      "followers_url": "https://api.github.com/users/changhiskhan/followers",
      "following_url": "https://api.github.com/users/changhiskhan/following{/other_user}",
      "gists_url": "https://api.github.com/users/changhiskhan/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/changhiskhan/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/changhiskhan/subscriptions",
      "organizations_url": "https://api.github.com/users/changhiskhan/orgs",
      "repos_url": "https://api.github.com/users/changhiskhan/repos",
      "events_url": "https://api.github.com/users/changhiskhan/events{/privacy}",
      "received_events_url": "https://api.github.com/users/changhiskhan/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2018-07-14T06:12:48Z",
    "updated_at": "2018-07-14T06:12:48Z",
    "author_association": "CONTRIBUTOR",
    "body": "The easier solution would be to just add a parsing option and have the tokenizer swallow the nul bytes.\r\nTo preserve all of the nul characters you'd to do a bit of surgery to `COLITER_NEXT` and also modify some of the khash code in the parser. That's like 10x the effort of the first option easily.\r\nWhat's more important here, preserving the data after nulls OR preserving the nulls?"
  }
]
