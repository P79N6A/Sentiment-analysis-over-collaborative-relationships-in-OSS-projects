[
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/4502684",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-4502684",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 4502684,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQ1MDI2ODQ=",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-03-14T17:04:27Z",
    "updated_at": "2012-03-14T17:04:27Z",
    "author_association": "MEMBER",
    "body": "Another, more expensive idea would be to hash incoming results, and deduplicate the contents of the cache.  While this would improve performance of my test cases dramatically, it probably wouldn't have much effect on real-world code.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/5546034",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-5546034",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 5546034,
    "node_id": "MDEyOklzc3VlQ29tbWVudDU1NDYwMzQ=",
    "user": {
      "login": "jonovik",
      "id": 608322,
      "node_id": "MDQ6VXNlcjYwODMyMg==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/608322?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jonovik",
      "html_url": "https://github.com/jonovik",
      "followers_url": "https://api.github.com/users/jonovik/followers",
      "following_url": "https://api.github.com/users/jonovik/following{/other_user}",
      "gists_url": "https://api.github.com/users/jonovik/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jonovik/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jonovik/subscriptions",
      "organizations_url": "https://api.github.com/users/jonovik/orgs",
      "repos_url": "https://api.github.com/users/jonovik/repos",
      "events_url": "https://api.github.com/users/jonovik/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jonovik/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-05-07T08:49:02Z",
    "updated_at": "2012-05-07T08:49:02Z",
    "author_association": "NONE",
    "body": "> One thing we could do is have AsyncResult objects remove results from \n> the Client's result dict when they are retrieved.\n\nThat would be nice, especially as Client.results.clear() cannot safely \nbe used while iterating over an AsyncResult.\n\n```\nfrom IPython.parallel import Client\nc = Client()\nlv = c.load_balanced_view()\n\n@lv.parallel(ordered=False)\ndef g(i, s):\n    import time\n    import os\n    time.sleep(s)\n    return i, os.getpid()\n\na = g.map(range(10), [0.1] * 10)\nfor i in a:\n        print i\n        c.results.clear()\n\n(0, 7520)\n(6, 7016)\nNone\nNone\nNone\nNone\nNone\nNone\n(8, 7520)\n(9, 4820)\n```\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/7797791",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-7797791",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 7797791,
    "node_id": "MDEyOklzc3VlQ29tbWVudDc3OTc3OTE=",
    "user": {
      "login": "jankatins",
      "id": 890156,
      "node_id": "MDQ6VXNlcjg5MDE1Ng==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/890156?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jankatins",
      "html_url": "https://github.com/jankatins",
      "followers_url": "https://api.github.com/users/jankatins/followers",
      "following_url": "https://api.github.com/users/jankatins/following{/other_user}",
      "gists_url": "https://api.github.com/users/jankatins/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jankatins/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jankatins/subscriptions",
      "organizations_url": "https://api.github.com/users/jankatins/orgs",
      "repos_url": "https://api.github.com/users/jankatins/repos",
      "events_url": "https://api.github.com/users/jankatins/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jankatins/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-16T20:14:28Z",
    "updated_at": "2012-08-16T20:14:28Z",
    "author_association": "CONTRIBUTOR",
    "body": "It seems that this is not only a caching problem:\n\n```\ntask = []\nres = []\nfor a in batches: # 20 batches\n        for x in vals: # starts 800 new tasks\n                tasks.append(lv.apply(...))                \n        for task in tasks:\n            res.extend(task.get()) # Each task returns a list of dicts: 50 dicts a 18 elements         \n        import asizeof  # from http://code.activestate.com/recipes/546530/\n        print (\"in lv= \" +str(asizeof.asizeof(lv))) # prints size of lv in bytes\n        print (\"in c= \" +str(asizeof.asizeof(c)))\n        lv.purge_results(\"all\") # does nothing for the size of lv?\n        print (\"in lv= \" +str(asizeof.asizeof(lv)))\n        print (\"in c= \" +str(asizeof.asizeof(c)))\n        lv.results.clear() # again nothing\n        print (\"in lv= \" +str(asizeof.asizeof(lv)))\n        print (\"in c= \" +str(asizeof.asizeof(c)))\n        lv.client.results.clear() # frees a lot\n        print (\"in lv= \" +str(asizeof.asizeof(lv)))\n        print (\"in c= \" +str(asizeof.asizeof(c)))\n        lv.client.metadata.clear() # frees some more, but still more as in the last round\n        tasks = []\n        print (\"out lv= \" +str(asizeof.asizeof(lv)))\n        print (\"out c= \" +str(asizeof.asizeof(c)))\n        print(\"res= \" + str(asizeof.asizeof(res)))\n        print(\"batch done\"\n```\n\nResults in the following output:\n\n```\nSaving: 1/20 batches\nin lv= 54067416\nin c= 54024640\nin lv= 54067488\nin c= 54024712\nin lv= 54067488\nin c= 54024712\nin lv= 2612456\nin c= 2569680\nout lv= 470208\nout c= 427352\nres= 51285320\nbatch done\nin lv= 54310224\nin c= 54261496\nin lv= 54310296\nin c= 54261568\nin lv= 54310296\nin c= 54261568\nin lv= 2855264\nin c= 2806536\nout lv= 712584\nout c= 663776\nres= 102580072\nbatch done\n[... skipping 17 batches...]\nin lv= 60679952\nin c= 60503648\nin lv= 60680024\nin c= 60503720\nin lv= 60680024\nin c= 60503720\nin lv= 9224992\nin c= 9048688\nout lv= 7082312\nout c= 6905928\nres= 1026169976\nbatch done\n```\n\nSo with 16.000 tasks the client grows from 430kb to 6MB.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/7801426",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-7801426",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 7801426,
    "node_id": "MDEyOklzc3VlQ29tbWVudDc4MDE0MjY=",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-16T22:17:00Z",
    "updated_at": "2012-08-16T22:17:00Z",
    "author_association": "MEMBER",
    "body": "A few points:\n- purge_results is a purely remote call, there is no reason for it to affect local caches\n- view.results should _be_ client.results, but there is a bug in traitlets preventing this for views created before the first result is retrieved (see #2311)\n\nThere are two other objects that grow as a function of the number of messages, but only matter if you have quite a large number of messages (they are not a function of the size of messages, only the number):\n- `Client.history` is a list of msg_ids that the client has submitted (each View also has a history)\n- `Client.session.digest_history` is a `set` of digests used for authentication (prevents snoops from sending duplicate messages)\n\nI think these are the only remaining stateful structures, so a _complete_ reset of a Client should be:\n\n``` python\ndef clear_client(rc):\n    assert not rc.outstanding, \"Can't clear a client with outstanding tasks!\"\n    rc.results.clear()\n    rc.metadata.clear()\n    rc.history = []\n    rc.session.digest_history.clear()\n```\n\nand `view.history = []` for the View's history as well.\n\nWith these calls, you should be able to get permanent zero-growth (which I have confirmed with an adaptation of your  script).\n\nI wouldn't be _too_ concerned about the history/digests, because it really does take a large amount of tasks for them to grow.  A quick asizeof check suggests that you should get about 3000 tasks per MB, which means you need to submit a quarter million before hitting 100 MB.  If you are submitting that many, then I suppose you do need to stay on top of these.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/7815055",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-7815055",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 7815055,
    "node_id": "MDEyOklzc3VlQ29tbWVudDc4MTUwNTU=",
    "user": {
      "login": "jankatins",
      "id": 890156,
      "node_id": "MDQ6VXNlcjg5MDE1Ng==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/890156?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jankatins",
      "html_url": "https://github.com/jankatins",
      "followers_url": "https://api.github.com/users/jankatins/followers",
      "following_url": "https://api.github.com/users/jankatins/following{/other_user}",
      "gists_url": "https://api.github.com/users/jankatins/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jankatins/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jankatins/subscriptions",
      "organizations_url": "https://api.github.com/users/jankatins/orgs",
      "repos_url": "https://api.github.com/users/jankatins/repos",
      "events_url": "https://api.github.com/users/jankatins/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jankatins/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-17T10:54:24Z",
    "updated_at": "2012-08-20T07:08:20Z",
    "author_association": "CONTRIBUTOR",
    "body": "Thanks a lot for all the explanations!\n\nWould you pull such an addition to `Client`? \n\nIf I understand the code right, a \"non-caching\" `Client` would also be possible if the `AsyncResult` in [line 154](https://github.com/ipython/ipython/blob/master/IPython/parallel/client/asyncresult.py#L154) instead of only getting the results would also remove the `msg_ids` from both `self._client.results` and `self._client.metadata`?\n\nSo something along the lines of \n\n```\nresults = map(self._client.results.get, self.msg_ids)\nif self._client._remove_from_cache:\n    for id in self.msg_ids:\n        del self._client.results[id]\n        del self._client.metadata[id]\n```\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/7828368",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-7828368",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 7828368,
    "node_id": "MDEyOklzc3VlQ29tbWVudDc4MjgzNjg=",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-17T16:50:45Z",
    "updated_at": "2012-08-17T16:50:45Z",
    "author_association": "MEMBER",
    "body": "yes, pull requests are certainly welcome.  The model I have in mind is a boolean 'pop' property of AsyncResults that determines whether they would use pop or get from the Client's structures.  It might need some careful thought to get right in all cases.\n\nI think it's appropriate for the history/digest_history to be clearable via a public method, but they should _not_ be cleared under normal circumstances.  Possibly there should be a 'local' flag to client.purge_results.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/8027854",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-8027854",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 8027854,
    "node_id": "MDEyOklzc3VlQ29tbWVudDgwMjc4NTQ=",
    "user": {
      "login": "jankatins",
      "id": 890156,
      "node_id": "MDQ6VXNlcjg5MDE1Ng==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/890156?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jankatins",
      "html_url": "https://github.com/jankatins",
      "followers_url": "https://api.github.com/users/jankatins/followers",
      "following_url": "https://api.github.com/users/jankatins/following{/other_user}",
      "gists_url": "https://api.github.com/users/jankatins/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jankatins/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jankatins/subscriptions",
      "organizations_url": "https://api.github.com/users/jankatins/orgs",
      "repos_url": "https://api.github.com/users/jankatins/repos",
      "events_url": "https://api.github.com/users/jankatins/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jankatins/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-25T21:53:07Z",
    "updated_at": "2012-08-25T21:53:07Z",
    "author_association": "CONTRIBUTOR",
    "body": "Huih, the Code in `Client` and `AsyncResult` is a lot harder to understand than I thought... I also found a few minor bugs -> see a first [commit](https://github.com/JanSchulz/ipython/commits/client-clear-cache), which _only_ adresses the \"purge all cached values\" but not the \"pop instead of get\" case. \n\n(Is it better to already send a pull request for such \"unready\" code?)\n\nI also found some discrepancies between the docstring of `client.get_result()` and `client.wait()` and the actual code:\n\n``` python\nfrom IPython.parallel import Client \nc = Client(profile=\"laptop-only\")  \nlv = c.load_balanced_view()  \n\ndef do_work(input=None):\n    return input\ntasks = []\n\nfor i in range(10):\n    tasks.append(lv.apply(do_work, input=i))  \n\nfor task in tasks:\n    print(str(task.get()))\n\nlv.client.results.clear()\nlv.client.metadata.clear()\nlv.client.history = []\nlv.client.session.digest_history.clear()\nlv.results.clear()\n\n# Now \"redownload all the results from the hub\ntasks_renew = []\n\nfor task in tasks:\n    tasks_renew.append(c.get_result(task.msg_ids))\n\nc.wait()\n# Actually I would expect that all the results are now available   \n# But from the code the tasks in tasks_renew are all AsyncHubResults and \n# therefore only resubmitted on task.get().\n# So this will print both zero:\nprint(len(c.results))\nprint(len(c.outstanding))\n```\n\nWouldn't it make more sense to just let `client.get_results()` just resubmit all not locally available tasks (not in `client.outstanding` or `client.result`) and then return a AsyncResult directly? The code from `status_result()`could be refactored into a `_reget_from_hub(msg_ids)`.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/8028021",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-8028021",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 8028021,
    "node_id": "MDEyOklzc3VlQ29tbWVudDgwMjgwMjE=",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-25T22:12:46Z",
    "updated_at": "2012-08-25T22:12:46Z",
    "author_association": "MEMBER",
    "body": "On Sat, Aug 25, 2012 at 2:53 PM, JanSchulz notifications@github.com wrote:\n\n> Huih, the Code in Client and AsyncResult is a lot harder to understand\n> than I thought... I also found a few minor bugs -> see a first commithttps://github.com/JanSchulz/ipython/commits/client-clear-cache,\n> which _only_ adresses the \"purge all cached values\" but not the \"pop\n> instead of get\" case.\n> \n> I suspect the 'pop instead of get' bit will be far from trivial (this is\n> why I haven't done it already!).\n> \n> (Is it better to already send a pull request for such \"unready\" code?)\n\nA PR is totally appropriate as soon as you have code worth talking about.\n It doesn't need to be merge-ready.\n\n> I also found some discrepancies between the docstring of\n> client.get_result() and client.wait() and the actual code:\n> \n> from IPython.parallel import Client c = Client(profile=\"laptop-only\")  lv = c.load_balanced_view()\n> def do_work(input=None):\n>     return inputtasks = []\n> for i in range(10):\n>     tasks.append(lv.apply(do_work, input=i))\n> for task in tasks:\n>     print(str(task.get()))\n> lv.client.results.clear()lv.client.metadata.clear()lv.client.history = []lv.client.session.digest_history.clear()lv.results.clear()\n> \n> # Now \"redownload all the results from the hubtasks_renew = []\n> \n> for task in tasks:\n>     tasks_renew.append(c.get_result(task.msg_ids))\n> c.wait()# Actually I would expect that all the results are now available   # But from the code the tasks in tasks_renew are all AsyncHubResults and # therefore only resubmitted on task.get().\n> \n> Nothing is resubmitted - AsyncHubResults only poll for _results_.\n\nThere are two fixes for the discrepancy in client.wait() from what you are\nexpecting:\n1. update the docstring, so it is clear that client.wait() only checks for\n   outstanding tasks _that it submitted_.  This is the current behavior.\n2. when creating an AsyncHubResult, add to a parallel\n   `outstanding_from_hub` set, so that client.wait() polls for these as well\n   as regular outstanding tasks.\n\nWaiting on AsyncHubResults is quite expensive, so I might lean towards 1 -\nthat way HubResults are only awaited _explicitly_.  But maybe it makes\nsense, to avoid confusion like you faced.  AsyncHubResults are sufficiently\nuncommon to use, that it's probably okay.\n\n> # So this will print both zero:print(len(c.results))print(len(c.outstanding))\n> \n> Wouldn't it make more sense to just let client.get_results() just\n> resubmit all not locally available tasks (not in client.outstanding or\n> client.result) and then return a AsyncResult directly? The code from\n> status_result()could be refactored into a _reget_from_hub(msg_ids).\n> \n> Absolutely not.  Asking to recompute something that may have taken hours or\n> days when the result is sitting in the Hub's database would be silly, and\n> extraordinarily wasteful.  It also requires the rarely satisfied assumption\n> that all tasks are atomic, and not dependent on the state of the engines.\n> \n>  —\n> Reply to this email directly or view it on GitHubhttps://github.com/ipython/ipython/issues/1131#issuecomment-8027854.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/8028312",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-8028312",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 8028312,
    "node_id": "MDEyOklzc3VlQ29tbWVudDgwMjgzMTI=",
    "user": {
      "login": "jankatins",
      "id": 890156,
      "node_id": "MDQ6VXNlcjg5MDE1Ng==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/890156?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jankatins",
      "html_url": "https://github.com/jankatins",
      "followers_url": "https://api.github.com/users/jankatins/followers",
      "following_url": "https://api.github.com/users/jankatins/following{/other_user}",
      "gists_url": "https://api.github.com/users/jankatins/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jankatins/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jankatins/subscriptions",
      "organizations_url": "https://api.github.com/users/jankatins/orgs",
      "repos_url": "https://api.github.com/users/jankatins/repos",
      "events_url": "https://api.github.com/users/jankatins/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jankatins/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-25T22:59:59Z",
    "updated_at": "2012-08-25T23:03:49Z",
    "author_association": "CONTRIBUTOR",
    "body": "> There are two fixes for the discrepancy in client.wait() from what you are\n> expecting:\n> [...]\n\nActually I would expect that they are submitted to the hub on `get_result()` and just become available on `wait()`, just like normal tasks. If I would use `time.sleep(...)` instead of `client.wait()` normal tasks would be ready (assuming more wait time than execution time) and iterating over them would be instant but with AsyncHubResult they just get submitted while iterating which in worst case (hundreds of AsyncHubResults with one msg_id each, no results cached on the hub) would mean they are serialized.\n\n> Absolutely not.  Asking to recompute something that may have taken hours or\n> days when the result is sitting in the Hub's database would be silly, and\n> extraordinarily wasteful.  It also requires the rarely satisfied assumption\n> that all tasks are atomic, and not dependent on the state of the engines.\n\nActually I don't want `_reget_from_hub()` to recompute it. From my understanding of the whole `AsyncHubResult` mechanism and the `client.purge_results()` method, it just gets the result from the hub cache (I haven't looked into the Controller: what is cached there)? \n\nIf so, I don't get why waiting on `AsnycHubResult` is more expensive than waiting on `AsyncResult`: best case the result is in the hub cache; worst case it's the same as if resubmitting a new task and waiting for that.\n\nOr did I miss something?\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/8028696",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-8028696",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 8028696,
    "node_id": "MDEyOklzc3VlQ29tbWVudDgwMjg2OTY=",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-25T23:54:11Z",
    "updated_at": "2012-08-25T23:54:11Z",
    "author_association": "MEMBER",
    "body": "On Sat, Aug 25, 2012 at 4:00 PM, JanSchulz notifications@github.com wrote:\n\n>  There are two fixes for the discrepancy in client.wait() from what you\n> are\n> expecting:\n> [...]\n> \n>  Actually I would expect that they are submitted to the hub on\n> get_result() and just become available on wait(), just like normal tasks.\n> If I would use time.sleep(...) instead of client.wait() normal tasks\n> would be ready (assuming more wait time than execution time) and iterating\n> over them would be instant but with AsyncHubResult they just get submitted\n> while iterating which in worst case (hundreds of AsyncHubResults with one\n> msg_id each, no results cached on the hub) would mean they are serialized.\n> \n> Ah, sorry, I was confused - you keep saying 'resubmit', which has a very\n> specific meaning wrt IPython Client/Controller, and is apparently not what\n> you mean by it.  See below for what 'resubmit' means in this context.\n> \n>  Absolutely not. Asking to recompute something that may have taken hours\n> or\n> days when the result is sitting in the Hub's database would be silly, and\n> extraordinarily wasteful. It also requires the rarely satisfied assumption\n> that all tasks are atomic, and not dependent on the state of the engines.\n> \n> Actually I don't want _reget_from_hub() to recompute it. From my\n> understanding of the whole AsyncHubResult mechanism and the\n> client.purge_results() method, it just gets the result from the hub cache\n> (I haven't looked into the Controller: what is cached there)?\n> \n> The hub caches all requests and results of tasks (assuming the Task DB is\n> enabled), so they can be resubmitted or retrieved at a later point.  The\n> AsyncHubResult is the API for the retrieval.  client.resubmit() does the\n> resubmission.\n> \n>  If so, I don't get why waiting on AsnycHubResult is more expensive than\n> waiting on AsyncResult: best case the result: it's in the hub cache;\n> worst case it's the same as if resubmitting a new task and waiting for that.\n> \n> Waiting for AsyncHubResults that are not yet done is _totally different_\n> from waiting for a regular AsyncResult from an Engine (this is the whole\n> reason they are different classes).  This is what is so much more expensive.\n> \n> Or did I miss something?\n> \n> There are two ways to get results:\n1. you submitted a request for execution (AsyncResult).  In this case,\n   there will be a reply coming in on the same socket that requested the\n   computation.  The only work you do for a 'wait' is simply poll on that zmq\n   socket.  It is very cheap, with a cost in the nanoseconds.\n2. You did not request that result (or you discarded it), in which case you\n   request it from the Hub (AsyncHubResult).  Requesting a result from the hub\n   involves making 1-many 'result_status' requests of the Hub.  If you request\n   the result and it is not ready, you have to keep submitting\n   `result_requests` until it is.  It's the actual waiting that is more\n   expensive, because unlike simply polling on a zmq socket, you are polling\n   the Hub with a complete message request/reply. This is at least a few\n   orders of magnitude more expensive than simply polling on a socket.\n\nThe critical difference: Client interactions with Engines are async.\n Client interactions with the Hub are _not_ async.\n\n>  —\n> Reply to this email directly or view it on GitHubhttps://github.com/ipython/ipython/issues/1131#issuecomment-8028312.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/8032041",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-8032041",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 8032041,
    "node_id": "MDEyOklzc3VlQ29tbWVudDgwMzIwNDE=",
    "user": {
      "login": "jankatins",
      "id": 890156,
      "node_id": "MDQ6VXNlcjg5MDE1Ng==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/890156?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jankatins",
      "html_url": "https://github.com/jankatins",
      "followers_url": "https://api.github.com/users/jankatins/followers",
      "following_url": "https://api.github.com/users/jankatins/following{/other_user}",
      "gists_url": "https://api.github.com/users/jankatins/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jankatins/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jankatins/subscriptions",
      "organizations_url": "https://api.github.com/users/jankatins/orgs",
      "repos_url": "https://api.github.com/users/jankatins/repos",
      "events_url": "https://api.github.com/users/jankatins/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jankatins/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-26T10:44:24Z",
    "updated_at": "2012-08-26T10:44:24Z",
    "author_association": "CONTRIBUTOR",
    "body": "Hm, this is rather unfortunate: isn't there a way to make the \"re-receive content\" request just tell the hub to resend the results for `msg_ids` again over the normal channels and on the client side handle it as in  `send_apply_request`? This would make this whole thing _much_ easier: Then there is no difference to the normal `apply_request`: the wait would simple wait until it is available in `results` (Meaning AsyncHubResult can go away) and `status_result()` could even loose the `status_only` parameter and the corresponding code for the `status_only==False` case.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/8032095",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-8032095",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 8032095,
    "node_id": "MDEyOklzc3VlQ29tbWVudDgwMzIwOTU=",
    "user": {
      "login": "jankatins",
      "id": 890156,
      "node_id": "MDQ6VXNlcjg5MDE1Ng==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/890156?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jankatins",
      "html_url": "https://github.com/jankatins",
      "followers_url": "https://api.github.com/users/jankatins/followers",
      "following_url": "https://api.github.com/users/jankatins/following{/other_user}",
      "gists_url": "https://api.github.com/users/jankatins/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jankatins/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jankatins/subscriptions",
      "organizations_url": "https://api.github.com/users/jankatins/orgs",
      "repos_url": "https://api.github.com/users/jankatins/repos",
      "events_url": "https://api.github.com/users/jankatins/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jankatins/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-26T10:53:17Z",
    "updated_at": "2012-08-26T10:53:17Z",
    "author_association": "CONTRIBUTOR",
    "body": "The Client-Hub protocoll would then be:\n\nsend_apply_request -> tell the hub to compute the result for (msg_ids, functions) and send the result back to me (async)\nre-send_result_request -> tell the hub to resend the result for msg_ids (or an Exception) (async)\nresubmit_request -> tell the hub to recompute a rsult for msg_ids and send it back to me (async)\nstatus -> ask the hub for a status in certain msg_ids (not ansync)\n\nThe AsyncResult could then simple wait until all result for it's msg_ids are available in `client.results` and then get (or pop) them to the `AsyncResult._result`\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/8034719",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-8034719",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 8034719,
    "node_id": "MDEyOklzc3VlQ29tbWVudDgwMzQ3MTk=",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-26T16:50:48Z",
    "updated_at": "2012-08-26T16:50:48Z",
    "author_association": "MEMBER",
    "body": "No, the Hub is not involved in apply requests at all. all those go directly to engines via the Schedulers.  The Hub does not have access to the channels used for execution.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/8138618",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-8138618",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 8138618,
    "node_id": "MDEyOklzc3VlQ29tbWVudDgxMzg2MTg=",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-29T19:50:19Z",
    "updated_at": "2012-08-29T19:50:19Z",
    "author_association": "MEMBER",
    "body": "@JanSchulz I think your experience would be improved in most cases if `ar.wait(0)` were added to [get_result](https://github.com/ipython/ipython/blob/master/IPython/parallel/client/client.py#L1396) when it's a HubResult.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/8165974",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-8165974",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 8165974,
    "node_id": "MDEyOklzc3VlQ29tbWVudDgxNjU5NzQ=",
    "user": {
      "login": "jankatins",
      "id": 890156,
      "node_id": "MDQ6VXNlcjg5MDE1Ng==",
      "avatar_url": "https://avatars3.githubusercontent.com/u/890156?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jankatins",
      "html_url": "https://github.com/jankatins",
      "followers_url": "https://api.github.com/users/jankatins/followers",
      "following_url": "https://api.github.com/users/jankatins/following{/other_user}",
      "gists_url": "https://api.github.com/users/jankatins/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jankatins/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jankatins/subscriptions",
      "organizations_url": "https://api.github.com/users/jankatins/orgs",
      "repos_url": "https://api.github.com/users/jankatins/repos",
      "events_url": "https://api.github.com/users/jankatins/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jankatins/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-30T16:48:47Z",
    "updated_at": "2012-08-30T16:48:47Z",
    "author_association": "CONTRIBUTOR",
    "body": "@minrk: Yep, that would be the easiest way. \n\nOk, I just had a closer look how \"telling the hub\" works and I think I actually meant \"the part on the controller responsible for receiving messages from the client\". As far as I understand the code on the client side, everything goes to the same socket: `self._query_socket` is used for `purge_results`, `reslt_status` and `resubmit`. So the hub/scheduler/hub should have access to everything the client sent? \n\nAnyway: the hub has access to everything which is sent back to the client (engine results are cached on the hub): so why not implement the hub side so that \"results\" are always sent via the same stream/socket instead of once via the query one and once via the one used to pass on the engine results? \n\nAs far as I understand the code it would mean:\n- let \"status only\" be handled by a new message \"status_request\" (instead of a part of \"result_request\") and \"status_reply\".\n- splitting the `hub.get_results()` function in two (`get_status()` and `get_results()`) and using two different sockets/streams to send the \"result\" and the \"status\". \n- On the client side it would mean to split `client.result_status()` also in two (status-only part and `client.reget_from_hub()`), and change the code in `get_result()` to add the \"regetted\" msg_ids to `client._outstanding` and then use the new `client.reget_from_hub()` when results are not locally cached and return a normal `AsyncResult`.\n\nThis would make lots of code on the client side much easier: `AsyncHubResult` could simple be deleted and `c.result_status()` would lose the code duplication with `_handle_apply_reply()`/`_handle_apply_reply()` and would have a cleaner interface (\"Status only\" instead of both status and results).\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/8167847",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-8167847",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 8167847,
    "node_id": "MDEyOklzc3VlQ29tbWVudDgxNjc4NDc=",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2012-08-30T17:40:41Z",
    "updated_at": "2012-08-30T17:40:41Z",
    "author_association": "MEMBER",
    "body": "> As far as I understand the code on the client side, everything goes to the same socket: `self._query_socket` is used for `purge_results`, `result_status` and `resubmit`.\n\nWith the critical exception, of course, that messages associated with execution (`apply`, `execute`, `abort`) are sent via the Schedulers, and are not seen by the Hub until they have passed through the Scheduler.  The query socket is used for making queries of the cluster state, which resides on the Hub.  This is the sole function of the Hub - to monitor the state of the cluster (perhaps StateMonitor would have been a better name :), and handle queries thereof.\n\n> Anyway:why not implement the hub side so that \"results\" are always sent via the same stream/socket instead of once via the query one and once via the one used to pass on the engine results?\n\nBecause the Hub simply does not have access to the Scheduler sockets on which the original results arrived (those are in different processes, and potentially on different physical machines).\n\nI think your proposal for splitting result_status and get_results still makes good sense - the logic for having both implementations interwoven is gross, and I should have split them as soon as I saw that, rather than slowly adding to it.  But you cannot get rid of AsyncHubResult as you have proposed without the following addition to Hub.get_result:\n- when get_result is called and a result is not available, remember the client_id and msg_id in a table.\n- Every time results arrive, check the missed result request table, and if it's been requested, send it along.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/12408660",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-12408660",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 12408660,
    "node_id": "MDEyOklzc3VlQ29tbWVudDEyNDA4NjYw",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-01-18T05:48:00Z",
    "updated_at": "2013-01-18T05:48:00Z",
    "author_association": "MEMBER",
    "body": "#2340 gives better manual hooks for this, but doesn't get the automatic cache reduction part necessary to close this issue.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/17252123",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-17252123",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 17252123,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3MjUyMTIz",
    "user": {
      "login": "inti",
      "id": 376997,
      "node_id": "MDQ6VXNlcjM3Njk5Nw==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/376997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/inti",
      "html_url": "https://github.com/inti",
      "followers_url": "https://api.github.com/users/inti/followers",
      "following_url": "https://api.github.com/users/inti/following{/other_user}",
      "gists_url": "https://api.github.com/users/inti/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/inti/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/inti/subscriptions",
      "organizations_url": "https://api.github.com/users/inti/orgs",
      "repos_url": "https://api.github.com/users/inti/repos",
      "events_url": "https://api.github.com/users/inti/events{/privacy}",
      "received_events_url": "https://api.github.com/users/inti/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-04-30T20:30:08Z",
    "updated_at": "2013-04-30T20:30:08Z",
    "author_association": "NONE",
    "body": "Hi,\nI am having trouble with the memory build up, both in the client and on the engines. I wonder whether you could suggest a set of commands that will sort this out. I have tried with *.purge_results('all') and *results.clear() with little success.\nthanks in advance for your help,\nInto\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/17260390",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-17260390",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 17260390,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3MjYwMzkw",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-04-30T23:19:43Z",
    "updated_at": "2013-04-30T23:19:43Z",
    "author_association": "MEMBER",
    "body": "How many tasks and how big? It can be tricky to make sure that you actually do clear everything on the Client side.\n\nHow are you running tasks on the Engine side? Is it exclusively through apply/etc, or do you do a lot of execute/%px?  apply should be fairly stateless.\n\nIn general, it helps to summarize your tasks: how many, how frequently, how much data do you move around, what APIs you use the most, etc.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/17273698",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-17273698",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 17273698,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3MjczNjk4",
    "user": {
      "login": "inti",
      "id": 376997,
      "node_id": "MDQ6VXNlcjM3Njk5Nw==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/376997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/inti",
      "html_url": "https://github.com/inti",
      "followers_url": "https://api.github.com/users/inti/followers",
      "following_url": "https://api.github.com/users/inti/following{/other_user}",
      "gists_url": "https://api.github.com/users/inti/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/inti/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/inti/subscriptions",
      "organizations_url": "https://api.github.com/users/inti/orgs",
      "repos_url": "https://api.github.com/users/inti/repos",
      "events_url": "https://api.github.com/users/inti/events{/privacy}",
      "received_events_url": "https://api.github.com/users/inti/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-01T08:59:21Z",
    "updated_at": "2013-05-01T08:59:21Z",
    "author_association": "NONE",
    "body": "Hi, thanks for the reply. Sorry for not providing enough info, I am quite new to python and more so to iPython parallel.\n- How many tasks and how big?\n\nI am using ipython parallel as part of a [MCMC](http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) sampler, each iteration has in the order of 10s to 100s tasks and I use in the order of 1000s of iterations. \n- How are you running tasks on the Engine side?\n\nIn general I send to the engines an object which encapsulates my data and additional information necessary to perform the jobs. \n- Is it exclusively through apply/etc, or do you do a lot of execute/%px? \n\nI am using map with block=True on a load-balanced view. I only use execute to load modules on the engines.\n\nThe data itself are numpy arrays with integers of variable length, from 100s to 10000s elements each. I normally have about 10s to 100 of this arrays. The data is encapsulated on a object that has the data itself plus some other variables related to the MCMC. it is useful to pass the whole object because it is self contained in terms of the actual computations.\n\nAfter each iteration, i.e., after running each batch of the tasks I do c.purge_results('all'), where c is the client. The memory usage goes up quite quickly into 10-15 GB ...\n\nTo do honest, I do not even know how to diagnose what is using so much memory. I do know that if I close the ipython shell a good bunch of the memory (~1/3) is free and then the rest when I close the ipython cluster.\n\nThanks a lot in advance for your help,\nInti\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/17667517",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-17667517",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 17667517,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE3NjY3NTE3",
    "user": {
      "login": "inti",
      "id": 376997,
      "node_id": "MDQ6VXNlcjM3Njk5Nw==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/376997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/inti",
      "html_url": "https://github.com/inti",
      "followers_url": "https://api.github.com/users/inti/followers",
      "following_url": "https://api.github.com/users/inti/following{/other_user}",
      "gists_url": "https://api.github.com/users/inti/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/inti/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/inti/subscriptions",
      "organizations_url": "https://api.github.com/users/inti/orgs",
      "repos_url": "https://api.github.com/users/inti/repos",
      "events_url": "https://api.github.com/users/inti/events{/privacy}",
      "received_events_url": "https://api.github.com/users/inti/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-09T14:35:27Z",
    "updated_at": "2013-05-09T14:35:27Z",
    "author_association": "NONE",
    "body": "Hi, I am not sure If I provided enough info on the previous meesage. Please let me know if there is something I am missing and that I can say to help in getting some help. BW, Inti\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/18652044",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-18652044",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 18652044,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE4NjUyMDQ0",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-29T23:06:09Z",
    "updated_at": "2013-05-29T23:06:09Z",
    "author_association": "MEMBER",
    "body": "Sorry, this one dropped off my radar.  Can you provide an actual code sample to reproduce the issue?  I have found `purge_results('all')` ( or `purge_everything()`, in the case of _very_ large numbers of tasks) to be effective at keeping the cache down, as long as I am not also storing the results elsewhere.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/18668631",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-18668631",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 18668631,
    "node_id": "MDEyOklzc3VlQ29tbWVudDE4NjY4NjMx",
    "user": {
      "login": "inti",
      "id": 376997,
      "node_id": "MDQ6VXNlcjM3Njk5Nw==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/376997?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/inti",
      "html_url": "https://github.com/inti",
      "followers_url": "https://api.github.com/users/inti/followers",
      "following_url": "https://api.github.com/users/inti/following{/other_user}",
      "gists_url": "https://api.github.com/users/inti/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/inti/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/inti/subscriptions",
      "organizations_url": "https://api.github.com/users/inti/orgs",
      "repos_url": "https://api.github.com/users/inti/repos",
      "events_url": "https://api.github.com/users/inti/events{/privacy}",
      "received_events_url": "https://api.github.com/users/inti/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-05-30T08:51:53Z",
    "updated_at": "2013-05-30T08:51:53Z",
    "author_association": "NONE",
    "body": "Hi,\nI'll try to provide some code. the parallele tasks are part of a larger \npiece of code which may not be easy to reproduce at the moment outside \nmy computer. Give me a couple of days to put together a reproduce \nexample. I'll try the |purge_everything() call and see how that goes.\nthanks !\nInti\n|\nOn 30/05/13 00:06, Min RK wrote:\n\n> Sorry, this one dropped off my radar. Can you provide an actual code \n> sample to reproduce the issue? I have found |purge_results('all')| ( \n> or |purge_everything()|, in the case of /very/ large numbers of tasks) \n> to be effective at keeping the cache down, as long as I am not also \n> storing the results elsewhere.\n> \n> —\n> Reply to this email directly or view it on GitHub \n> https://github.com/ipython/ipython/issues/1131#issuecomment-18652044.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/42072650",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-42072650",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 42072650,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDcyNjUw",
    "user": {
      "login": "pbrady",
      "id": 3905022,
      "node_id": "MDQ6VXNlcjM5MDUwMjI=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/3905022?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/pbrady",
      "html_url": "https://github.com/pbrady",
      "followers_url": "https://api.github.com/users/pbrady/followers",
      "following_url": "https://api.github.com/users/pbrady/following{/other_user}",
      "gists_url": "https://api.github.com/users/pbrady/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/pbrady/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/pbrady/subscriptions",
      "organizations_url": "https://api.github.com/users/pbrady/orgs",
      "repos_url": "https://api.github.com/users/pbrady/repos",
      "events_url": "https://api.github.com/users/pbrady/events{/privacy}",
      "received_events_url": "https://api.github.com/users/pbrady/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-05-02T20:00:22Z",
    "updated_at": "2014-05-02T20:00:22Z",
    "author_association": "NONE",
    "body": "Has there been any progress on this.  I'm using IPython 2.0 and am trying use map_async on a load balanced view.  However, my parameter study is somewhat large - roughly 25,000 and the memory used by the clients steadily increases and crashes my maching after processing 1,000 of the 25,000 calls needed.  I was able to rewrite my functions using the ProcessPoolExecuter and the memory for each process stays pegged ad 0.8% of my system memory.  I would prefer to use the IPython.parallel infrastructure.\n\nThe function that I'm mapping takes 10-15 arguments but produces no output (i.e. all output is dumped to a file)\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/42075392",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-42075392",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 42075392,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDc1Mzky",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-05-02T20:26:31Z",
    "updated_at": "2014-05-02T20:26:31Z",
    "author_association": "MEMBER",
    "body": "@pbrady large requests shouldn't be cached in the client, so only results should make the Client memory really grow. What version of pyzmq are you using?\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/42075550",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-42075550",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 42075550,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDc1NTUw",
    "user": {
      "login": "pbrady",
      "id": 3905022,
      "node_id": "MDQ6VXNlcjM5MDUwMjI=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/3905022?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/pbrady",
      "html_url": "https://github.com/pbrady",
      "followers_url": "https://api.github.com/users/pbrady/followers",
      "following_url": "https://api.github.com/users/pbrady/following{/other_user}",
      "gists_url": "https://api.github.com/users/pbrady/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/pbrady/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/pbrady/subscriptions",
      "organizations_url": "https://api.github.com/users/pbrady/orgs",
      "repos_url": "https://api.github.com/users/pbrady/repos",
      "events_url": "https://api.github.com/users/pbrady/events{/privacy}",
      "received_events_url": "https://api.github.com/users/pbrady/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-05-02T20:28:23Z",
    "updated_at": "2014-05-02T20:28:23Z",
    "author_association": "NONE",
    "body": "looks like version 14.1.0\n\n$ conda list\n\n# packages in environment at /home/ptb/miniconda3:\n\n#\nbackports.ssl-match-hostname 3.4.0.2                   <pip>\nconda                     3.4.2                    py34_0\nconda-build               1.3.2                    py34_0\ncython                    0.20.1                   py34_0\ndateutil                  2.1                      py34_2\nfreetype                  2.4.10                        0\nipython                   2.0.0                    py34_0\nipython-notebook          2.0.0                    py34_0\njinja2                    2.7.2                    py34_0\nlibpng                    1.5.13                        1\nlibsodium                 0.4.5                         0\nllvm                      3.3                           0\nllvmpy                    0.12.4                   py34_0\nmarkupsafe                0.18                     py34_0\nmatplotlib                1.3.1                np18py34_0\nmpld3                     0.2                       <pip>\nnumba                     0.13.1               np18py34_0\nnumexpr                   2.3.1                np18py34_0\nnumpy                     1.8.1                    py34_0\nopenssl                   1.0.1g                        0\npandas                    0.13.1               np18py34_0\npatchelf                  0.6                           0\npatsy                     0.2.1                np18py34_0\npip                       1.5.4                    py34_0\npycosat                   0.6.0                    py34_0\npygments                  1.6                      py34_0\npyparsing                 2.0.1                    py34_0\npython                    3.4.0                         0\npython-dateutil           2.1                       <pip>\npytz                      2014.2                   py34_0\npyyaml                    3.11                     py34_0\npyzmq                     14.1.0                   py34_0\nreadline                  6.2                           2\nscipy                     0.13.3               np18py34_0\nsetuptools                3.4.4                    py34_0\nsix                       1.6.1                    py34_0\nsqlite                    3.8.4.1                       0\nssl_match_hostname        3.4.0.2                  py34_0\nstatsmodels               0.5.0                np18py34_0\nsympy                     0.7.5                    py34_0\nsystem                    5.8                           1\ntk                        8.5.13                        0\ntornado                   3.2.0                    py34_0\nutil-linux                2.21                          0\nyaml                      0.1.4                         0\nzeromq                    4.0.4                         0\nzlib                      1.2.7                         0\n\nOn Fri, May 2, 2014 at 2:26 PM, Min RK notifications@github.com wrote:\n\n> @pbrady https://github.com/pbrady large requests shouldn't be cached in\n> the client, so only results should make the Client memory really grow. What\n> version of pyzmq are you using?\n> \n> —\n> Reply to this email directly or view it on GitHubhttps://github.com/ipython/ipython/issues/1131#issuecomment-42075392\n> .\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/42075631",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-42075631",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 42075631,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDc1NjMx",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-05-02T20:29:16Z",
    "updated_at": "2014-05-02T20:29:16Z",
    "author_association": "MEMBER",
    "body": "Try `conda update pyzmq`. There is a memory leak in pyzmq 14.0-14.1, fixed in 14.2 (current on conda).\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/42078152",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-42078152",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 42078152,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDc4MTUy",
    "user": {
      "login": "pbrady",
      "id": 3905022,
      "node_id": "MDQ6VXNlcjM5MDUwMjI=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/3905022?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/pbrady",
      "html_url": "https://github.com/pbrady",
      "followers_url": "https://api.github.com/users/pbrady/followers",
      "following_url": "https://api.github.com/users/pbrady/following{/other_user}",
      "gists_url": "https://api.github.com/users/pbrady/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/pbrady/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/pbrady/subscriptions",
      "organizations_url": "https://api.github.com/users/pbrady/orgs",
      "repos_url": "https://api.github.com/users/pbrady/repos",
      "events_url": "https://api.github.com/users/pbrady/events{/privacy}",
      "received_events_url": "https://api.github.com/users/pbrady/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-05-02T20:55:57Z",
    "updated_at": "2014-05-02T21:06:07Z",
    "author_association": "NONE",
    "body": "I update pyzmq but still see the same memory growth.  Here's the relevant\nportion of the code, AFAIK\n\n``` python\nfrom IPython.parallel import Client\nrc = Client()\ndv = rc[:]\n\nh = symbols('h')\nident = 'NonCons'\nfile_set = simulation.file_set(ident)\nopt_dict =\n{'interior':cons_dict['int_eq'],'lboundary':cons_dict['lboundary'],\n\n'rboundary':cons_dict['rboundary'],'material_props':{'conv':0.9,'diff':0.01},\n            'file_ident':ident,'numeric':True,'file_set':file_set}\n# get function ready for async parallel execution\neig = lambda args :\n simulation.eig_analysis(npoints=args[1],ib_frac=args[0],\n                                             sympy_subs=args[2],**opt_dict)\ndv.execute('import simulation')\ndv['opt_dict'] = opt_dict\nib_frac = [1.0/2**i for i in range(20)]\n#nx = [20*2**i for i in range(4)]\nnx = [40]\ne0_subs = list((-1e3/h)/2**i for i in range(18))\ne0_subs.extend(list((1e3/h)/2**i for i in range(18)))\nd1_subs = list((-1e3/h)/2**i for i in range(18))\nd1_subs.extend(list((1e3/h)/2**i for i in range(18)))\n\nsubs = ( ((e0,e),(d1,d)) for e in e0_subs for d in d1_subs)\nlview = rc.load_balanced_view()\nar = lview.map_async(eig,(p for p in\nproduct(zip(repeat(nx[-1]),ib_frac),nx,subs)) )\n```\n\nAm I calling map_async in an incorrect/inefficient way?\n\nThanks for the quick replies\n\nOn Fri, May 2, 2014 at 2:29 PM, Min RK notifications@github.com wrote:\n\n> Try conda update pyzmq. There is a memory leak in pyzmq 14.0-14.1, fixed\n> in 14.2 (current on conda).\n> \n> —\n> Reply to this email directly or view it on GitHubhttps://github.com/ipython/ipython/issues/1131#issuecomment-42075631\n> .\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/42080958",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-42080958",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 42080958,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDgwOTU4",
    "user": {
      "login": "minrk",
      "id": 151929,
      "node_id": "MDQ6VXNlcjE1MTkyOQ==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/151929?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/minrk",
      "html_url": "https://github.com/minrk",
      "followers_url": "https://api.github.com/users/minrk/followers",
      "following_url": "https://api.github.com/users/minrk/following{/other_user}",
      "gists_url": "https://api.github.com/users/minrk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/minrk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/minrk/subscriptions",
      "organizations_url": "https://api.github.com/users/minrk/orgs",
      "repos_url": "https://api.github.com/users/minrk/repos",
      "events_url": "https://api.github.com/users/minrk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/minrk/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-05-02T21:28:34Z",
    "updated_at": "2014-05-02T21:28:34Z",
    "author_association": "MEMBER",
    "body": "I'll dig into it.​\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/42104576",
    "html_url": "https://github.com/ipython/ipython/issues/1131#issuecomment-42104576",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/1131",
    "id": 42104576,
    "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTA0NTc2",
    "user": {
      "login": "pbrady",
      "id": 3905022,
      "node_id": "MDQ6VXNlcjM5MDUwMjI=",
      "avatar_url": "https://avatars3.githubusercontent.com/u/3905022?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/pbrady",
      "html_url": "https://github.com/pbrady",
      "followers_url": "https://api.github.com/users/pbrady/followers",
      "following_url": "https://api.github.com/users/pbrady/following{/other_user}",
      "gists_url": "https://api.github.com/users/pbrady/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/pbrady/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/pbrady/subscriptions",
      "organizations_url": "https://api.github.com/users/pbrady/orgs",
      "repos_url": "https://api.github.com/users/pbrady/repos",
      "events_url": "https://api.github.com/users/pbrady/events{/privacy}",
      "received_events_url": "https://api.github.com/users/pbrady/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2014-05-03T13:04:19Z",
    "updated_at": "2014-05-03T13:04:19Z",
    "author_association": "NONE",
    "body": "So I'm not sure what I changed but now I see the same memory growth using\nthe ProcessPoolExecuter and even running in serial so I don't think this is\nan IPython issue.  Any ideas on how to debug memory leaks in python?\n\nOn Fri, May 2, 2014 at 3:28 PM, Min RK notifications@github.com wrote:\n\n> I'll dig into it.​\n> \n> —\n> Reply to this email directly or view it on GitHubhttps://github.com/ipython/ipython/issues/1131#issuecomment-42080958\n> .\n"
  }
]
