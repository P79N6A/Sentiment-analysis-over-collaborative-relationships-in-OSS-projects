[
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/26798519",
    "html_url": "https://github.com/ipython/ipython/issues/4418#issuecomment-26798519",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/4418",
    "id": 26798519,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI2Nzk4NTE5",
    "user": {
      "login": "ahmadia",
      "id": 512293,
      "node_id": "MDQ6VXNlcjUxMjI5Mw==",
      "avatar_url": "https://avatars0.githubusercontent.com/u/512293?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ahmadia",
      "html_url": "https://github.com/ahmadia",
      "followers_url": "https://api.github.com/users/ahmadia/followers",
      "following_url": "https://api.github.com/users/ahmadia/following{/other_user}",
      "gists_url": "https://api.github.com/users/ahmadia/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ahmadia/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ahmadia/subscriptions",
      "organizations_url": "https://api.github.com/users/ahmadia/orgs",
      "repos_url": "https://api.github.com/users/ahmadia/repos",
      "events_url": "https://api.github.com/users/ahmadia/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ahmadia/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-22T12:30:17Z",
    "updated_at": "2013-10-22T12:30:17Z",
    "author_association": "CONTRIBUTOR",
    "body": "@dusans - It would be impossible to do this deterministically, as there are far too many unknowns at play in any parallel job.  What you could do in your own code is dynamically measure throughput and the last delta in throughput from increasing or decreasing the number of cores, and gradually adjust this until you reach the optimal number of cores.  The most common thing to do is to set the number of processes = the number of physical cores on the system, which tends to work right for most HPC workloads.\n"
  },
  {
    "url": "https://api.github.com/repos/ipython/ipython/issues/comments/26981418",
    "html_url": "https://github.com/ipython/ipython/issues/4418#issuecomment-26981418",
    "issue_url": "https://api.github.com/repos/ipython/ipython/issues/4418",
    "id": 26981418,
    "node_id": "MDEyOklzc3VlQ29tbWVudDI2OTgxNDE4",
    "user": {
      "login": "dusans",
      "id": 875020,
      "node_id": "MDQ6VXNlcjg3NTAyMA==",
      "avatar_url": "https://avatars1.githubusercontent.com/u/875020?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dusans",
      "html_url": "https://github.com/dusans",
      "followers_url": "https://api.github.com/users/dusans/followers",
      "following_url": "https://api.github.com/users/dusans/following{/other_user}",
      "gists_url": "https://api.github.com/users/dusans/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dusans/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dusans/subscriptions",
      "organizations_url": "https://api.github.com/users/dusans/orgs",
      "repos_url": "https://api.github.com/users/dusans/repos",
      "events_url": "https://api.github.com/users/dusans/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dusans/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2013-10-24T10:28:06Z",
    "updated_at": "2013-10-24T10:28:06Z",
    "author_association": "NONE",
    "body": "Thanks ahmadia. I did for the first implementation spawn as many engines as there are cpu cores.\n\nThe docs mentions UnmetDependency which enabled me to write a simple test:\n\n```\n@require('subprocess', 'os', 'psutil', 'time', 'multiprocessing')\ndef long_running_func(args):\n    from IPython.parallel.error import UnmetDependency\n\n    if not has_enought_free_cpu():\n        raise UnmetDependency('not enought free_cpu')\n```\n"
  }
]
